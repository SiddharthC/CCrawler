[[[{"content": "\n          \n            \n  \nScrapy 0.16 documentation\u00b6\nThis documentation contains everything you need to know about Scrapy.\n\nGetting help\u00b6\nHaving trouble? We\u2019d like to help!\nTry the FAQ \u2013 it\u2019s got answers to some common questions.\nLooking for specific information? Try the Index or Module Index.\nSearch for information in the archives of the scrapy-users mailing list, or\npost a question.\nAsk a question in the #scrapy IRC channel.\nReport bugs with Scrapy in our issue tracker.\n\n\nFirst steps\u00b6\n\n\nScrapy at a glance\nUnderstand what Scrapy is and how it can help you.\nInstallation guide\nGet Scrapy installed on your computer.\nScrapy Tutorial\nWrite your first Scrapy project.\nExamples\nLearn more by playing with a pre-made Scrapy project.\n\n\nBasic concepts\u00b6\n\n\nCommand line tool\nLearn about the command-line tool used to manage your Scrapy project.\nItems\nDefine the data you want to scrape.\nSpiders\nWrite the rules to crawl your websites.\nSelectors\nExtract the data from web pages using XPath.\nScrapy shell\nTest your extraction code in an interactive environment.\nItem Loaders\nPopulate your items with the extracted data.\nItem Pipeline\nPost-process and store your scraped data.\nFeed exports\nOutput your scraped data using different formats and storages.\nLink Extractors\nConvenient classes to extract links to follow from pages.\n\n\nBuilt-in services\u00b6\n\n\nLogging\nUnderstand the simple logging facility provided by Scrapy.\nStats Collection\nCollect statistics about your scraping crawler.\nSending e-mail\nSend email notifications when certain events occur.\nTelnet Console\nInspect a running crawler using a built-in Python console.\nWeb Service\nMonitor and control a crawler using a web service.\n\n\nSolving specific problems\u00b6\n\n\nFrequently Asked Questions\nGet answers to most frequently asked questions.\nDebugging Spiders\nLearn how to debug common problems of your scrapy spider.\nSpiders Contracts\nLearn how to use contracts for testing your spiders.\nCommon Practices\nGet familiar with some Scrapy common practices.\nBroad Crawls\nTune Scrapy for crawling a lot domains in parallel.\nUsing Firefox for scraping\nLearn how to scrape with Firefox and some useful add-ons.\nUsing Firebug for scraping\nLearn how to scrape efficiently using Firebug.\nDebugging memory leaks\nLearn how to find and get rid of memory leaks in your crawler.\nDownloading Item Images\nDownload static images associated with your scraped items.\nUbuntu packages\nInstall latest Scrapy packages easily on Ubuntu\nScrapy Service (scrapyd)\nDeploying your Scrapy project in production.\nAutoThrottle extension\nAdjust crawl rate dynamically based on load.\nJobs: pausing and resuming crawls\nLearn how to pause and resume crawls for large spiders.\nDjangoItem\nWrite scraped items using Django models.\n\n\nExtending Scrapy\u00b6\n\n\nArchitecture overview\nUnderstand the Scrapy architecture.\nDownloader Middleware\nCustomize how pages get requested and downloaded.\nSpider Middleware\nCustomize the input and output of your spiders.\nExtensions\nExtend Scrapy with your custom functionality\nCore API\nUse it on extensions and middlewares to extend Scrapy functionality\n\n\nReference\u00b6\n\n\nCommand line tool\nLearn about the command-line tool and see all available commands.\nRequests and Responses\nUnderstand the classes used to represent HTTP requests and responses.\nSettings\nLearn how to configure Scrapy and see all available settings.\nSignals\nSee all available signals and how to work with them.\nExceptions\nSee all available exceptions and their meaning.\nItem Exporters\nQuickly export your scraped items to a file (XML, CSV, etc).\n\n\nAll the rest\u00b6\n\n\nRelease notes\nSee what has changed in recent Scrapy versions.\nContributing to Scrapy\nLearn how to contribute to the Scrapy project.\nVersioning and API Stability\nUnderstand Scrapy versioning and API stability.\nExperimental features\nLearn about bleeding-edge features.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/", "title": ["Scrapy 0.16 documentation \u2014 Scrapy 0.16.4 documentation"]}][[[[[[[{"content": "\n          \n            \n  \nScrapy 0.16 documentation\u00b6\nThis documentation contains everything you need to know about Scrapy.\n\nGetting help\u00b6\nHaving trouble? We\u2019d like to help!\nTry the FAQ \u2013 it\u2019s got answers to some common questions.\nLooking for specific information? Try the Index or Module Index.\nSearch for information in the archives of the scrapy-users mailing list, or\npost a question.\nAsk a question in the #scrapy IRC channel.\nReport bugs with Scrapy in our issue tracker.\n\n\nFirst steps\u00b6\n\n\nScrapy at a glance\nUnderstand what Scrapy is and how it can help you.\nInstallation guide\nGet Scrapy installed on your computer.\nScrapy Tutorial\nWrite your first Scrapy project.\nExamples\nLearn more by playing with a pre-made Scrapy project.\n\n\nBasic concepts\u00b6\n\n\nCommand line tool\nLearn about the command-line tool used to manage your Scrapy project.\nItems\nDefine the data you want to scrape.\nSpiders\nWrite the rules to crawl your websites.\nSelectors\nExtract the data from web pages using XPath.\nScrapy shell\nTest your extraction code in an interactive environment.\nItem Loaders\nPopulate your items with the extracted data.\nItem Pipeline\nPost-process and store your scraped data.\nFeed exports\nOutput your scraped data using different formats and storages.\nLink Extractors\nConvenient classes to extract links to follow from pages.\n\n\nBuilt-in services\u00b6\n\n\nLogging\nUnderstand the simple logging facility provided by Scrapy.\nStats Collection\nCollect statistics about your scraping crawler.\nSending e-mail\nSend email notifications when certain events occur.\nTelnet Console\nInspect a running crawler using a built-in Python console.\nWeb Service\nMonitor and control a crawler using a web service.\n\n\nSolving specific problems\u00b6\n\n\nFrequently Asked Questions\nGet answers to most frequently asked questions.\nDebugging Spiders\nLearn how to debug common problems of your scrapy spider.\nSpiders Contracts\nLearn how to use contracts for testing your spiders.\nCommon Practices\nGet familiar with some Scrapy common practices.\nBroad Crawls\nTune Scrapy for crawling a lot domains in parallel.\nUsing Firefox for scraping\nLearn how to scrape with Firefox and some useful add-ons.\nUsing Firebug for scraping\nLearn how to scrape efficiently using Firebug.\nDebugging memory leaks\nLearn how to find and get rid of memory leaks in your crawler.\nDownloading Item Images\nDownload static images associated with your scraped items.\nUbuntu packages\nInstall latest Scrapy packages easily on Ubuntu\nScrapy Service (scrapyd)\nDeploying your Scrapy project in production.\nAutoThrottle extension\nAdjust crawl rate dynamically based on load.\nJobs: pausing and resuming crawls\nLearn how to pause and resume crawls for large spiders.\nDjangoItem\nWrite scraped items using Django models.\n\n\nExtending Scrapy\u00b6\n\n\nArchitecture overview\nUnderstand the Scrapy architecture.\nDownloader Middleware\nCustomize how pages get requested and downloaded.\nSpider Middleware\nCustomize the input and output of your spiders.\nExtensions\nExtend Scrapy with your custom functionality\nCore API\nUse it on extensions and middlewares to extend Scrapy functionality\n\n\nReference\u00b6\n\n\nCommand line tool\nLearn about the command-line tool and see all available commands.\nRequests and Responses\nUnderstand the classes used to represent HTTP requests and responses.\nSettings\nLearn how to configure Scrapy and see all available settings.\nSignals\nSee all available signals and how to work with them.\nExceptions\nSee all available exceptions and their meaning.\nItem Exporters\nQuickly export your scraped items to a file (XML, CSV, etc).\n\n\nAll the rest\u00b6\n\n\nRelease notes\nSee what has changed in recent Scrapy versions.\nContributing to Scrapy\nLearn how to contribute to the Scrapy project.\nVersioning and API Stability\nUnderstand Scrapy versioning and API stability.\nExperimental features\nLearn about bleeding-edge features.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/", "title": ["Scrapy 0.16 documentation \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nScrapy at a glance\u00b6\nScrapy is an application framework for crawling web sites and extracting\nstructured data which can be used for a wide range of useful applications, like\ndata mining, information processing or historical archival.\nEven though Scrapy was originally designed for screen scraping (more\nprecisely, web scraping), it can also be used to extract data using APIs\n(such as Amazon Associates Web Services) or as a general purpose web\ncrawler.\nThe purpose of this document is to introduce you to the concepts behind Scrapy\nso you can get an idea of how it works and decide if Scrapy is what you need.\nWhen you\u2019re ready to start a project, you can start with the tutorial.\n\nPick a website\u00b6\nSo you need to extract some information from a website, but the website doesn\u2019t\nprovide any API or mechanism to access that info programmatically.  Scrapy can\nhelp you extract that information.\nLet\u2019s say we want to extract the URL, name, description and size of all torrent\nfiles added today in the Mininova site.\nThe list of all torrents added today can be found on this page:\n\nhttp://www.mininova.org/today\n\n\nDefine the data you want to scrape\u00b6\nThe first thing is to define the data we want to scrape. In Scrapy, this is\ndone through Scrapy Items (Torrent files, in this case).\nThis would be our Item:\nfrom scrapy.item import Item, Field\n\nclass Torrent(Item):\n    url = Field()\n    name = Field()\n    description = Field()\n    size = Field()\n\n\n\n\nWrite a Spider to extract the data\u00b6\nThe next thing is to write a Spider which defines the start URL\n(http://www.mininova.org/today), the rules for following links and the rules\nfor extracting the data from pages.\nIf we take a look at that page content we\u2019ll see that all torrent URLs are like\nhttp://www.mininova.org/tor/NUMBER where NUMBER is an integer. We\u2019ll use\nthat to construct the regular expression for the links to follow: /tor/\\d+.\nWe\u2019ll use XPath for selecting the data to extract from the web page HTML\nsource. Let\u2019s take one of those torrent pages:\n\nhttp://www.mininova.org/tor/2657665\nAnd look at the page HTML source to construct the XPath to select the data we\nwant which is: torrent name, description and size.\nBy looking at the page HTML source we can see that the file name is contained\ninside a <h1> tag:\n<h1>Home[2009][Eng]XviD-ovd</h1>\n\n\nAn XPath expression to extract the name could be:\n//h1/text()\n\n\nAnd the description is contained inside a <div> tag with id=\"description\":\n<h2>Description:</h2>\n\n<div id=\"description\">\n\"HOME\" - a documentary film by Yann Arthus-Bertrand\n<br/>\n<br/>\n***\n<br/>\n<br/>\n\"We are living in exceptional times. Scientists tell us that we have 10 years to change the way we live, avert the depletion of natural resources and the catastrophic evolution of the Earth's climate.\n\n...\n\n\nAn XPath expression to select the description could be:\n//div[@id='description']\n\n\nFinally, the file size is contained in the second <p> tag inside the <div>\ntag with id=specifications:\n<div id=\"specifications\">\n\n<p>\n<strong>Category:</strong>\n<a href=\"/cat/4\">Movies</a> &gt; <a href=\"/sub/35\">Documentary</a>\n</p>\n\n<p>\n<strong>Total size:</strong>\n699.79&nbsp;megabyte</p>\n\n\nAn XPath expression to select the description could be:\n//div[@id='specifications']/p[2]/text()[2]\n\n\nFor more information about XPath see the XPath reference.\nFinally, here\u2019s the spider code:\nclass MininovaSpider(CrawlSpider):\n\n    name = 'mininova.org'\n    allowed_domains = ['mininova.org']\n    start_urls = ['http://www.mininova.org/today']\n    rules = [Rule(SgmlLinkExtractor(allow=['/tor/\\d+']), 'parse_torrent')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n\n        torrent = TorrentItem()\n        torrent['url'] = response.url\n        torrent['name'] = x.select(\"//h1/text()\").extract()\n        torrent['description'] = x.select(\"//div[@id='description']\").extract()\n        torrent['size'] = x.select(\"//div[@id='info-left']/p[2]/text()[2]\").extract()\n        return torrent\n\n\nFor brevity\u2019s sake, we intentionally left out the import statements. The\nTorrent item is defined above.\n\n\nRun the spider to extract the data\u00b6\nFinally, we\u2019ll run the spider to crawl the site an output file\nscraped_data.json with the scraped data in JSON format:\nscrapy crawl mininova.org -o scraped_data.json -t json\n\nThis uses feed exports to generate the JSON file.\nYou can easily change the export format (XML or CSV, for example) or the\nstorage backend (FTP or Amazon S3, for example).\nYou can also write an item pipeline to store the\nitems in a database very easily.\n\n\nReview scraped data\u00b6\nIf you check the scraped_data.json file after the process finishes, you\u2019ll\nsee the scraped items there:\n[{\"url\": \"http://www.mininova.org/tor/2657665\", \"name\": [\"Home[2009][Eng]XviD-ovd\"], \"description\": [\"HOME - a documentary film by ...\"], \"size\": [\"699.69 megabyte\"]},\n# ... other items ...\n]\n\n\nYou\u2019ll notice that all field values (except for the url which was assigned\ndirectly) are actually lists. This is because the selectors return lists. You may want to store single values, or\nperform some additional parsing/cleansing to the values. That\u2019s what\nItem Loaders are for.\n\n\nWhat else?\u00b6\nYou\u2019ve seen how to extract and store items from a website using Scrapy, but\nthis is just the surface. Scrapy provides a lot of powerful features for making\nscraping easy and efficient, such as:\nBuilt-in support for selecting and extracting data\nfrom HTML and XML sources\nBuilt-in support for cleaning and sanitizing the scraped data using a\ncollection of reusable filters (called Item Loaders)\nshared between all the spiders.\nBuilt-in support for generating feed exports in\nmultiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,\nS3, local filesystem)\nA media pipeline for automatically downloading images\n(or any other media) associated with the scraped items\nSupport for extending Scrapy by plugging\nyour own functionality using signals and a\nwell-defined API (middlewares, extensions, and\npipelines).\nWide range of built-in middlewares and extensions for:cookies and session handling\nHTTP compression\nHTTP authentication\nHTTP cache\nuser-agent spoofing\nrobots.txt\ncrawl depth restriction\nand more\n\nRobust encoding support and auto-detection, for dealing with foreign,\nnon-standard and broken encoding declarations.\nSupport for creating spiders based on pre-defined templates, to speed up\nspider creation and make their code more consistent on large projects. See\ngenspider command for more details.\nExtensible stats collection for multiple spider\nmetrics, useful for monitoring the performance of your spiders and detecting\nwhen they get broken\nAn Interactive shell console for trying XPaths, very\nuseful for writing and debugging your spiders\nA System service designed to ease the deployment and\nrun of your spiders in production.\nA built-in Web service for monitoring and\ncontrolling your bot\nA Telnet console for hooking into a Python\nconsole running inside your Scrapy process, to introspect and debug your\ncrawler\nLogging facility that you can hook on to for catching\nerrors during the scraping process.\nSupport for crawling based on URLs discovered through Sitemaps\nA caching DNS resolver\n\n\nWhat\u2019s next?\u00b6\nThe next obvious steps are for you to download Scrapy, read the\ntutorial and join the community. Thanks for your\ninterest!\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/overview.html", "title": ["Scrapy at a glance \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nInstallation guide\u00b6\n\nPre-requisites\u00b6\nThe installation steps assume that you have the following things installed:\nPython 2.6 or 2.7\nOpenSSL. This comes preinstalled in all operating systems except Windows (see Platform specific installation notes)\npip or easy_install Python package managers\n\n\nInstalling Scrapy\u00b6\nYou can install Scrapy using easy_install or pip (which is the canonical way to\ndistribute and install Python packages).\n\nNote\nCheck Platform specific installation notes first.\n\nTo install using pip:\npip install Scrapy\n\nTo install using easy_install:\neasy_install Scrapy\n\n\n\nPlatform specific installation notes\u00b6\n\nWindows\u00b6\nAfter installing Python, follow these steps before installing Scrapy:\nadd the C:\\python27\\Scripts and C:\\python27 folders to the system\npath by adding those directories to the PATH environment variable from\nthe Control Panel.\ninstall OpenSSL by following these steps:go to Win32 OpenSSL page\ndownload Visual C++ 2008 redistributables for your Windows and architecture\ndownload OpenSSL for your Windows and architecture (the regular version, not the light one)\nadd the c:\\openssl-win32\\bin (or similar) directory to your PATH, the same way you added python27 in the first step`` in the first step\n\nsome binary packages that Scrapy depends on (like Twisted, lxml and pyOpenSSL) require a compiler available to install, and fail if you don\u2019t have Visual Studio installed. You can find Windows installers for those in the following links. Make sure you respect your Python version and Windows architecture.pywin32: http://sourceforge.net/projects/pywin32/files/\nTwisted: http://twistedmatrix.com/trac/wiki/Downloads\nzope.interface: download the egg from zope.interface pypi page and install it by running easy_install file.egg\nlxml: http://pypi.python.org/pypi/lxml/\npyOpenSSL: https://launchpad.net/pyopenssl\n\n\nUbuntu 9.10 or above\u00b6\nDon\u2019t use the python-scrapy package provided by Ubuntu, they are\ntypically too old and slow to catch up with latest Scrapy.\nInstead, use the official Ubuntu Packages, which already\nsolve all dependencies for you and are continuously updated with the latest bug\nfixes.\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/install.html", "title": ["Installation guide \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nScrapy Tutorial\u00b6\nIn this tutorial, we\u2019ll assume that Scrapy is already installed on your system.\nIf that\u2019s not the case, see Installation guide.\nWe are going to use Open directory project (dmoz) as\nour example domain to scrape.\nThis tutorial will walk you through these tasks:\nCreating a new Scrapy project\nDefining the Items you will extract\nWriting a spider to crawl a site and extract\nItems\nWriting an Item Pipeline to store the\nextracted Items\nScrapy is written in Python. If you\u2019re new to the language you might want to\nstart by getting an idea of what the language is like, to get the most out of\nScrapy.  If you\u2019re already familiar with other languages, and want to learn\nPython quickly, we recommend Learn Python The Hard Way.  If you\u2019re new to programming\nand want to start with Python, take a look at this list of Python resources\nfor non-programmers.\n\nCreating a project\u00b6\nBefore you start scraping, you will have set up a new Scrapy project. Enter a\ndirectory where you\u2019d like to store your code and then run:\nscrapy startproject tutorial\n\nThis will create a tutorial directory with the following contents:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\nThese are basically:\nscrapy.cfg: the project configuration file\ntutorial/: the project\u2019s python module, you\u2019ll later import your code from\nhere.\ntutorial/items.py: the project\u2019s items file.\ntutorial/pipelines.py: the project\u2019s pipelines file.\ntutorial/settings.py: the project\u2019s settings file.\ntutorial/spiders/: a directory where you\u2019ll later put your spiders.\n\n\nDefining our Item\u00b6\nItems are containers that will be loaded with the scraped data; they work\nlike simple python dicts but provide additional protecting against populating\nundeclared fields, to prevent typos.\nThey are declared by creating an scrapy.item.Item class an defining\nits attributes as scrapy.item.Field objects, like you will in an ORM\n(don\u2019t worry if you\u2019re not familiar with ORMs, you will see that this is an\neasy task).\nWe begin by modeling the item that we will use to hold the sites data obtained\nfrom dmoz.org, as we want to capture the name, url and description of the\nsites, we define fields for each of these three attributes. To do that, we edit\nitems.py, found in the tutorial directory. Our Item class looks like this:\nfrom scrapy.item import Item, Field\n\nclass DmozItem(Item):\n    title = Field()\n    link = Field()\n    desc = Field()\n\n\nThis may seem complicated at first, but defining the item allows you to use other handy\ncomponents of Scrapy that need to know how your item looks like.\n\n\nOur first Spider\u00b6\nSpiders are user-written classes used to scrape information from a domain (or group\nof domains).\nThey define an initial list of URLs to download, how to follow links, and how\nto parse the contents of those pages to extract items.\nTo create a Spider, you must subclass scrapy.spider.BaseSpider, and\ndefine the three main, mandatory, attributes:\nname: identifies the Spider. It must be\nunique, that is, you can\u2019t set the same name for different Spiders.\n\nstart_urls: is a list of URLs where the\nSpider will begin to crawl from.  So, the first pages downloaded will be those\nlisted here. The subsequent URLs will be generated successively from data\ncontained in the start URLs.\n\nparse() is a method of the spider, which will\nbe called with the downloaded Response object of each\nstart URL. The response is passed to the method as the first and only\nargument.\nThis method is responsible for parsing the response data and extracting\nscraped data (as scraped items) and more URLs to follow.\nThe parse() method is in charge of processing\nthe response and returning scraped data (as Item\nobjects) and more URLs to follow (as Request objects).\n\nThis is the code for our first Spider; save it in a file named\ndmoz_spider.py under the dmoz/spiders directory:\nfrom scrapy.spider import BaseSpider\n\nclass DmozSpider(BaseSpider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        open(filename, 'wb').write(response.body)\n\n\n\nCrawling\u00b6\nTo put our spider to work, go to the project\u2019s top level directory and run:\nscrapy crawl dmoz\n\nThe crawl dmoz command runs the spider for the dmoz.org domain. You\nwill get an output similar to this:\n2008-08-20 03:51:13-0300 [scrapy] INFO: Started project: dmoz\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled extensions: ...\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled downloader middlewares: ...\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled spider middlewares: ...\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled item pipelines: ...\n2008-08-20 03:51:14-0300 [dmoz] INFO: Spider opened\n2008-08-20 03:51:14-0300 [dmoz] DEBUG: Crawled <http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: <None>)\n2008-08-20 03:51:14-0300 [dmoz] DEBUG: Crawled <http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: <None>)\n2008-08-20 03:51:14-0300 [dmoz] INFO: Spider closed (finished)\n\nPay attention to the lines containing [dmoz], which corresponds to our\nspider. You can see a log line for each URL defined in start_urls. Because\nthese URLs are the starting ones, they have no referrers, which is shown at the\nend of the log line, where it says (referer: <None>).\nBut more interesting, as our parse method instructs, two files have been\ncreated: Books and Resources, with the content of both URLs.\n\nWhat just happened under the hood?\u00b6\nScrapy creates scrapy.http.Request objects for each URL in the\nstart_urls attribute of the Spider, and assigns them the parse method of\nthe spider as their callback function.\nThese Requests are scheduled, then executed, and\nscrapy.http.Response objects are returned and then fed back to the\nspider, through the parse() method.\n\n\n\nExtracting Items\u00b6\n\nIntroduction to Selectors\u00b6\nThere are several ways to extract data from web pages. Scrapy uses a mechanism\nbased on XPath expressions called XPath selectors.\nFor more information about selectors and other extraction mechanisms see the\nXPath selectors documentation.\nHere are some examples of XPath expressions and their meanings:\n/html/head/title: selects the <title> element, inside the <head>\nelement of a HTML document\n/html/head/title/text(): selects the text inside the aforementioned\n<title> element.\n//td: selects all the <td> elements\n//div[@class=\"mine\"]: selects all div elements which contain an\nattribute class=\"mine\"\nThese are just a couple of simple examples of what you can do with XPath, but\nXPath expressions are indeed much more powerful. To learn more about XPath we\nrecommend this XPath tutorial.\nFor working with XPaths, Scrapy provides a XPathSelector\nclass, which comes in two flavours, HtmlXPathSelector\n(for HTML data) and XmlXPathSelector (for XML data). In\norder to use them you must instantiate the desired class with a\nResponse object.\nYou can see selectors as objects that represent nodes in the document\nstructure. So, the first instantiated selectors are associated to the root\nnode, or the entire document.\nSelectors have three methods (click on the method to see the complete API\ndocumentation).\nselect(): returns a list of selectors, each of\nthem representing the nodes selected by the xpath expression given as\nargument.\n\nextract(): returns a unicode string with\nthe data selected by the XPath selector.\n\n\nre(): returns a list of unicode strings\nextracted by applying the regular expression given as argument.\n\n\n\nTrying Selectors in the Shell\u00b6\nTo illustrate the use of Selectors we\u2019re going to use the built-in Scrapy\nshell, which also requires IPython (an extended Python console)\ninstalled on your system.\nTo start a shell, you must go to the project\u2019s top level directory and run:\nscrapy shell http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\n\nThis is what the shell looks like:\n[ ... Scrapy log here ... ]\n\n[s] Available Scrapy objects:\n[s] 2010-08-19 21:45:59-0300 [default] INFO: Spider closed (finished)\n[s]   hxs        <HtmlXPathSelector (http://www.dmoz.org/Computers/Programming/Languages/Python/Books/) xpath=None>\n[s]   item       Item()\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   spider     <BaseSpider 'default' at 0x1b6c2d0>\n[s]   xxs        <XmlXPathSelector (http://www.dmoz.org/Computers/Programming/Languages/Python/Books/) xpath=None>\n[s] Useful shortcuts:\n[s]   shelp()           Print this help\n[s]   fetch(req_or_url) Fetch a new request or URL and update shell objects\n[s]   view(response)    View response in a browser\n\nIn [1]:\n\nAfter the shell loads, you will have the response fetched in a local\nresponse variable, so if you type response.body you will see the body\nof the response, or you can type response.headers to see its headers.\nThe shell also instantiates two selectors, one for HTML (in the hxs\nvariable) and one for XML (in the xxs variable) with this response. So let\u2019s\ntry them:\nIn [1]: hxs.select('//title')\nOut[1]: [<HtmlXPathSelector (title) xpath=//title>]\n\nIn [2]: hxs.select('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: hxs.select('//title/text()')\nOut[3]: [<HtmlXPathSelector (text) xpath=//title/text()>]\n\nIn [4]: hxs.select('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: hxs.select('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\nExtracting the data\u00b6\nNow, let\u2019s try to extract some real information from those pages.\nYou could type response.body in the console, and inspect the source code to\nfigure out the XPaths you need to use. However, inspecting the raw HTML code\nthere could become a very tedious task. To make this an easier task, you can\nuse some Firefox extensions like Firebug. For more information see\nUsing Firebug for scraping and Using Firefox for scraping.\nAfter inspecting the page source, you\u2019ll find that the web sites information\nis inside a <ul> element, in fact the second <ul> element.\nSo we can select each <li> element belonging to the sites list with this\ncode:\nhxs.select('//ul/li')\n\n\nAnd from them, the sites descriptions:\nhxs.select('//ul/li/text()').extract()\n\n\nThe sites titles:\nhxs.select('//ul/li/a/text()').extract()\n\n\nAnd the sites links:\nhxs.select('//ul/li/a/@href').extract()\n\n\nAs we said before, each select() call returns a list of selectors, so we can\nconcatenate further select() calls to dig deeper into a node. We are going to use\nthat property here, so:\nsites = hxs.select('//ul/li')\nfor site in sites:\n    title = site.select('a/text()').extract()\n    link = site.select('a/@href').extract()\n    desc = site.select('text()').extract()\n    print title, link, desc\n\n\n\nNote\nFor a more detailed description of using nested selectors, see\nNesting selectors and\nWorking with relative XPaths in the Selectors\ndocumentation\n\nLet\u2019s add this code to our spider:\nfrom scrapy.spider import BaseSpider\nfrom scrapy.selector import HtmlXPathSelector\n\nclass DmozSpider(BaseSpider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        sites = hxs.select('//ul/li')\n        for site in sites:\n            title = site.select('a/text()').extract()\n            link = site.select('a/@href').extract()\n            desc = site.select('text()').extract()\n            print title, link, desc\n\n\nNow try crawling the dmoz.org domain again and you\u2019ll see sites being printed\nin your output, run:\nscrapy crawl dmoz\n\n\n\n\nUsing our item\u00b6\nItem objects are custom python dicts; you can access the\nvalues of their fields (attributes of the class we defined earlier) using the\nstandard dict syntax like:\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\nSpiders are expected to return their scraped data inside\nItem objects. So, in order to return the data we\u2019ve\nscraped so far, the final code for our Spider would be like this:\nfrom scrapy.spider import BaseSpider\nfrom scrapy.selector import HtmlXPathSelector\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(BaseSpider):\n   name = \"dmoz\"\n   allowed_domains = [\"dmoz.org\"]\n   start_urls = [\n       \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n       \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n   ]\n\n   def parse(self, response):\n       hxs = HtmlXPathSelector(response)\n       sites = hxs.select('//ul/li')\n       items = []\n       for site in sites:\n           item = DmozItem()\n           item['title'] = site.select('a/text()').extract()\n           item['link'] = site.select('a/@href').extract()\n           item['desc'] = site.select('text()').extract()\n           items.append(item)\n       return items\n\n\n\nNote\nYou can find a fully-functional variant of this spider in the dirbot\nproject available at https://github.com/scrapy/dirbot\n\nNow doing a crawl on the dmoz.org domain yields DmozItem\u2018s:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\nStoring the scraped data\u00b6\nThe simplest way to store the scraped data is by using the Feed exports, with the following command:\nscrapy crawl dmoz -o items.json -t json\n\nThat will generate a items.json file containing all scraped items,\nserialized in JSON.\nIn small projects (like the one in this tutorial), that should be enough.\nHowever, if you want to perform more complex things with the scraped items, you\ncan write an Item Pipeline. As with Items, a\nplaceholder file for Item Pipelines has been set up for you when the project is\ncreated, in tutorial/pipelines.py. Though you don\u2019t need to implement any item\npipeline if you just want to store the scraped items.\n\n\nNext steps\u00b6\nThis tutorial covers only the basics of Scrapy, but there\u2019s a lot of other\nfeatures not mentioned here. Check the What else? section in\nScrapy at a glance chapter for a quick overview of the most important ones.\nThen, we recommend you continue by playing with an example project (see\nExamples), and then continue with the section\nBasic concepts.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/tutorial.html", "title": ["Scrapy Tutorial \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExamples\u00b6\nThe best way to learn is with examples, and Scrapy is no exception. For this\nreason, there is an example Scrapy project named dirbot, that you can use to\nplay and learn more about Scrapy. It contains the dmoz spider described in the\ntutorial.\nThis dirbot project is available at: https://github.com/scrapy/dirbot\nIt contains a README file with a detailed description of the project contents.\nIf you\u2019re familiar with git, you can checkout the code. Otherwise you can\ndownload a tarball or zip file of the project by clicking on Downloads.\nThe scrapy tag on Snipplr is used for sharing code snippets such as spiders,\nmiddlewares, extensions, or scripts. Feel free (and encouraged!) to share any\ncode there.\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/examples.html", "title": ["Examples \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nCommand line tool\u00b6\n\nNew in version 0.10.\nScrapy is controlled through the scrapy command-line tool, to be referred\nhere as the \u201cScrapy tool\u201d to differentiate it from their sub-commands which we\njust call \u201ccommands\u201d, or \u201cScrapy commands\u201d.\nThe Scrapy tool provides several commands, for multiple purposes, and each one\naccepts a different set of arguments and options.\n\nDefault structure of Scrapy projects\u00b6\nBefore delving into the command-line tool and its sub-commands, let\u2019s first\nunderstand the directory structure of a Scrapy project.\nEven thought it can be modified, all Scrapy projects have the same file\nstructure by default, similar to this:\nscrapy.cfg\nmyproject/\n    __init__.py\n    items.py\n    pipelines.py\n    settings.py\n    spiders/\n        __init__.py\n        spider1.py\n        spider2.py\n        ...\n\nThe directory where the scrapy.cfg file resides is known as the project\nroot directory. That file contains the name of the python module that defines\nthe project settings. Here is an example:\n[settings]\ndefault = myproject.settings\n\n\n\n\nUsing the scrapy tool\u00b6\nYou can start by running the Scrapy tool with no arguments and it will print\nsome usage help and the available commands:\nScrapy X.Y - no active project\n\nUsage:\n  scrapy <command> [options] [args]\n\nAvailable commands:\n  crawl         Start crawling a spider or URL\n  fetch         Fetch a URL using the Scrapy downloader\n[...]\n\nThe first line will print the currently active project, if you\u2019re inside a\nScrapy project. In this, it was run from outside a project. If run from inside\na project it would have printed something like this:\nScrapy X.Y - project: myproject\n\nUsage:\n  scrapy <command> [options] [args]\n\n[...]\n\n\nCreating projects\u00b6\nThe first thing you typically do with the scrapy tool is create your Scrapy\nproject:\nscrapy startproject myproject\n\nThat will create a Scrapy project under the myproject directory.\nNext, you go inside the new project directory:\ncd myproject\n\nAnd you\u2019re ready to use use the scrapy command to manage and control your\nproject from there.\n\n\nControlling projects\u00b6\nYou use the scrapy tool from inside your projects to control and manage\nthem.\nFor example, to create a new spider:\nscrapy genspider mydomain mydomain.com\n\nSome Scrapy commands (like crawl) must be run from inside a Scrapy\nproject. See the commands reference below for more\ninformation on which commands must be run from inside projects, and which not.\nAlso keep in mind that some commands may have slightly different behaviours\nwhen running them from inside projects. For example, the fetch command will use\nspider-overridden behaviours (such as the user_agent attribute to override\nthe user-agent) if the url being fetched is associated with some specific\nspider. This is intentional, as the fetch command is meant to be used to\ncheck how spiders are downloading pages.\n\n\n\nAvailable tool commands\u00b6\nThis section contains a list of the available built-in commands with a\ndescription and some usage examples. Remember you can always get more info\nabout each command by running:\nscrapy <command> -h\n\n\nAnd you can see all available commands with:\nscrapy -h\n\n\nThere are two kinds of commands, those that only work from inside a Scrapy\nproject (Project-specific commands) and those that also work without an active\nScrapy project (Global commands), though they may behave slightly different\nwhen running from inside a project (as they would use the project overridden\nsettings).\nGlobal commands:\nstartproject\nsettings\nrunspider\nshell\nfetch\nview\nversion\nProject-only commands:\ncrawl\ncheck\nlist\nedit\nparse\ngenspider\nserver\ndeploy\n\nstartproject\u00b6\nSyntax: scrapy startproject <project_name>\nRequires project: no\nCreates a new Scrapy project named project_name, under the project_name\ndirectory.\nUsage example:\n$ scrapy startproject myproject\n\n\n\ngenspider\u00b6\nSyntax: scrapy genspider [-t template] <name> <domain>\nRequires project: yes\nCreate a new spider in the current project.\nThis is just a convenient shortcut command for creating spiders based on\npre-defined templates, but certainly not the only way to create spiders. You\ncan just create the spider source code files yourself, instead of using this\ncommand.\nUsage example:\n$ scrapy genspider -l\nAvailable templates:\n  basic\n  crawl\n  csvfeed\n  xmlfeed\n\n$ scrapy genspider -d basic\nfrom scrapy.spider import BaseSpider\n\nclass $classname(BaseSpider):\n    name = \"$name\"\n    allowed_domains = [\"$domain\"]\n    start_urls = (\n        'http://www.$domain/',\n        )\n\n    def parse(self, response):\n        pass\n\n$ scrapy genspider -t basic example example.com\nCreated spider 'example' using template 'basic' in module:\n  mybot.spiders.example\n\n\n\ncrawl\u00b6\nSyntax: scrapy crawl <spider>\nRequires project: yes\nStart crawling a spider.\nUsage examples:\n$ scrapy crawl myspider\n[ ... myspider starts crawling ... ]\n\n\n\ncheck\u00b6\nSyntax: scrapy check [-l] <spider>\nRequires project: yes\nRun contract checks.\nUsage examples:\n$ scrapy check -l\nfirst_spider\n  * parse\n  * parse_item\nsecond_spider\n  * parse\n  * parse_item\n\n$ scrapy check\n[FAILED] first_spider:parse_item\n>>> 'RetailPricex' field is missing\n\n[FAILED] first_spider:parse\n>>> Returned 92 requests, expected 0..4\n\n\n\nserver\u00b6\nSyntax: scrapy server\nRequires project: yes\nStart Scrapyd server for this project, which can be referred from the JSON API\nwith the project name default. For more info see: Scrapy Service (scrapyd).\nUsage example:\n$ scrapy server\n[ ... scrapyd starts and stays idle waiting for spiders to get scheduled ... ]\n\nTo schedule spiders, use the Scrapyd JSON API.\n\n\nlist\u00b6\nSyntax: scrapy list\nRequires project: yes\nList all available spiders in the current project. The output is one spider per\nline.\nUsage example:\n$ scrapy list\nspider1\nspider2\n\n\n\nedit\u00b6\nSyntax: scrapy edit <spider>\nRequires project: yes\nEdit the given spider using the editor defined in the EDITOR\nsetting.\nThis command is provided only as a convenient shortcut for the most common\ncase, the developer is of course free to choose any tool or IDE to write and\ndebug his spiders.\nUsage example:\n$ scrapy edit spider1\n\n\n\nfetch\u00b6\nSyntax: scrapy fetch <url>\nRequires project: no\nDownloads the given URL using the Scrapy downloader and writes the contents to\nstandard output.\nThe interesting thing about this command is that it fetches the page how the\nthe spider would download it. For example, if the spider has an USER_AGENT\nattribute which overrides the User Agent, it will use that one.\nSo this command can be used to \u201csee\u201d how your spider would fetch certain page.\nIf used outside a project, no particular per-spider behaviour would be applied\nand it will just use the default Scrapy downloder settings.\nUsage examples:\n$ scrapy fetch --nolog http://www.example.com/some/page.html\n[ ... html content here ... ]\n\n$ scrapy fetch --nolog --headers http://www.example.com/\n{'Accept-Ranges': ['bytes'],\n 'Age': ['1263   '],\n 'Connection': ['close     '],\n 'Content-Length': ['596'],\n 'Content-Type': ['text/html; charset=UTF-8'],\n 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],\n 'Etag': ['\"573c1-254-48c9c87349680\"'],\n 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],\n 'Server': ['Apache/2.2.3 (CentOS)']}\n\n\n\nview\u00b6\nSyntax: scrapy view <url>\nRequires project: no\nOpens the given URL in a browser, as your Scrapy spider would \u201csee\u201d it.\nSometimes spiders see pages differently from regular users, so this can be used\nto check what the spider \u201csees\u201d and confirm it\u2019s what you expect.\nUsage example:\n$ scrapy view http://www.example.com/some/page.html\n[ ... browser starts ... ]\n\n\n\nshell\u00b6\nSyntax: scrapy shell [url]\nRequires project: no\nStarts the Scrapy shell for the given URL (if given) or empty if not URL is\ngiven. See Scrapy shell for more info.\nUsage example:\n$ scrapy shell http://www.example.com/some/page.html\n[ ... scrapy shell starts ... ]\n\n\n\nparse\u00b6\nSyntax: scrapy parse <url> [options]\nRequires project: yes\nFetches the given URL and parses with the spider that handles it, using the\nmethod passed with the --callback option, or parse if not given.\nSupported options:\n--callback or -c: spider method to use as callback for parsing the\nresponse\n--rules or -r: use CrawlSpider\nrules to discover the callback (ie. spider method) to use for parsing the\nresponse\n--noitems: don\u2019t show scraped items\n--nolinks: don\u2019t show extracted links\n--depth or -d: depth level for which the requests should be followed\nrecursively (default: 1)\n--verbose or -v: display information for each depth level\nUsage example:\n$ scrapy parse http://www.example.com/ -c parse_item\n[ ... scrapy log lines crawling example.com spider ... ]\n\n>>> STATUS DEPTH LEVEL 1 <<<\n# Scraped Items  ------------------------------------------------------------\n[{'name': u'Example item',\n 'category': u'Furniture',\n 'length': u'12 cm'}]\n\n# Requests  -----------------------------------------------------------------\n[]\n\n\n\nsettings\u00b6\nSyntax: scrapy settings [options]\nRequires project: no\nGet the value of a Scrapy setting.\nIf used inside a project it\u2019ll show the project setting value, otherwise it\u2019ll\nshow the default Scrapy value for that setting.\nExample usage:\n$ scrapy settings --get BOT_NAME\nscrapybot\n$ scrapy settings --get DOWNLOAD_DELAY\n0\n\n\n\nrunspider\u00b6\nSyntax: scrapy runspider <spider_file.py>\nRequires project: no\nRun a spider self-contained in a Python file, without having to create a\nproject.\nExample usage:\n$ scrapy runspider myspider.py\n[ ... spider starts crawling ... ]\n\n\n\nversion\u00b6\nSyntax: scrapy version [-v]\nRequires project: no\nPrints the Scrapy version. If used with -v it also prints Python, Twisted\nand Platform info, which is useful for bug reports.\n\n\ndeploy\u00b6\n\nNew in version 0.11.\nSyntax: scrapy deploy [ <target:project> | -l <target> | -L ]\nRequires project: yes\nDeploy the project into a Scrapyd server. See Deploying your project.\n\n\n\nCustom project commands\u00b6\nYou can also add your custom project commands by using the\nCOMMANDS_MODULE setting. See the Scrapy commands in\nscrapy/commands for examples on how to implement your commands.\n\nCOMMANDS_MODULE\u00b6\nDefault: '' (empty string)\nA module to use for looking custom Scrapy commands. This is used to add custom\ncommands for your Scrapy project.\nExample:\nCOMMANDS_MODULE = 'mybot.commands'\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/commands.html", "title": ["Command line tool \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nItems\u00b6\nThe main goal in scraping is to extract structured data from unstructured\nsources, typically, web pages. Scrapy provides the Item class for this\npurpose.\nItem objects are simple containers used to collect the scraped data.\nThey provide a dictionary-like API with a convenient syntax for declaring\ntheir available fields.\n\nDeclaring Items\u00b6\nItems are declared using a simple class definition syntax and Field\nobjects. Here is an example:\nfrom scrapy.item import Item, Field\n\nclass Product(Item):\n    name = Field()\n    price = Field()\n    stock = Field()\n    last_updated = Field(serializer=str)\n\n\n\nNote\nThose familiar with Django will notice that Scrapy Items are\ndeclared similar to Django Models, except that Scrapy Items are much\nsimpler as there is no concept of different field types.\n\n\n\nItem Fields\u00b6\nField objects are used to specify metadata for each field. For\nexample, the serializer function for the last_updated field illustrated in\nthe example above.\nYou can specify any kind of metadata for each field. There is no restriction on\nthe values accepted by Field objects. For this same\nreason, there isn\u2019t a reference list of all available metadata keys. Each key\ndefined in Field objects could be used by a different components, and\nonly those components know about it. You can also define and use any other\nField key in your project too, for your own needs. The main goal of\nField objects is to provide a way to define all field metadata in one\nplace. Typically, those components whose behaviour depends on each field use\ncertain field keys to configure that behaviour. You must refer to their\ndocumentation to see which metadata keys are used by each component.\nIt\u2019s important to note that the Field objects used to declare the item\ndo not stay assigned as class attributes. Instead, they can be accessed through\nthe Item.fields attribute.\nAnd that\u2019s all you need to know about declaring items.\n\n\nWorking with Items\u00b6\nHere are some examples of common tasks performed with items, using the\nProduct item declared above. You will\nnotice the API is very similar to the dict API.\n\nCreating items\u00b6\n>>> product = Product(name='Desktop PC', price=1000)\n>>> print product\nProduct(name='Desktop PC', price=1000)\n\n\n\n\nGetting field values\u00b6\n>>> product['name']\nDesktop PC\n>>> product.get('name')\nDesktop PC\n\n>>> product['price']\n1000\n\n>>> product['last_updated']\nTraceback (most recent call last):\n    ...\nKeyError: 'last_updated'\n\n>>> product.get('last_updated', 'not set')\nnot set\n\n>>> product['lala'] # getting unknown field\nTraceback (most recent call last):\n    ...\nKeyError: 'lala'\n\n>>> product.get('lala', 'unknown field')\n'unknown field'\n\n>>> 'name' in product  # is name field populated?\nTrue\n\n>>> 'last_updated' in product  # is last_updated populated?\nFalse\n\n>>> 'last_updated' in product.fields  # is last_updated a declared field?\nTrue\n\n>>> 'lala' in product.fields  # is lala a declared field?\nFalse\n\n\n\n\nSetting field values\u00b6\n>>> product['last_updated'] = 'today'\n>>> product['last_updated']\ntoday\n\n>>> product['lala'] = 'test' # setting unknown field\nTraceback (most recent call last):\n    ...\nKeyError: 'Product does not support field: lala'\n\n\n\n\nAccessing all populated values\u00b6\nTo access all populated values, just use the typical dict API:\n>>> product.keys()\n['price', 'name']\n\n>>> product.items()\n[('price', 1000), ('name', 'Desktop PC')]\n\n\n\n\nOther common tasks\u00b6\nCopying items:\n>>> product2 = Product(product)\n>>> print product2\nProduct(name='Desktop PC', price=1000)\n\n\nCreating dicts from items:\n>>> dict(product) # create a dict from all populated values\n{'price': 1000, 'name': 'Desktop PC'}\n\n\nCreating items from dicts:\n>>> Product({'name': 'Laptop PC', 'price': 1500})\nProduct(price=1500, name='Laptop PC')\n\n>>> Product({'name': 'Laptop PC', 'lala': 1500}) # warning: unknown field in dict\nTraceback (most recent call last):\n    ...\nKeyError: 'Product does not support field: lala'\n\n\n\n\n\nExtending Items\u00b6\nYou can extend Items (to add more fields or to change some metadata for some\nfields) by declaring a subclass of your original Item.\nFor example:\nclass DiscountedProduct(Product):\n    discount_percent = Field(serializer=str)\n    discount_expiration_date = Field()\n\n\nYou can also extend field metadata by using the previous field metadata and\nappending more values, or changing existing values, like this:\nclass SpecificProduct(Product):\n    name = Field(Product.fields['name'], serializer=my_serializer)\n\n\nThat adds (or replaces) the serializer metadata key for the name field,\nkeeping all the previously existing metadata values.\n\n\nItem objects\u00b6\n\nclass scrapy.item.Item([arg])\u00b6\nReturn a new Item optionally initialized from the given argument.\nItems replicate the standard dict API, including its constructor. The\nonly additional attribute provided by Items is:\n\nfields\u00b6\nA dictionary containing all declared fields for this Item, not only\nthose populated. The keys are the field names and the values are the\nField objects used in the Item declaration.\n\n\nField objects\u00b6\n\nclass scrapy.item.Field([arg])\u00b6\nThe Field class is just an alias to the built-in dict class and\ndoesn\u2019t provide any extra functionality or attributes. In other words,\nField objects are plain-old Python dicts. A separate class is used\nto support the item declaration syntax\nbased on class attributes.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/items.html", "title": ["Items \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSpiders\u00b6\nSpiders are classes which define how a certain site (or group of sites) will be\nscraped, including how to perform the crawl (ie. follow links) and how to\nextract structured data from their pages (ie. scraping items). In other words,\nSpiders are the place where you define the custom behaviour for crawling and\nparsing pages for a particular site (or, in some cases, group of sites).\nFor spiders, the scraping cycle goes through something like this:\nYou start by generating the initial Requests to crawl the first URLs, and\nspecify a callback function to be called with the response downloaded from\nthose requests.\nThe first requests to perform are obtained by calling the\nstart_requests() method which (by default)\ngenerates Request for the URLs specified in the\nstart_urls and the\nparse method as callback function for the\nRequests.\n\nIn the callback function, you parse the response (web page) and return either\nItem objects, Request objects,\nor an iterable of both. Those Requests will also contain a callback (maybe\nthe same) and will then be downloaded by Scrapy and then their\nresponse handled by the specified callback.\n\nIn callback functions, you parse the page contents, typically using\nSelectors (but you can also use BeautifulSoup, lxml or whatever\nmechanism you prefer) and generate items with the parsed data.\n\nFinally, the items returned from the spider will be typically persisted to a\ndatabase (in some Item Pipeline) or written to\na file using Feed exports.\n\nEven though this cycle applies (more or less) to any kind of spider, there are\ndifferent kinds of default spiders bundled into Scrapy for different purposes.\nWe will talk about those types here.\n\nSpider arguments\u00b6\nSpiders can receive arguments that modify their behaviour. Some common uses for\nspider arguments are to define the start URLs or to restrict the crawl to\ncertain sections of the site, but they can be used to configure any\nfunctionality of the spider.\nSpider arguments are passed through the crawl command using the\n-a option. For example:\nscrapy crawl myspider -a category=electronics\n\nSpiders receive arguments in their constructors:\nclass MySpider(BaseSpider):\n    name = 'myspider'\n\n    def __init__(self, category=None):\n        self.start_urls = ['http://www.example.com/categories/%s' % category]\n        # ...\n\n\nSpider arguments can also be passed through the schedule.json API.\n\n\nBuilt-in spiders reference\u00b6\nScrapy comes with some useful generic spiders that you can use, to subclass\nyour spiders from. Their aim is to provide convenient functionality for a few\ncommon scraping cases, like following all links on a site based on certain\nrules, crawling from Sitemaps, or parsing a XML/CSV feed.\nFor the examples used in the following spiders, we\u2019ll assume you have a project\nwith a TestItem declared in a myproject.items module:\nfrom scrapy.item import Item\n\nclass TestItem(Item):\n    id = Field()\n    name = Field()\n    description = Field()\n\n\n\nBaseSpider\u00b6\n\nclass scrapy.spider.BaseSpider\u00b6\nThis is the simplest spider, and the one from which every other spider\nmust inherit from (either the ones that come bundled with Scrapy, or the ones\nthat you write yourself). It doesn\u2019t provide any special functionality. It just\nrequests the given start_urls/start_requests, and calls the spider\u2019s\nmethod parse for each of the resulting responses.\n\nname\u00b6\nA string which defines the name for this spider. The spider name is how\nthe spider is located (and instantiated) by Scrapy, so it must be\nunique. However, nothing prevents you from instantiating more than one\ninstance of the same spider. This is the most important spider attribute\nand it\u2019s required.\nIf the spider scrapes a single domain, a common practice is to name the\nspider after the domain, or without the TLD. So, for example, a\nspider that crawls mywebsite.com would often be called\nmywebsite.\n\nallowed_domains\u00b6\nAn optional list of strings containing domains that this spider is\nallowed to crawl. Requests for URLs not belonging to the domain names\nspecified in this list won\u2019t be followed if\nOffsiteMiddleware is enabled.\n\nstart_urls\u00b6\nA list of URLs where the spider will begin to crawl from, when no\nparticular URLs are specified. So, the first pages downloaded will be those\nlisted here. The subsequent URLs will be generated successively from data\ncontained in the start URLs.\n\nstart_requests()\u00b6\nThis method must return an iterable with the first Requests to crawl for\nthis spider.\nThis is the method called by Scrapy when the spider is opened for\nscraping when no particular URLs are specified. If particular URLs are\nspecified, the make_requests_from_url() is used instead to create\nthe Requests. This method is also called only once from Scrapy, so it\u2019s\nsafe to implement it as a generator.\nThe default implementation uses make_requests_from_url() to\ngenerate Requests for each url in start_urls.\nIf you want to change the Requests used to start scraping a domain, this is\nthe method to override. For example, if you need to start by logging in using\na POST request, you could do:\ndef start_requests(self):\n    return [FormRequest(\"http://www.example.com/login\",\n                        formdata={'user': 'john', 'pass': 'secret'},\n                        callback=self.logged_in)]\n\ndef logged_in(self, response):\n    # here you would extract links to follow and return Requests for\n    # each of them, with another callback\n    pass\n\n\n\nmake_requests_from_url(url)\u00b6\nA method that receives a URL and returns a Request\nobject (or a list of Request objects) to scrape. This\nmethod is used to construct the initial requests in the\nstart_requests() method, and is typically used to convert urls to\nrequests.\nUnless overridden, this method returns Requests with the parse()\nmethod as their callback function, and with dont_filter parameter enabled\n(see Request class for more info).\n\nparse(response)\u00b6\nThis is the default callback used by Scrapy to process downloaded\nresponses, when their requests don\u2019t specify a callback.\nThe parse method is in charge of processing the response and returning\nscraped data and/or more URLs to follow. Other Requests callbacks have\nthe same requirements as the BaseSpider class.\nThis method, as well as any other Request callback, must return an\niterable of Request and/or\nItem objects.\nParameters:response (:class:~scrapy.http.Response`) \u2013 the response to parse\n\nlog(message[, level, component])\u00b6\nLog a message using the scrapy.log.msg() function, automatically\npopulating the spider argument with the name of this\nspider. For more information see Logging.\n\nBaseSpider example\u00b6\nLet\u2019s see an example:\nfrom scrapy import log # This module is useful for printing out debug information\nfrom scrapy.spider import BaseSpider\n\nclass MySpider(BaseSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = [\n        'http://www.example.com/1.html',\n        'http://www.example.com/2.html',\n        'http://www.example.com/3.html',\n    ]\n\n    def parse(self, response):\n        self.log('A response from %s just arrived!' % response.url)\n\n\nAnother example returning multiples Requests and Items from a single callback:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\nfrom myproject.items import MyItem\n\nclass MySpider(BaseSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = [\n        'http://www.example.com/1.html',\n        'http://www.example.com/2.html',\n        'http://www.example.com/3.html',\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        for h3 in hxs.select('//h3').extract():\n            yield MyItem(title=h3)\n\n        for url in hxs.select('//a/@href').extract():\n            yield Request(url, callback=self.parse)\n\n\n\n\n\nCrawlSpider\u00b6\n\nclass scrapy.contrib.spiders.CrawlSpider\u00b6\nThis is the most commonly used spider for crawling regular websites, as it\nprovides a convenient mechanism for following links by defining a set of rules.\nIt may not be the best suited for your particular web sites or project, but\nit\u2019s generic enough for several cases, so you can start from it and override it\nas needed for more custom functionality, or just implement your own spider.\nApart from the attributes inherited from BaseSpider (that you must\nspecify), this class supports a new attribute:\n\nrules\u00b6\nWhich is a list of one (or more) Rule objects.  Each Rule\ndefines a certain behaviour for crawling the site. Rules objects are\ndescribed below. If multiple rules match the same link, the first one\nwill be used, according to the order they\u2019re defined in this attribute.\n\nCrawling rules\u00b6\n\nclass scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)\u00b6\nlink_extractor is a Link Extractor object which\ndefines how links will be extracted from each crawled page.\ncallback is a callable or a string (in which case a method from the spider\nobject with that name will be used) to be called for each link extracted with\nthe specified link_extractor. This callback receives a response as its first\nargument and must return a list containing Item and/or\nRequest objects (or any subclass of them).\n\nWarning\nWhen writing crawl spider rules, avoid using parse as\ncallback, since the CrawlSpider uses the parse method\nitself to implement its logic. So if you override the parse method,\nthe crawl spider will no longer work.\n\ncb_kwargs is a dict containing the keyword arguments to be passed to the\ncallback function\nfollow is a boolean which specifies if links should be followed from each\nresponse extracted with this rule. If callback is None follow defaults\nto True, otherwise it default to False.\nprocess_links is a callable, or a string (in which case a method from the\nspider object with that name will be used) which will be called for each list\nof links extracted from each response using the specified link_extractor.\nThis is mainly used for filtering purposes.\nprocess_request is a callable, or a string (in which case a method from\nthe spider object with that name will be used) which will be called with\nevery request extracted by this rule, and must return a request or None (to\nfilter out the request).\n\n\nCrawlSpider example\u00b6\nLet\u2019s now take a look at an example CrawlSpider with rules:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.item import Item\n\nclass MySpider(CrawlSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com']\n\n    rules = (\n        # Extract links matching 'category.php' (but not matching 'subsection.php')\n        # and follow links from them (since no callback means follow=True by default).\n        Rule(SgmlLinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))),\n\n        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n        Rule(SgmlLinkExtractor(allow=('item\\.php', )), callback='parse_item'),\n    )\n\n    def parse_item(self, response):\n        self.log('Hi, this is an item page! %s' % response.url)\n\n        hxs = HtmlXPathSelector(response)\n        item = Item()\n        item['id'] = hxs.select('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)')\n        item['name'] = hxs.select('//td[@id=\"item_name\"]/text()').extract()\n        item['description'] = hxs.select('//td[@id=\"item_description\"]/text()').extract()\n        return item\n\n\nThis spider would start crawling example.com\u2019s home page, collecting category\nlinks, and item links, parsing the latter with the parse_item method. For\neach item response, some data will be extracted from the HTML using XPath, and\na Item will be filled with it.\n\n\n\nXMLFeedSpider\u00b6\n\nclass scrapy.contrib.spiders.XMLFeedSpider\u00b6\nXMLFeedSpider is designed for parsing XML feeds by iterating through them by a\ncertain node name.  The iterator can be chosen from: iternodes, xml,\nand html.  It\u2019s recommended to use the iternodes iterator for\nperformance reasons, since the xml and html iterators generate the\nwhole DOM at once in order to parse it.  However, using html as the\niterator may be useful when parsing XML with bad markup.\nTo set the iterator and the tag name, you must define the following class\nattributes:\n\niterator\u00b6\nA string which defines the iterator to use. It can be either:\n\n'iternodes' - a fast iterator based on regular expressions\n'html' - an iterator which uses HtmlXPathSelector. Keep in mind\nthis uses DOM parsing and must load all DOM in memory which could be a\nproblem for big feeds\n'xml' - an iterator which uses XmlXPathSelector. Keep in mind\nthis uses DOM parsing and must load all DOM in memory which could be a\nproblem for big feeds\n\nIt defaults to: 'iternodes'.\n\nitertag\u00b6\nA string with the name of the node (or element) to iterate in. Example:\nitertag = 'product'\n\n\n\nnamespaces\u00b6\nA list of (prefix, uri) tuples which define the namespaces\navailable in that document that will be processed with this spider. The\nprefix and uri will be used to automatically register\nnamespaces using the\nregister_namespace() method.\nYou can then specify nodes with namespaces in the itertag\nattribute.\nExample:\nclass YourSpider(XMLFeedSpider):\n\n    namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]\n    itertag = 'n:url'\n    # ...\n\n\nApart from these new attributes, this spider has the following overrideable\nmethods too:\n\nadapt_response(response)\u00b6\nA method that receives the response as soon as it arrives from the spider\nmiddleware, before the spider starts parsing it. It can be used to modify\nthe response body before parsing it. This method receives a response and\nalso returns a response (it could be the same or another one).\n\nparse_node(response, selector)\u00b6\nThis method is called for the nodes matching the provided tag name\n(itertag).  Receives the response and an XPathSelector for each node.\nOverriding this method is mandatory. Otherwise, you spider won\u2019t work.\nThis method must return either a Item object, a\nRequest object, or an iterable containing any of\nthem.\n\nprocess_results(response, results)\u00b6\nThis method is called for each result (item or request) returned by the\nspider, and it\u2019s intended to perform any last time processing required\nbefore returning the results to the framework core, for example setting the\nitem IDs. It receives a list of results and the response which originated\nthose results. It must return a list of results (Items or Requests).\n\nXMLFeedSpider example\u00b6\nThese spiders are pretty easy to use, let\u2019s have a look at one example:\nfrom scrapy import log\nfrom scrapy.contrib.spiders import XMLFeedSpider\nfrom myproject.items import TestItem\n\nclass MySpider(XMLFeedSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com/feed.xml']\n    iterator = 'iternodes' # This is actually unnecessary, since it's the default value\n    itertag = 'item'\n\n    def parse_node(self, response, node):\n        log.msg('Hi, this is a <%s> node!: %s' % (self.itertag, ''.join(node.extract())))\n\n        item = Item()\n        item['id'] = node.select('@id').extract()\n        item['name'] = node.select('name').extract()\n        item['description'] = node.select('description').extract()\n        return item\n\n\nBasically what we did up there was to create a spider that downloads a feed from\nthe given start_urls, and then iterates through each of its item tags,\nprints them out, and stores some random data in an Item.\n\n\n\nCSVFeedSpider\u00b6\n\nclass scrapy.contrib.spiders.CSVFeedSpider\u00b6\nThis spider is very similar to the XMLFeedSpider, except that it iterates\nover rows, instead of nodes. The method that gets called in each iteration\nis parse_row().\n\ndelimiter\u00b6\nA string with the separator character for each field in the CSV file\nDefaults to ',' (comma).\n\nheaders\u00b6\nA list of the rows contained in the file CSV feed which will be used to\nextract fields from it.\n\nparse_row(response, row)\u00b6\nReceives a response and a dict (representing each row) with a key for each\nprovided (or detected) header of the CSV file.  This spider also gives the\nopportunity to override adapt_response and process_results methods\nfor pre- and post-processing purposes.\n\nCSVFeedSpider example\u00b6\nLet\u2019s see an example similar to the previous one, but using a\nCSVFeedSpider:\nfrom scrapy import log\nfrom scrapy.contrib.spiders import CSVFeedSpider\nfrom myproject.items import TestItem\n\nclass MySpider(CSVFeedSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com/feed.csv']\n    delimiter = ';'\n    headers = ['id', 'name', 'description']\n\n    def parse_row(self, response, row):\n        log.msg('Hi, this is a row!: %r' % row)\n\n        item = TestItem()\n        item['id'] = row['id']\n        item['name'] = row['name']\n        item['description'] = row['description']\n        return item\n\n\n\n\n\nSitemapSpider\u00b6\n\nclass scrapy.contrib.spiders.SitemapSpider\u00b6\nSitemapSpider allows you to crawl a site by discovering the URLs using\nSitemaps.\nIt supports nested sitemaps and discovering sitemap urls from\nrobots.txt.\n\nsitemap_urls\u00b6\nA list of urls pointing to the sitemaps whose urls you want to crawl.\nYou can also point to a robots.txt and it will be parsed to extract\nsitemap urls from it.\n\nsitemap_rules\u00b6\nA list of tuples (regex, callback) where:\nregex is a regular expression to match urls extracted from sitemaps.\nregex can be either a str or a compiled regex object.\ncallback is the callback to use for processing the urls that match\nthe regular expression. callback can be a string (indicating the\nname of a spider method) or a callable.\nFor example:\nsitemap_rules = [('/product/', 'parse_product')]\n\n\nRules are applied in order, and only the first one that matches will be\nused.\nIf you omit this attribute, all urls found in sitemaps will be\nprocessed with the parse callback.\n\nsitemap_follow\u00b6\nA list of regexes of sitemap that should be followed. This is is only\nfor sites that use Sitemap index files that point to other sitemap\nfiles.\nBy default, all sitemaps are followed.\n\nSitemapSpider examples\u00b6\nSimplest example: process all urls discovered through sitemaps using the\nparse callback:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/sitemap.xml']\n\n    def parse(self, response):\n        pass # ... scrape item here ...\n\n\nProcess some urls with certain callback and other urls with a different\ncallback:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/sitemap.xml']\n    sitemap_rules = [\n        ('/product/', 'parse_product'),\n        ('/category/', 'parse_category'),\n    ]\n\n    def parse_product(self, response):\n        pass # ... scrape product ...\n\n    def parse_category(self, response):\n        pass # ... scrape category ...\n\n\nFollow sitemaps defined in the robots.txt file and only follow sitemaps\nwhose url contains /sitemap_shop:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/robots.txt']\n    sitemap_rules = [\n        ('/shop/', 'parse_shop'),\n    ]\n    sitemap_follow = ['/sitemap_shops']\n\n    def parse_shop(self, response):\n        pass # ... scrape shop here ...\n\n\nCombine SitemapSpider with other sources of urls:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/robots.txt']\n    sitemap_rules = [\n        ('/shop/', 'parse_shop'),\n    ]\n\n    other_urls = ['http://www.example.com/about']\n\n    def start_requests(self):\n        requests = list(super(MySpider, self).start_requests())\n        requests += [Request(x, callback=self.parse_other) for x in self.other_urls]\n        return requests\n\n    def parse_shop(self, response):\n        pass # ... scrape shop here ...\n\n    def parse_other(self, response):\n        pass # ... scrape other here ...\n\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/spiders.html", "title": ["Spiders \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nLink Extractors\u00b6\nLinkExtractors are objects whose only purpose is to extract links from web\npages (scrapy.http.Response objects) which will be eventually\nfollowed.\nThere are two Link Extractors available in Scrapy by default, but you create\nyour own custom Link Extractors to suit your needs by implementing a simple\ninterface.\nThe only public method that every LinkExtractor has is extract_links,\nwhich receives a Response object and returns a list\nof links. Link Extractors are meant to be instantiated once and their\nextract_links method called several times with different responses, to\nextract links to follow.\nLink extractors are used in the CrawlSpider\nclass (available in Scrapy), through a set of rules, but you can also use it in\nyour spiders, even if you don\u2019t subclass from\nCrawlSpider, as its purpose is very simple: to\nextract links.\n\nBuilt-in link extractors reference\u00b6\nAll available link extractors classes bundled with Scrapy are provided in the\nscrapy.contrib.linkextractors module.\n\nSgmlLinkExtractor\u00b6\n\nclass scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None)\u00b6\nThe SgmlLinkExtractor extends the base BaseSgmlLinkExtractor by\nproviding additional filters that you can specify to extract links,\nincluding regular expressions patterns that the links must match to be\nextracted. All those filters are configured through these constructor\nparameters:\nParameters:allow (a regular expression (or list of)) \u2013 a single regular expression (or list of regular expressions)\nthat the (absolute) urls must match in order to be extracted. If not\ngiven (or empty), it will match all links.\ndeny (a regular expression (or list of)) \u2013 a single regular expression (or list of regular expressions)\nthat the (absolute) urls must match in order to be excluded (ie. not\nextracted). It has precedence over the allow parameter. If not\ngiven (or empty) it won\u2019t exclude any links.\nallow_domains (str or list) \u2013 a single value or a list of string containing\ndomains which will be considered for extracting the links\ndeny_domains (str or list) \u2013 a single value or a list of strings containing\ndomains which won\u2019t be considered for extracting the links\ndeny_extensions (list) \u2013 a list of extensions that should be ignored when\nextracting links. If not given, it will default to the\nIGNORED_EXTENSIONS list defined in the scrapy.linkextractor\nmodule.\nrestrict_xpaths (str or list) \u2013 is a XPath (or list of XPath\u2019s) which defines\nregions inside the response where links should be extracted from.\nIf given, only the text selected by those XPath will be scanned for\nlinks. See examples below.\ntags (str or list) \u2013 a tag or a list of tags to consider when extracting links.\nDefaults to ('a', 'area').\nattrs (list) \u2013 list of attributes which should be considered when looking\nfor links to extract (only for those tags specified in the tags\nparameter). Defaults to ('href',)\ncanonicalize (boolean) \u2013 canonicalize each extracted url (using\nscrapy.utils.url.canonicalize_url). Defaults to True.\nunique (boolean) \u2013 whether duplicate filtering should be applied to extracted\nlinks.\nprocess_value (callable) \u2013 see process_value argument of\nBaseSgmlLinkExtractor class constructor\n\n\n\nBaseSgmlLinkExtractor\u00b6\n\nclass scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor(tag=\"a\", attr=\"href\", unique=False, process_value=None)\u00b6\nThe purpose of this Link Extractor is only to serve as a base class for the\nSgmlLinkExtractor. You should use that one instead.\nThe constructor arguments are:\nParameters:tag (str or callable) \u2013 either a string (with the name of a tag) or a function that\nreceives a tag name and returns True if links should be extracted from\nthat tag, or False if they shouldn\u2019t. Defaults to 'a'.  request\n(once it\u2019s downloaded) as its first parameter. For more information, see\nPassing additional data to callback functions.\nattr (str or callable) \u2013 either string (with the name of a tag attribute), or a\nfunction that receives an attribute name and returns True if\nlinks should be extracted from it, or False if they shouldn\u2019t.\nDefaults to href.\nunique (boolean) \u2013 is a boolean that specifies if a duplicate filtering should\nbe applied to links extracted.\nprocess_value (callable) \u2013 a function which receives each value extracted from\nthe tag and attributes scanned and can modify the value and return a\nnew one, or return None to ignore the link altogether. If not\ngiven, process_value defaults to lambda x: x.\nFor example, to extract links from this code:\n<a href=\"javascript:goToPage('../other/page.html'); return false\">Link text</a>\n\n\nYou can use the following function in process_value:\ndef process_value(value):\n    m = re.search(\"javascript:goToPage\\('(.*?)'\", value)\n    if m:\n        return m.group(1)\n\n\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/link-extractors.html", "title": ["Link Extractors \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nLogging\u00b6\nScrapy provides a logging facility which can be used through the\nscrapy.log module. The current underlying implementation uses Twisted\nlogging but this may change in the future.\nThe logging service must be explicitly started through the scrapy.log.start() function.\n\nLog levels\u00b6\nScrapy provides 5 logging levels:\nCRITICAL - for critical errors\nERROR - for regular errors\nWARNING - for warning messages\nINFO - for informational messages\nDEBUG - for debugging messages\n\n\nHow to set the log level\u00b6\nYou can set the log level using the \u2013loglevel/-L command line option, or\nusing the LOG_LEVEL setting.\n\n\nHow to log messages\u00b6\nHere\u2019s a quick example of how to log a message using the WARNING level:\nfrom scrapy import log\nlog.msg(\"This is a warning\", level=log.WARNING)\n\n\n\n\nLogging from Spiders\u00b6\nThe recommended way to log from spiders is by using the Spider\nlog() method, which already populates the\nspider argument of the scrapy.log.msg() function. The other arguments\nare passed directly to the msg() function.\n\n\nscrapy.log module\u00b6\n\nscrapy.log.start(logfile=None, loglevel=None, logstdout=None)\u00b6\nStart the logging facility. This must be called before actually logging any\nmessages. Otherwise, messages logged before this call will get lost.\nParameters:logfile (str) \u2013 the file path to use for logging output. If omitted, the\nLOG_FILE setting will be used. If both are None, the log\nwill be sent to standard error.\nloglevel \u2013 the minimum logging level to log. Available values are:\nCRITICAL, ERROR, WARNING, INFO and\nDEBUG.\nlogstdout (boolean) \u2013 if True, all standard output (and error) of your\napplication will be logged instead. For example if you \u201cprint \u2018hello\u2019\u201d\nit will appear in the Scrapy log. If omitted, the LOG_STDOUT\nsetting will be used.\n\n\nscrapy.log.msg(message, level=INFO, spider=None)\u00b6\nLog a message\nParameters:message (str) \u2013 the message to log\nlevel \u2013 the log level for this message. See\nLog levels.\nspider (BaseSpider object) \u2013 the spider to use for logging this message. This parameter\nshould always be used when logging things related to a particular\nspider.\n\n\nscrapy.log.CRITICAL\u00b6\nLog level for critical errors\n\nscrapy.log.ERROR\u00b6\nLog level for errors\n\nscrapy.log.WARNING\u00b6\nLog level for warnings\n\nscrapy.log.INFO\u00b6\nLog level for informational messages (recommended level for production\ndeployments)\n\nscrapy.log.DEBUG\u00b6\nLog level for debugging messages (recommended level for development)\n\n\nLogging settings\u00b6\nThese settings can be used to configure the logging:\nLOG_ENABLED\nLOG_ENCODING\nLOG_FILE\nLOG_LEVEL\nLOG_STDOUT\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/logging.html", "title": ["Logging \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nStats Collection\u00b6\nScrapy provides a convenient facility for collecting stats in the form of\nkey/values, where values are often counters. The facility is called the Stats\nCollector, and can be accessed through the stats\nattribute of the Crawler API, as illustrated by the examples in\nthe Common Stats Collector uses section below.\nHowever, the Stats Collector is always available, so you can always import it\nin your module and use its API (to increment or set new stat keys), regardless\nof whether the stats collection is enabled or not. If it\u2019s disabled, the API\nwill still work but it won\u2019t collect anything. This is aimed at simplifying the\nstats collector usage: you should spend no more than one line of code for\ncollecting stats in your spider, Scrapy extension, or whatever code you\u2019re\nusing the Stats Collector from.\nAnother feature of the Stats Collector is that it\u2019s very efficient (when\nenabled) and extremely efficient (almost unnoticeable) when disabled.\nThe Stats Collector keeps a stats table per open spider which is automatically\nopened when the spider is opened, and closed when the spider is closed.\n\nCommon Stats Collector uses\u00b6\nAccess the stats collector through the stats\nattribute. Here is an example of an extension that access stats:\nclass ExtensionThatAccessStats(object):\n\n    def __init__(self, stats):\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.stats)\n\n\nSet stat value:\nstats.set_value('hostname', socket.gethostname())\n\n\nIncrement stat value:\nstats.inc_value('pages_crawled')\n\n\nSet stat value only if greater than previous:\nstats.max_value('max_items_scraped', value)\n\n\nSet stat value only if lower than previous:\nstats.min_value('min_free_memory_percent', value)\n\n\nGet stat value:\n>>> stats.get_value('pages_crawled')\n8\n\n\nGet all stats:\n>>> stats.get_stats()\n{'pages_crawled': 1238, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}\n\n\n\n\nAvailable Stats Collectors\u00b6\nBesides the basic StatsCollector there are other Stats Collectors\navailable in Scrapy which extend the basic Stats Collector. You can select\nwhich Stats Collector to use through the STATS_CLASS setting. The\ndefault Stats Collector used is the MemoryStatsCollector.\n\nMemoryStatsCollector\u00b6\n\nclass scrapy.statscol.MemoryStatsCollector\u00b6\nA simple stats collector that keeps the stats of the last scraping run (for\neach spider) in memory, after they\u2019re closed. The stats can be accessed\nthrough the spider_stats attribute, which is a dict keyed by spider\ndomain name.\nThis is the default Stats Collector used in Scrapy.\n\nspider_stats\u00b6\nA dict of dicts (keyed by spider name) containing the stats of the last\nscraping run for each spider.\n\n\nDummyStatsCollector\u00b6\n\nclass scrapy.statscol.DummyStatsCollector\u00b6\nA Stats collector which does nothing but is very efficient (because it does\nnothing). This stats collector can be set via the STATS_CLASS\nsetting, to disable stats collect in order to improve performance. However,\nthe performance penalty of stats collection is usually marginal compared to\nother Scrapy workload like parsing pages.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/stats.html", "title": ["Stats Collection \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSending e-mail\u00b6\nAlthough Python makes sending e-mails relatively easy via the smtplib\nlibrary, Scrapy provides its own facility for sending e-mails which is very\neasy to use and it\u2019s implemented using Twisted non-blocking IO, to avoid\ninterfering with the non-blocking IO of the crawler. It also provides a\nsimple API for sending attachments and it\u2019s very easy to configure, with a few\nsettings.\n\nQuick example\u00b6\nThere are two ways to instantiate the mail sender. You can instantiate it using\nthe standard constructor:\nfrom scrapy.mail import MailSender\nmailer = MailSender()\n\n\nOr you can instantiate it passing a Scrapy settings object, which will respect\nthe settings:\nmailer = MailSender.from_settings(settings)\n\n\nAnd here is how to use it to send an e-mail (without attachments):\nmailer.send(to=[\"someone@example.com\"], subject=\"Some subject\", body=\"Some body\", cc=[\"another@example.com\"])\n\n\n\n\nMailSender class reference\u00b6\nMailSender is the preferred class to use for sending emails from Scrapy, as it\nuses Twisted non-blocking IO, like the rest of the framework.\n\nclass scrapy.mail.MailSender(smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, smtpport=None)\u00b6\nParameters:smtphost (str) \u2013 the SMTP host to use for sending the emails. If omitted, the\nMAIL_HOST setting will be used.\nmailfrom (str) \u2013 the address used to send emails (in the From: header).\nIf omitted, the MAIL_FROM setting will be used.\nsmtpuser \u2013 the SMTP user. If omitted, the MAIL_USER\nsetting will be used. If not given, no SMTP authentication will be\nperformed.\nsmtppass (str) \u2013 the SMTP pass for authentication.\nsmtpport (int) \u2013 the SMTP port to connect to\n\n\nclassmethod from_settings(settings)\u00b6\nInstantiate using a Scrapy settings object, which will respect\nthese Scrapy settings.\nParameters:settings (scrapy.settings.Settings object) \u2013 the e-mail recipients\n\nsend(to, subject, body, cc=None, attachs=())\u00b6\nSend email to the given recipients.\nParameters:to (list) \u2013 the e-mail recipients\nsubject (str) \u2013 the subject of the e-mail\ncc (list) \u2013 the e-mails to CC\nbody (str) \u2013 the e-mail body\nattachs (iterable) \u2013 an iterable of tuples (attach_name, mimetype,\nfile_object) where  attach_name is a string with the name that will\nappear on the e-mail\u2019s attachment, mimetype is the mimetype of the\nattachment and file_object is a readable file object with the\ncontents of the attachment\n\n\n\nMail settings\u00b6\nThese settings define the default constructor values of the MailSender\nclass, and can be used to configure e-mail notifications in your project without\nwriting any code (for those extensions and code that uses MailSender).\n\nMAIL_FROM\u00b6\nDefault: 'scrapy@localhost'\nSender email to use (From: header) for sending emails.\n\n\nMAIL_HOST\u00b6\nDefault: 'localhost'\nSMTP host to use for sending emails.\n\n\nMAIL_PORT\u00b6\nDefault: 25\nSMTP port to use for sending emails.\n\n\nMAIL_USER\u00b6\nDefault: None\nUser to use for SMTP authentication. If disabled no SMTP authentication will be\nperformed.\n\n\nMAIL_PASS\u00b6\nDefault: None\nPassword to use for SMTP authentication, along with MAIL_USER.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/email.html", "title": ["Sending e-mail \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nTelnet Console\u00b6\nScrapy comes with a built-in telnet console for inspecting and controlling a\nScrapy running process. The telnet console is just a regular python shell\nrunning inside the Scrapy process, so you can do literally anything from it.\nThe telnet console is a built-in Scrapy extension which comes enabled by default, but you can also\ndisable it if you want. For more information about the extension itself see\nTelnet console extension.\n\nHow to access the telnet console\u00b6\nThe telnet console listens in the TCP port defined in the\nTELNETCONSOLE_PORT setting, which defaults to 6023. To access\nthe console you need to type:\ntelnet localhost 6023\n>>>\n\n\nYou need the telnet program which comes installed by default in Windows, and\nmost Linux distros.\n\n\nAvailable variables in the telnet console\u00b6\nThe telnet console is like a regular Python shell running inside the Scrapy\nprocess, so you can do anything from it including importing new modules, etc.\nHowever, the telnet console comes with some default variables defined for\nconvenience:\nShortcut\nDescription\ncrawler\nthe Scrapy Crawler (scrapy.crawler.Crawler object)\nengine\nCrawler.engine attribute\nspider\nthe active spider\nslot\nthe engine slot\nextensions\nthe Extension Manager (Crawler.extensions attribute)\nstats\nthe Stats Collector (Crawler.stats attribute)\nsettings\nthe Scrapy settings object (Crawler.settings attribute)\nest\nprint a report of the engine status\nprefs\nfor memory debugging (see Debugging memory leaks)\np\na shortcut to the pprint.pprint function\nhpy\nfor memory debugging (see Debugging memory leaks)\n\n\nTelnet console usage examples\u00b6\nHere are some example tasks you can do with the telnet console:\n\nView engine status\u00b6\nYou can use the est() method of the Scrapy engine to quickly show its state\nusing the telnet console:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 21.3188259602\nengine.is_idle()                                : False\nengine.has_capacity()                           : True\nengine.scheduler.is_idle()                      : False\nlen(engine.scheduler.pending_requests)          : 1\nengine.downloader.is_idle()                     : False\nlen(engine.downloader.slots)                    : 1\nengine.scraper.is_idle()                        : False\nlen(engine.scraper.slots)                       : 1\n\nSpider: <GayotSpider 'gayotcom' at 0x2dc2b10>\n  engine.spider_is_idle(spider)                      : False\n  engine.slots[spider].closing                       : False\n  len(engine.scheduler.pending_requests[spider])     : 11504\n  len(engine.downloader.slots[spider].queue)         : 9\n  len(engine.downloader.slots[spider].active)        : 17\n  len(engine.downloader.slots[spider].transferring)  : 8\n  engine.downloader.slots[spider].lastseen           : 1311311093.61\n  len(engine.scraper.slots[spider].queue)            : 0\n  len(engine.scraper.slots[spider].active)           : 0\n  engine.scraper.slots[spider].active_size           : 0\n  engine.scraper.slots[spider].itemproc_size         : 0\n  engine.scraper.slots[spider].needs_backout()       : False\n\n\n\n\nPause, resume and stop the Scrapy engine\u00b6\nTo pause:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\nTo resume:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\nTo stop:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet Console signals\u00b6\n\nscrapy.telnet.update_telnet_vars(telnet_vars)\u00b6\nSent just before the telnet console is opened. You can hook up to this\nsignal to add, remove or update the variables that will be available in the\ntelnet local namespace. In order to do that, you need to update the\ntelnet_vars dict in your handler.\nParameters:telnet_vars (dict) \u2013 the dict of telnet variables\n\n\nTelnet settings\u00b6\nThese are the settings that control the telnet console\u2019s behaviour:\n\nTELNETCONSOLE_PORT\u00b6\nDefault: [6023, 6073]\nThe port range to use for the telnet console. If set to None or 0, a\ndynamically assigned port is used.\n\n\nTELNETCONSOLE_HOST\u00b6\nDefault: '0.0.0.0'\nThe interface the telnet console should listen on\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/telnetconsole.html", "title": ["Telnet Console \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nWeb Service\u00b6\nScrapy comes with a built-in web service for monitoring and controlling a\nrunning crawler. The service exposes most resources using the JSON-RPC 2.0\nprotocol, but there are also other (read-only) resources which just output JSON\ndata.\nProvides an extensible web service for managing a Scrapy process. It\u2019s enabled\nby the WEBSERVICE_ENABLED setting. The web server will listen in the\nport specified in WEBSERVICE_PORT, and will log to the file\nspecified in WEBSERVICE_LOGFILE.\nThe web service is a built-in Scrapy extension\nwhich comes enabled by default, but you can also disable it if you\u2019re running\ntight on memory.\n\nWeb service resources\u00b6\nThe web service contains several resources, defined in the\nWEBSERVICE_RESOURCES setting. Each resource provides a different\nfunctionality. See Available JSON-RPC resources for a list of\nresources available by default.\nAlthough you can implement your own resources using any protocol, there are\ntwo kinds of resources bundled with Scrapy:\nSimple JSON resources - which are read-only and just output JSON data\nJSON-RPC resources - which provide direct access to certain Scrapy objects\nusing the JSON-RPC 2.0 protocol\n\nAvailable JSON-RPC resources\u00b6\nThese are the JSON-RPC resources available by default in Scrapy:\n\nCrawler JSON-RPC resource\u00b6\n\nclass scrapy.contrib.webservice.crawler.CrawlerResource\u00b6\nProvides access to the main Crawler object that controls the Scrapy\nprocess.\nAvailable by default at: http://localhost:6080/crawler\n\n\nStats Collector JSON-RPC resource\u00b6\n\nclass scrapy.contrib.webservice.stats.StatsResource\u00b6\nProvides access to the Stats Collector used by the crawler.\nAvailable by default at: http://localhost:6080/stats\n\n\nSpider Manager JSON-RPC resource\u00b6\nYou can access the spider manager JSON-RPC resource through the\nCrawler JSON-RPC resource at: http://localhost:6080/crawler/spiders\n\n\nExtension Manager JSON-RPC resource\u00b6\nYou can access the extension manager JSON-RPC resource through the\nCrawler JSON-RPC resource at: http://localhost:6080/crawler/spiders\n\n\n\nAvailable JSON resources\u00b6\nThese are the JSON resources available by default:\n\nEngine status JSON resource\u00b6\n\nclass scrapy.contrib.webservice.enginestatus.EngineStatusResource\u00b6\nProvides access to engine status metrics.\nAvailable by default at: http://localhost:6080/enginestatus\n\n\n\n\nWeb service settings\u00b6\nThese are the settings that control the web service behaviour:\n\nWEBSERVICE_ENABLED\u00b6\nDefault: True\nA boolean which specifies if the web service will be enabled (provided its\nextension is also enabled).\n\n\nWEBSERVICE_LOGFILE\u00b6\nDefault: None\nA file to use for logging HTTP requests made to the web service. If unset web\nthe log is sent to standard scrapy log.\n\n\nWEBSERVICE_PORT\u00b6\nDefault: [6080, 7030]\nThe port range to use for the web service. If set to None or 0, a\ndynamically assigned port is used.\n\n\nWEBSERVICE_HOST\u00b6\nDefault: '0.0.0.0'\nThe interface the web service should listen on\n\n\nWEBSERVICE_RESOURCES\u00b6\nDefault: {}\nThe list of web service resources enabled for your project. See\nWeb service resources. These are added to the ones available by\ndefault in Scrapy, defined in the WEBSERVICE_RESOURCES_BASE setting.\n\n\nWEBSERVICE_RESOURCES_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n    'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n    'scrapy.contrib.webservice.stats.StatsResource': 1,\n}\n\n\nThe list of web service resources available by default in Scrapy. You shouldn\u2019t\nchange this setting in your project, change WEBSERVICE_RESOURCES\ninstead. If you want to disable some resource set its value to None in\nWEBSERVICE_RESOURCES.\n\n\n\nWriting a web service resource\u00b6\nWeb service resources are implemented using the Twisted Web API. See this\nTwisted Web guide for more information on Twisted web and Twisted web\nresources.\nTo write a web service resource you should subclass the JsonResource or\nJsonRpcResource classes and implement the renderGET method.\n\nclass scrapy.webservice.JsonResource\u00b6\nA subclass of twisted.web.resource.Resource that implements a JSON web\nservice resource. See\n\nws_name\u00b6\nThe name by which the Scrapy web service will known this resource, and\nalso the path where this resource will listen. For example, assuming\nScrapy web service is listening on http://localhost:6080/ and the\nws_name is 'resource1' the URL for that resource will be:\n\nhttp://localhost:6080/resource1/\n\nclass scrapy.webservice.JsonRpcResource(crawler, target=None)\u00b6\nThis is a subclass of JsonResource for implementing JSON-RPC\nresources. JSON-RPC resources wrap Python (Scrapy) objects around a\nJSON-RPC API. The resource wrapped must be returned by the\nget_target() method, which returns the target passed in the\nconstructor by default\n\nget_target()\u00b6\nReturn the object wrapped by this JSON-RPC resource. By default, it\nreturns the object passed on the constructor.\n\n\nExamples of web service resources\u00b6\n\nStatsResource (JSON-RPC resource)\u00b6\nfrom scrapy.webservice import JsonRpcResource\n\nclass StatsResource(JsonRpcResource):\n\n    ws_name = 'stats'\n\n    def __init__(self, crawler):\n        JsonRpcResource.__init__(self, crawler, crawler.stats)\n\n\n\n\nEngineStatusResource (JSON resource)\u00b6\nfrom scrapy.webservice import JsonResource\nfrom scrapy.utils.engine import get_engine_status\n\nclass EngineStatusResource(JsonResource):\n\n    ws_name = 'enginestatus'\n\n    def __init__(self, crawler, spider_name=None):\n        JsonResource.__init__(self, crawler)\n        self._spider_name = spider_name\n        self.isLeaf = spider_name is not None\n\n    def render_GET(self, txrequest):\n        status = get_engine_status(self.crawler.engine)\n        if self._spider_name is None:\n            return status\n        for sp, st in status['spiders'].items():\n            if sp.name == self._spider_name:\n                return st\n\n    def getChild(self, name, txrequest):\n        return EngineStatusResource(name, self.crawler)\n\n\n\n\n\nExample of web service client\u00b6\n\nscrapy-ws.py script\u00b6\n#!/usr/bin/env python\n\"\"\"\nExample script to control a Scrapy server using its JSON-RPC web service.\n\nIt only provides a reduced functionality as its main purpose is to illustrate\nhow to write a web service client. Feel free to improve or write you own.\n\nAlso, keep in mind that the JSON-RPC API is not stable. The recommended way for\ncontrolling a Scrapy server is through the execution queue (see the \"queue\"\ncommand).\n\n\"\"\"\n\nimport sys, optparse, urllib, json\nfrom urlparse import urljoin\n\nfrom scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError\n\ndef get_commands():\n    return {\n        'help': cmd_help,\n        'stop': cmd_stop,\n        'list-available': cmd_list_available,\n        'list-running': cmd_list_running,\n        'list-resources': cmd_list_resources,\n        'get-global-stats': cmd_get_global_stats,\n        'get-spider-stats': cmd_get_spider_stats,\n    }\n\ndef cmd_help(args, opts):\n    \"\"\"help - list available commands\"\"\"\n    print \"Available commands:\"\n    for _, func in sorted(get_commands().items()):\n        print \"  \", func.__doc__\n\ndef cmd_stop(args, opts):\n    \"\"\"stop <spider> - stop a running spider\"\"\"\n    jsonrpc_call(opts, 'crawler/engine', 'close_spider', args[0])\n\ndef cmd_list_running(args, opts):\n    \"\"\"list-running - list running spiders\"\"\"\n    for x in json_get(opts, 'crawler/engine/open_spiders'):\n        print x\n\ndef cmd_list_available(args, opts):\n    \"\"\"list-available - list name of available spiders\"\"\"\n    for x in jsonrpc_call(opts, 'crawler/spiders', 'list'):\n        print x\n\ndef cmd_list_resources(args, opts):\n    \"\"\"list-resources - list available web service resources\"\"\"\n    for x in json_get(opts, '')['resources']:\n        print x\n\ndef cmd_get_spider_stats(args, opts):\n    \"\"\"get-spider-stats <spider> - get stats of a running spider\"\"\"\n    stats = jsonrpc_call(opts, 'stats', 'get_stats', args[0])\n    for name, value in stats.items():\n        print \"%-40s %s\" % (name, value)\n\ndef cmd_get_global_stats(args, opts):\n    \"\"\"get-global-stats - get global stats\"\"\"\n    stats = jsonrpc_call(opts, 'stats', 'get_stats')\n    for name, value in stats.items():\n        print \"%-40s %s\" % (name, value)\n\ndef get_wsurl(opts, path):\n    return urljoin(\"http://%s:%s/\"% (opts.host, opts.port), path)\n\ndef jsonrpc_call(opts, path, method, *args, **kwargs):\n    url = get_wsurl(opts, path)\n    return jsonrpc_client_call(url, method, *args, **kwargs)\n\ndef json_get(opts, path):\n    url = get_wsurl(opts, path)\n    return json.loads(urllib.urlopen(url).read())\n\ndef parse_opts():\n    usage = \"%prog [options] <command> [arg] ...\"\n    description = \"Scrapy web service control script. Use '%prog help' \" \\\n        \"to see the list of available commands.\"\n    op = optparse.OptionParser(usage=usage, description=description)\n    op.add_option(\"-H\", dest=\"host\", default=\"localhost\", \\\n        help=\"Scrapy host to connect to\")\n    op.add_option(\"-P\", dest=\"port\", type=\"int\", default=6080, \\\n        help=\"Scrapy port to connect to\")\n    opts, args = op.parse_args()\n    if not args:\n        op.print_help()\n        sys.exit(2)\n    cmdname, cmdargs, opts = args[0], args[1:], opts\n    commands = get_commands()\n    if cmdname not in commands:\n        sys.stderr.write(\"Unknown command: %s\\n\\n\" % cmdname)\n        cmd_help(None, None)\n        sys.exit(1)\n    return commands[cmdname], cmdargs, opts\n\ndef main():\n    cmd, args, opts = parse_opts()\n    try:\n        cmd(args, opts)\n    except IndexError:\n        print cmd.__doc__\n    except JsonRpcError, e:\n        print str(e)\n        if e.data:\n            print \"Server Traceback below:\"\n            print e.data\n\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/webservice.html", "title": ["Web Service \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nFrequently Asked Questions\u00b6\n\nHow does Scrapy compare to BeautifulSoup or lxml?\u00b6\nBeautifulSoup and lxml are libraries for parsing HTML and XML. Scrapy is\nan application framework for writing web spiders that crawl web sites and\nextract data from them.\nScrapy provides a built-in mechanism for extracting data (called\nselectors) but you can easily use BeautifulSoup\n(or lxml) instead, if you feel more comfortable working with them. After\nall, they\u2019re just parsing libraries which can be imported and used from any\nPython code.\nIn other words, comparing BeautifulSoup (or lxml) to Scrapy is like\ncomparing jinja2 to Django.\n\n\nWhat Python versions does Scrapy support?\u00b6\nScrapy runs in Python 2.6 and 2.7.\n\n\nDoes Scrapy work with Python 3.0?\u00b6\nNo, and there are no plans to port Scrapy to Python 3.0 yet. At the moment,\nScrapy works with Python 2.6 and 2.7.\n\nSee also\nWhat Python versions does Scrapy support?.\n\n\n\nDid Scrapy \u201csteal\u201d X from Django?\u00b6\nProbably, but we don\u2019t like that word. We think Django is a great open source\nproject and an example to follow, so we\u2019ve used it as an inspiration for\nScrapy.\nWe believe that, if something is already done well, there\u2019s no need to reinvent\nit. This concept, besides being one of the foundations for open source and free\nsoftware, not only applies to software but also to documentation, procedures,\npolicies, etc. So, instead of going through each problem ourselves, we choose\nto copy ideas from those projects that have already solved them properly, and\nfocus on the real problems we need to solve.\nWe\u2019d be proud if Scrapy serves as an inspiration for other projects. Feel free\nto steal from us!\n\n\nDoes Scrapy work with HTTP proxies?\u00b6\nYes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP\nProxy downloader middleware. See\nHttpProxyMiddleware.\n\n\nScrapy crashes with: ImportError: No module named win32api\u00b6\nYou need to install pywin32 because of this Twisted bug.\n\n\nHow can I simulate a user login in my spider?\u00b6\nSee Using FormRequest.from_response() to simulate a user login.\n\n\nDoes Scrapy crawl in breath-first or depth-first order?\u00b6\nBy default, Scrapy uses a LIFO queue for storing pending requests, which\nbasically means that it crawls in DFO order. This order is more convenient\nin most cases. If you do want to crawl in true BFO order, you can do it by\nsetting the following settings:\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'\n\n\n\n\nMy Scrapy crawler has memory leaks. What can I do?\u00b6\nSee Debugging memory leaks.\nAlso, Python has a builtin memory leak issue which is described in\nLeaks without leaks.\n\n\nHow can I make Scrapy consume less memory?\u00b6\nSee previous question.\n\n\nCan I use Basic HTTP Authentication in my spiders?\u00b6\nYes, see HttpAuthMiddleware.\n\n\nWhy does Scrapy download pages in English instead of my native language?\u00b6\nTry changing the default Accept-Language request header by overriding the\nDEFAULT_REQUEST_HEADERS setting.\n\n\nWhere can I find some example Scrapy projects?\u00b6\nSee Examples.\n\n\nCan I run a spider without creating a project?\u00b6\nYes. You can use the runspider command. For example, if you have a\nspider written in a my_spider.py file you can run it with:\nscrapy runspider my_spider.py\n\nSee runspider command for more info.\n\n\nI get \u201cFiltered offsite request\u201d messages. How can I fix them?\u00b6\nThose messages (logged with DEBUG level) don\u2019t necessarily mean there is a\nproblem, so you may not need to fix them.\nThose message are thrown by the Offsite Spider Middleware, which is a spider\nmiddleware (enabled by default) whose purpose is to filter out requests to\ndomains outside the ones covered by the spider.\nFor more info see:\nOffsiteMiddleware.\n\n\nWhat is the recommended way to deploy a Scrapy crawler in production?\u00b6\nSee Scrapy Service (scrapyd).\n\n\nCan I use JSON for large exports?\u00b6\nIt\u2019ll depend on how large your output is. See this warning in JsonItemExporter\ndocumentation.\n\n\nCan I return (Twisted) deferreds from signal handlers?\u00b6\nSome signals support returning deferreds from their handlers, others don\u2019t. See\nthe Built-in signals reference to know which ones.\n\n\nWhat does the response status code 999 means?\u00b6\n999 is a custom reponse status code used by Yahoo sites to throttle requests.\nTry slowing down the crawling speed by using a download delay of 2 (or\nhigher) in your spider:\nclass MySpider(CrawlSpider):\n\n    name = 'myspider'\n\n    DOWNLOAD_DELAY = 2\n\n    # [ ... rest of the spider code ... ]\n\n\nOr by setting a global download delay in your project with the\nDOWNLOAD_DELAY setting.\n\n\nCan I call pdb.set_trace() from my spiders to debug them?\u00b6\nYes, but you can also use the Scrapy shell which allows you too quickly analyze\n(and even modify) the response being processed by your spider, which is, quite\noften, more useful than plain old pdb.set_trace().\nFor more info see Invoking the shell from spiders to inspect responses.\n\n\nSimplest way to dump all my scraped items into a JSON/CSV/XML file?\u00b6\nTo dump into a JSON file:\nscrapy crawl myspider -o items.json -t json\n\nTo dump into a CSV file:\nscrapy crawl myspider -o items.csv -t csv\n\nTo dump into a XML file:\nscrapy crawl myspider -o items.xml -t xml\n\nFor more information see Feed exports\n\n\nWhat\u2019s this huge cryptic __VIEWSTATE parameter used in some forms?\u00b6\nThe __VIEWSTATE parameter is used in sites built with ASP.NET/VB.NET. For\nmore info on how it works see this page. Also, here\u2019s an example spider\nwhich scrapes one of these sites.\n\n\nWhat\u2019s the best way to parse big XML/CSV data feeds?\u00b6\nParsing big feeds with XPath selectors can be problematic since they need to\nbuild the DOM of the entire feed in memory, and this can be quite slow and\nconsume a lot of memory.\nIn order to avoid parsing all the entire feed at once in memory, you can use\nthe functions xmliter and csviter from scrapy.utils.iterators\nmodule. In fact, this is what the feed spiders (see Spiders) use\nunder the cover.\n\n\nDoes Scrapy manage cookies automatically?\u00b6\nYes, Scrapy receives and keeps track of cookies sent by servers, and sends them\nback on subsequent requests, like any regular web browser does.\nFor more info see Requests and Responses and CookiesMiddleware.\n\n\nHow can I see the cookies being sent and received from Scrapy?\u00b6\nEnable the COOKIES_DEBUG setting.\n\n\nHow can I instruct a spider to stop itself?\u00b6\nRaise the CloseSpider exception from a callback. For\nmore info see: CloseSpider.\n\n\nHow can I prevent my Scrapy bot from getting banned?\u00b6\nSee Avoiding getting banned.\n\n\nShould I use spider arguments or settings to configure my spider?\u00b6\nBoth spider arguments and settings\ncan be used to configure your spider. There is no strict rule that mandates to\nuse one or the other, but settings are more suited for parameters that, once\nset, don\u2019t change much, while spider arguments are meant to change more often,\neven on each spider run and sometimes are required for the spider to run at all\n(for example, to set the start url of a spider).\nTo illustrate with an example, assuming you have a spider that needs to log\ninto a site to scrape data, and you only want to scrape data from a certain\nsection of the site (which varies each time). In that case, the credentials to\nlog in would be settings, while the url of the section to scrape would be a\nspider argument.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/faq.html", "title": ["Frequently Asked Questions \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDebugging Spiders\u00b6\nThis document explains the most common techniques for debugging spiders.\nConsider the following scrapy spider below:\nclass MySpider(BaseSpider):\n    name = 'myspider'\n    start_urls = (\n        'http://example.com/page1',\n        'http://example.com/page2',\n        )\n\n    def parse(self, response):\n        # collect `item_urls`\n        for item_url in item_urls:\n            yield Request(url=item_url, callback=self.parse_item)\n\n    def parse_item(self, response):\n        item = MyItem()\n        # populate `item` fields\n        yield Request(url=item_details_url, meta={'item': item},\n            callback=self.parse_details)\n\n    def parse_details(self, response):\n        item = response.meta['item']\n        # populate more `item` fields\n        return item\n\n\nBasically this is a simple spider which parses two pages of items (the\nstart_urls). Items also have a details page with additional information, so we\nuse the meta functionality of Request to pass a\npartially populated item.\n\nParse Command\u00b6\nThe most basic way of checking the output of your spider is to use the\nparse command. It allows to check the behaviour of different parts\nof the spider at the method level. It has the advantage of being flexible and\nsimple to use, but does not allow debugging code inside a method.\nIn order to see the item scraped from a specific url:\n$ scrapy parse --spider=myspider -c parse_item -d 2 <item_url>\n[ ... scrapy log lines crawling example.com spider ... ]\n\n>>> STATUS DEPTH LEVEL 2 <<<\n# Scraped Items  ------------------------------------------------------------\n[{'url': <item_url>}]\n\n# Requests  -----------------------------------------------------------------\n[]\n\nUsing the --verbose or -v option we can see the status at each depth level:\n$ scrapy parse --spider=myspider -c parse_item -d 2 -v <item_url>\n[ ... scrapy log lines crawling example.com spider ... ]\n\n>>> DEPTH LEVEL: 1 <<<\n# Scraped Items  ------------------------------------------------------------\n[]\n\n# Requests  -----------------------------------------------------------------\n[<GET item_details_url>]\n\n\n>>> DEPTH LEVEL: 2 <<<\n# Scraped Items  ------------------------------------------------------------\n[{'url': <item_url>}]\n\n# Requests  -----------------------------------------------------------------\n[]\n\nChecking items scraped from a single start_url, can also be easily achieved\nusing:\n$ scrapy parse --spider=myspider -d 3 'http://example.com/page1'\n\n\n\nScrapy Shell\u00b6\nWhile the parse command is very useful for checking behaviour of a\nspider, it is of little help to check what happens inside a callback, besides\nshowing the response received and the output. How to debug the situation when\nparse_details sometimes receives no item?\nFortunately, the shell is your bread and butter in this case (see\nInvoking the shell from spiders to inspect responses):\nfrom scrapy.shell import inspect_response\n\ndef parse_details(self, response):\n    item = response.meta.get('item', None)\n    if item:\n        # populate more `item` fields\n        return item\n    else:\n        inspect_response(response, self)\n\n\nSee also: Invoking the shell from spiders to inspect responses.\n\n\nOpen in browser\u00b6\nSometimes you just want to see how a certain response looks in a browser, you\ncan use the open_in_browser function for that. Here is an example of how\nyou would use it:\nfrom scrapy.utils.response import open_in_browser\n\ndef parse_details(self, response):\n    if \"item name\" not in response.body:\n        open_in_browser(response)\n\n\nopen_in_browser will open a browser with the response received by Scrapy at\nthat point, adjusting the base tag so that images and styles are displayed\nproperly.\n\n\nLogging\u00b6\nLogging is another useful option for getting information about your spider run.\nAlthough not as convenient, it comes with the advantage that the logs will be\navailable in all future runs should they be necessary again:\nfrom scrapy import log\n\ndef parse_details(self, response):\n    item = response.meta.get('item', None)\n    if item:\n        # populate more `item` fields\n        return item\n    else:\n        self.log('No item received for %s' % response.url,\n            level=log.WARNING)\n\n\nFor more information, check the Logging section.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/debug.html", "title": ["Debugging Spiders \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSpiders Contracts\u00b6\n\nNew in version 0.15.\n\nNote\nThis is a new feature (introduced in Scrapy 0.15) and may be subject\nto minor functionality/API updates. Check the release notes to\nbe notified of updates.\n\nTesting spiders can get particularly annoying and while nothing prevents you\nfrom writing unit tests the task gets cumbersome quickly. Scrapy offers an\nintegrated way of testing your spiders by the means of contracts.\nThis allows you to test each callback of your spider by hardcoding a sample url\nand check various constraints for how the callback processes the response. Each\ncontract is prefixed with an @ and included in the docstring. See the\nfollowing example:\ndef parse(self, response):\n    \"\"\" This function parses a sample response. Some contracts are mingled\n    with this docstring.\n\n    @url http://www.amazon.com/s?field-keywords=selfish+gene\n    @returns items 1 16\n    @returns requests 0 0\n    @scrapes Title Author Year Price\n    \"\"\"\n\n\nThis callback is tested using three built-in contracts:\n\nclass scrapy.contracts.default.UrlContract\u00b6\nThis contract (@url) sets the sample url used when checking other\ncontract conditions for this spider. This contract is mandatory. All\ncallbacks lacking this contract are ignored when running the checks:\n@url url\n\n\nclass scrapy.contracts.default.ReturnsContract\u00b6\nThis contract (@returns) sets lower and upper bounds for the items and\nrequests returned by the spider. The upper bound is optional:\n@returns item(s)|request(s) [min [max]]\n\n\nclass scrapy.contracts.default.ScrapesContract\u00b6\nThis contract (@scrapes) checks that all the items returned by the\ncallback have the specified fields:\n@scrapes field_1 field_2 ...\n\nUse the check command to run the contract checks.\n\nCustom Contracts\u00b6\nIf you find you need more power than the built-in scrapy contracts you can\ncreate and load your own contracts in the project by using the\nSPIDER_CONTRACTS setting:\nSPIDER_CONTRACTS = {\n    'myproject.contracts.ResponseCheck': 10,\n    'myproject.contracts.ItemValidate': 10,\n}\n\n\nEach contract must inherit from scrapy.contracts.Contract and can\noverride three methods:\n\nclass scrapy.contracts.Contract(method, *args)\u00b6\nParameters:method (function) \u2013 callback function to which the contract is associated\nargs (list) \u2013 list of arguments passed into the docstring (whitespace\nseparated)\n\n\nadjust_request_args(args)\u00b6\nThis receives a dict as an argument containing default arguments\nfor Request object. Must return the same or a\nmodified version of it.\n\npre_process(response)\u00b6\nThis allows hooking in various checks on the response received from the\nsample request, before it\u2019s being passed to the callback.\n\npost_process(output)\u00b6\nThis allows processing the output of the callback. Iterators are\nconverted listified before being passed to this hook.\nHere is a demo contract which checks the presence of a custom header in the\nresponse received. Raise scrapy.exceptions.ContractFail in order to\nget the failures pretty printed:\nfrom scrapy.contracts import Contract\nfrom scrapy.exceptions import ContractFail\n\nclass HasHeaderContract(Contract):\n    \"\"\" Demo contract which checks the presence of a custom header\n        @has_header X-CustomHeader\n    \"\"\"\n\n    name = 'has_header'\n\n    def pre_process(self, response):\n        for header in self.args:\n            if header not in response.headers:\n                raise ContractFail('X-CustomHeader not present')\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/contracts.html", "title": ["Spiders Contracts \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nCommon Practices\u00b6\nThis section documents common practices when using Scrapy. These are things\nthat cover many topics and don\u2019t often fall into any other specific section.\n\nRun Scrapy from a script\u00b6\nYou can use the API to run Scrapy from a script, instead of\nthe typical way of running Scrapy via scrapy crawl.\nWhat follows is a working example of how to do that, using the testspiders\nproject as example. Remember that Scrapy is built on top of the Twisted\nasynchronous networking library, so you need run it inside the Twisted reactor.\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\n\n\nSee also\nTwisted Reactor Overview.\n\n\n\nRunning multiple spiders in the same process\u00b6\nBy default, Scrapy runs a single spider per process when you run scrapy\ncrawl. However, Scrapy supports running multiple spiders per process using\nthe internal API.\nHere is an example, using the testspiders project:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\ndef setup_crawler(domain):\n    spider = FollowAllSpider(domain=domain)\n    crawler = Crawler(Settings())\n    crawler.configure()\n    crawler.crawl(spider)\n    crawler.start()\n\nfor domain in ['scrapinghub.com', 'insophia.com']:\n    setup_crawler(domain)\nlog.start()\nreactor.run()\n\n\n\nSee also\nRun Scrapy from a script.\n\n\n\nDistributed crawls\u00b6\nScrapy doesn\u2019t provide any built-in facility for running crawls in a distribute\n(multi-server) manner. However, there are some ways to distribute crawls, which\nvary depending on how you plan to distribute them.\nIf you have many spiders, the obvious way to distribute the load is to setup\nmany Scrapyd instances and distribute spider runs among those.\nIf you instead want to run a single (big) spider through many machines, what\nyou usually do is partition the urls to crawl and send them to each separate\nspider. Here is a concrete example:\nFirst, you prepare the list of urls to crawl and put them into separate\nfiles/urls:\nhttp://somedomain.com/urls-to-crawl/spider1/part1.list\nhttp://somedomain.com/urls-to-crawl/spider1/part2.list\nhttp://somedomain.com/urls-to-crawl/spider1/part3.list\n\nThen you fire a spider run on 3 different Scrapyd servers. The spider would\nreceive a (spider) argument part with the number of the partition to\ncrawl:\ncurl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1\ncurl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2\ncurl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3\n\n\n\nAvoiding getting banned\u00b6\nSome websites implement certain measures to prevent bots from crawling them,\nwith varying degrees of sophistication. Getting around those measures can be\ndifficult and tricky, and may sometimes require special infrastructure. Please\nconsider contacting commercial support if in doubt.\nHere are some tips to keep in mind when dealing with these kind of sites:\nrotate your user agent from a pool of well-known ones from browsers (google\naround to get a list of them)\ndisable cookies (see COOKIES_ENABLED) as some sites may use\ncookies to spot bot behaviour\nuse download delays (2 or higher). See DOWNLOAD_DELAY setting.\nif possible, use Google cache to fetch pages, instead of hitting the sites\ndirectly\nuse a pool of rotating IPs. For example, the free Tor project or paid\nservices like ProxyMesh\nIf you are still unable to prevent your bot getting banned, consider contacting\ncommercial support.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/practices.html", "title": ["Common Practices \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nBroad Crawls\u00b6\nScrapy defaults are optimized for crawling specific sites. These sites are\noften handled by a single Scrapy spider, although this is not necessary or\nrequired (for example, there are generic spiders that handle any given site\nthrown at them).\nIn addition to this \u201cfocused crawl\u201d, there is another common type of crawling\nwhich covers a large (potentially unlimited) number of domains, and is only\nlimited by time or other arbitrary constraint, rather than stopping when the\ndomain was crawled to completion or when there are no more requests to perform.\nThese are called \u201cbroad crawls\u201d and is the typical crawlers employed by search\nengines.\nThese are some common properties often found in broad crawls:\nthey crawl many domains (often, unbounded) instead of a specific set of sites\nthey don\u2019t necessarily crawl domains to completion, because it would\nimpractical (or impossible) to do so, and instead limit the crawl by time or\nnumber of pages crawled\nthey are simpler in logic (as opposed to very complex spiders with many\nextraction rules) because data is often post-processed in a separate stage\nthey crawl many domains concurrently, which allows them to achieve faster\ncrawl speeds by not being limited by any particular site constraint (each site\nis crawled slowly to respect politeness, but many sites are crawled in\nparallel)\nAs said above, Scrapy default settings are optimized for focused crawls, not\nbroad crawls. However, due to its asynchronous architecture, Scrapy is very\nwell suited for performing fast broad crawls. This page summarize some things\nyou need to keep in mind when using Scrapy for doing broad crawls, along with\nconcrete suggestions of Scrapy settings to tune in order to achieve an\nefficient broad crawl.\n\nIncrease concurrency\u00b6\nConcurrency is the number of requests that are processed in parallel. There is\na global limit and a per-domain limit.\nThe default global concurrency limit in Scrapy is not suitable for crawling\nmany different  domains in parallel, so you will want to increase it. How much\nto increase it will depend on how much CPU you crawler will have available. A\ngood starting point is 100, but the best way to find out is by doing some\ntrials and identifying at what concurrency your Scrapy process gets CPU\nbounded. For optimum performance, You should pick a concurrency where CPU usage\nis at 80-90%.\nTo increase the global concurrency use:\nCONCURRENT_REQUESTS = 100\n\n\n\n\nReduce log level\u00b6\nWhen doing broad crawls you are often only interested in the crawl rates you\nget and any errors found. These stats are reported by Scrapy when using the\nINFO log level. In order to save CPU (and log storage requirements) you\nshould not use DEBUG log level when preforming large broad crawls in\nproduction. Using DEBUG level when developing your (broad) crawler may fine\nthough.\nTo set the log level use:\nLOG_LEVEL = 'INFO'\n\n\n\n\nDisable cookies\u00b6\nDisable cookies unless you really need. Cookies are often not needed when\ndoing broad crawls (search engine crawlers ignore them), and they improve\nperformance by saving some CPU cycles and reducing the memory footprint of your\nScrapy crawler.\nTo disable cookies use:\nCOOKIES_ENABLED = False\n\n\n\n\nDisable retries\u00b6\nRetrying failed HTTP requests can slow down the crawls substantially, specially\nwhen sites causes are very slow (or fail) to respond, thus causing a timeout\nerror which gets retried many times, unnecessarily, preventing crawler capacity\nto be reused for other domains.\nTo disable retries use:\nRETRY_ENABLED = False\n\n\n\n\nReduce download timeout\u00b6\nUnless you are crawling from a very slow connection (which shouldn\u2019t be the\ncase for broad crawls) reduce the download timeout so that stuck requests are\ndiscarded quickly and free up capacity to process the next ones.\nTo reduce the download timeout use:\nDOWNLOAD_TIMEOUT = 15\n\n\n\n\nDisable redirects\u00b6\nConsider disabling redirects, unless you are interested in following them. When\ndoing broad crawls it\u2019s common to save redirects and resolve them when\nrevisiting the site at a later crawl. This also help to keep the number of\nrequest constant per crawl batch, otherwise redirect loops may cause the\ncrawler to dedicate too many resources on any specific domain.\nTo disable redirects use:\nREDIRECT_ENABLED = False\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/broad-crawls.html", "title": ["Broad Crawls \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nUsing Firefox for scraping\u00b6\nHere is a list of tips and advice on using Firefox for scraping, along with a\nlist of useful Firefox add-ons to ease the scraping process.\n\nCaveats with inspecting the live browser DOM\u00b6\nSince Firefox add-ons operate on a live browser DOM, what you\u2019ll actually see\nwhen inspecting the page source is not the original HTML, but a modified one\nafter applying some browser clean up and executing Javascript code.  Firefox,\nin particular, is known for adding <tbody> elements to tables.  Scrapy, on\nthe other hand, does not modify the original page HTML, so you won\u2019t be able to\nextract any data if you use <tbody in your XPath expressions.\nTherefore, you should keep in mind the following things when working with\nFirefox and XPath:\nDisable Firefox Javascript while inspecting the DOM looking for XPaths to be\nused in Scrapy\nNever use full XPath paths, use relative and clever ones based on attributes\n(such as id, class, width, etc) or any identifying features like\ncontains(@href, 'image').\nNever include <tbody> elements in your XPath expressions unless you\nreally know what you\u2019re doing\n\n\nUseful Firefox add-ons for scraping\u00b6\n\nFirebug\u00b6\nFirebug is a widely known tool among web developers and it\u2019s also very\nuseful for scraping. In particular, its Inspect Element feature comes very\nhandy when you need to construct the XPaths for extracting data because it\nallows you to view the HTML code of each page element while moving your mouse\nover it.\nSee Using Firebug for scraping for a detailed guide on how to use Firebug with\nScrapy.\n\n\nXPather\u00b6\nXPather allows you to test XPath expressions directly on the pages.\n\n\nXPath Checker\u00b6\nXPath Checker is another Firefox add-on for testing XPaths on your pages.\n\n\nTamper Data\u00b6\nTamper Data is a Firefox add-on which allows you to view and modify the HTTP\nrequest headers sent by Firefox. Firebug also allows to view HTTP headers, but\nnot to modify them.\n\n\nFirecookie\u00b6\nFirecookie makes it easier to view and manage cookies. You can use this\nextension to create a new cookie, delete existing cookies, see a list of cookies\nfor the current site, manage cookies permissions and a lot more.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/firefox.html", "title": ["Using Firefox for scraping \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nUsing Firebug for scraping\u00b6\n\nNote\nGoogle Directory, the example website used in this guide is no longer\navailable as it has been shut down by Google. The concepts in this guide\nare still valid though. If you want to update this guide to use a new\n(working) site, your contribution will be more than welcome!. See Contributing to Scrapy\nfor information on how to do so.\n\n\nIntroduction\u00b6\nThis document explains how to use Firebug (a Firefox add-on) to make the\nscraping process easier and more fun. For other useful Firefox add-ons see\nUseful Firefox add-ons for scraping. There are some caveats with using Firefox add-ons\nto inspect pages, see Caveats with inspecting the live browser DOM.\nIn this example, we\u2019ll show how to use Firebug to scrape data from the\nGoogle Directory, which contains the same data as the Open Directory\nProject used in the tutorial but with a different\nface.\nFirebug comes with a very useful feature called Inspect Element which allows\nyou to inspect the HTML code of the different page elements just by hovering\nyour mouse over them. Otherwise you would have to search for the tags manually\nthrough the HTML body which can be a very tedious task.\nIn the following screenshot you can see the Inspect Element tool in action.\n\nAt first sight, we can see that the directory is divided in categories, which\nare also divided in subcategories.\nHowever, it seems that there are more subcategories than the ones being shown\nin this page, so we\u2019ll keep looking:\n\nAs expected, the subcategories contain links to other subcategories, and also\nlinks to actual websites, which is the purpose of the directory.\n\n\nGetting links to follow\u00b6\nBy looking at the category URLs we can see they share a pattern:\n\nhttp://directory.google.com/Category/Subcategory/Another_Subcategory\nOnce we know that, we are able to construct a regular expression to follow\nthose links. For example, the following one:\ndirectory\\.google\\.com/[A-Z][a-zA-Z_/]+$\n\nSo, based on that regular expression we can create the first crawling rule:\nRule(SgmlLinkExtractor(allow='directory.google.com/[A-Z][a-zA-Z_/]+$', ),\n    'parse_category',\n    follow=True,\n),\n\n\nThe Rule object instructs\nCrawlSpider based spiders how to follow the\ncategory links. parse_category will be a method of the spider which will\nprocess and extract data from those pages.\nThis is how the spider would look so far:\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\n\nclass GoogleDirectorySpider(CrawlSpider):\n    name = 'directory.google.com'\n    allowed_domains = ['directory.google.com']\n    start_urls = ['http://directory.google.com/']\n\n    rules = (\n        Rule(SgmlLinkExtractor(allow='directory\\.google\\.com/[A-Z][a-zA-Z_/]+$'),\n            'parse_category', follow=True,\n        ),\n    )\n\n    def parse_category(self, response):\n        # write the category page data extraction code here\n        pass\n\n\n\n\nExtracting the data\u00b6\nNow we\u2019re going to write the code to extract data from those pages.\nWith the help of Firebug, we\u2019ll take a look at some page containing links to\nwebsites (say http://directory.google.com/Top/Arts/Awards/) and find out how we can\nextract those links using XPath selectors. We\u2019ll also\nuse the Scrapy shell to test those XPath\u2019s and make sure\nthey work as we expect.\n\nAs you can see, the page markup is not very descriptive: the elements don\u2019t\ncontain id, class or any attribute that clearly identifies them, so\nwe\u2019\u2018ll use the ranking bars as a reference point to select the data to extract\nwhen we construct our XPaths.\nAfter using FireBug, we can see that each link is inside a td tag, which is\nitself inside a tr tag that also contains the link\u2019s ranking bar (in\nanother td).\nSo we can select the ranking bar, then find its parent (the tr), and then\nfinally, the link\u2019s td (which contains the data we want to scrape).\nThis results in the following XPath:\n//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td//a\n\nIt\u2019s important to use the Scrapy shell to test these\ncomplex XPath expressions and make sure they work as expected.\nBasically, that expression will look for the ranking bar\u2019s td element, and\nthen select any td element who has a descendant a element whose\nhref attribute contains the string #pagerank\u201c\nOf course, this is not the only XPath, and maybe not the simpler one to select\nthat data. Another approach could be, for example, to find any font tags\nthat have that grey colour of the links,\nFinally, we can write our parse_category() method:\ndef parse_category(self, response):\n    hxs = HtmlXPathSelector(response)\n\n    # The path to website links in directory page\n    links = hxs.select('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n\n    for link in links:\n        item = DirectoryItem()\n        item['name'] = link.select('a/text()').extract()\n        item['url'] = link.select('a/@href').extract()\n        item['description'] = link.select('font[2]/text()').extract()\n        yield item\n\n\nBe aware that you may find some elements which appear in Firebug but\nnot in the original HTML, such as the typical case of <tbody>\nelements.\nor tags which Therefer   in page HTML\nsources may on Firebug inspects the live DOM\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/firebug.html", "title": ["Using Firebug for scraping \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDebugging memory leaks\u00b6\nIn Scrapy, objects such as Requests, Responses and Items have a finite\nlifetime: they are created, used for a while, and finally destroyed.\nFrom all those objects, the Request is probably the one with the longest\nlifetime, as it stays waiting in the Scheduler queue until it\u2019s time to process\nit. For more info see Architecture overview.\nAs these Scrapy objects have a (rather long) lifetime, there is always the risk\nof accumulating them in memory without releasing them properly and thus causing\nwhat is known as a \u201cmemory leak\u201d.\nTo help debugging memory leaks, Scrapy provides a built-in mechanism for\ntracking objects references called trackref,\nand you can also use a third-party library called Guppy for more advanced memory debugging (see below for more\ninfo). Both mechanisms must be used from the Telnet Console.\n\nCommon causes of memory leaks\u00b6\nIt happens quite often (sometimes by accident, sometimes on purpose) that the\nScrapy developer passes objects referenced in Requests (for example, using the\nmeta attribute or the request callback function)\nand that effectively bounds the lifetime of those referenced objects to the\nlifetime of the Request. This is, by far, the most common cause of memory leaks\nin Scrapy projects, and a quite difficult one to debug for newcomers.\nIn big projects, the spiders are typically written by different people and some\nof those spiders could be \u201cleaking\u201d and thus affecting the rest of the other\n(well-written) spiders when they get to run concurrently, which, in turn,\naffects the whole crawling process.\nAt the same time, it\u2019s hard to avoid the reasons that cause these leaks\nwithout restricting the power of the framework, so we have decided not to\nrestrict the functionally but provide useful tools for debugging these leaks,\nwhich quite often consist in an answer to the question: which spider is leaking?.\nThe leak could also come from a custom middleware, pipeline or extension that\nyou have written, if you are not releasing the (previously allocated) resources\nproperly. For example, if you\u2019re allocating resources on\nspider_opened but not releasing them on spider_closed.\n\n\nDebugging memory leaks with trackref\u00b6\ntrackref is a module provided by Scrapy to debug the most common cases of\nmemory leaks. It basically tracks the references to all live Requests,\nResponses, Item and Selector objects.\nYou can enter the telnet console and inspect how many objects (of the classes\nmentioned above) are currently alive using the prefs() function which is an\nalias to the print_live_refs() function:\ntelnet localhost 6023\n\n>>> prefs()\nLive References\n\nExampleSpider                       1   oldest: 15s ago\nHtmlResponse                       10   oldest: 1s ago\nXPathSelector                       2   oldest: 0s ago\nFormRequest                       878   oldest: 7s ago\n\nAs you can see, that report also shows the \u201cage\u201d of the oldest object in each\nclass.\nIf you do have leaks, chances are you can figure out which spider is leaking by\nlooking at the oldest request or response. You can get the oldest object of\neach class using the get_oldest() function like\nthis (from the telnet console).\n\nWhich objects are tracked?\u00b6\nThe objects tracked by trackrefs are all from these classes (and all its\nsubclasses):\nscrapy.http.Request\nscrapy.http.Response\nscrapy.item.Item\nscrapy.selector.XPathSelector\nscrapy.spider.BaseSpider\nscrapy.selector.document.Libxml2Document\n\n\nA real example\u00b6\nLet\u2019s see a concrete example of an hypothetical case of memory leaks.\nSuppose we have some spider with a line similar to this one:\nreturn Request(\"http://www.somenastyspider.com/product.php?pid=%d\" % product_id,\n    callback=self.parse, meta={referer: response}\")\n\nThat line is passing a response reference inside a request which effectively\nties the response lifetime to the requests\u2019 one, and that would definitely\ncause memory leaks.\nLet\u2019s see how we can discover which one is the nasty spider (without knowing it\na-priori, of course) by using the trackref tool.\nAfter the crawler is running for a few minutes and we notice its memory usage\nhas grown a lot, we can enter its telnet console and check the live\nreferences:\n>>> prefs()\nLive References\n\nSomenastySpider                     1   oldest: 15s ago\nHtmlResponse                     3890   oldest: 265s ago\nXPathSelector                       2   oldest: 0s ago\nRequest                          3878   oldest: 250s ago\n\n\nThe fact that there are so many live responses (and that they\u2019re so old) is\ndefinitely suspicious, as responses should have a relatively short lifetime\ncompared to Requests. So let\u2019s check the oldest response:\n>>> from scrapy.utils.trackref import get_oldest\n>>> r = get_oldest('HtmlResponse')\n>>> r.url\n'http://www.somenastyspider.com/product.php?pid=123'\n\n\nThere it is. By looking at the URL of the oldest response we can see it belongs\nto the somenastyspider.com spider. We can now go and check the code of that\nspider to discover the nasty line that is generating the leaks (passing\nresponse references inside requests).\nIf you want to iterate over all objects, instead of getting the oldest one, you\ncan use the iter_all() function:\n>>> from scrapy.utils.trackref import iter_all\n>>> [r.url for r in iter_all('HtmlResponse')]\n['http://www.somenastyspider.com/product.php?pid=123',\n 'http://www.somenastyspider.com/product.php?pid=584',\n...\n\n\n\n\nToo many spiders?\u00b6\nIf your project has too many spiders, the output of prefs() can be\ndifficult to read. For this reason, that function has a ignore argument\nwhich can be used to ignore a particular class (and all its subclases). For\nexample, using:\n>>> from scrapy.spider import BaseSpider\n>>> prefs(ignore=BaseSpider)\n\n\nWon\u2019t show any live references to spiders.\n\n\nscrapy.utils.trackref module\u00b6\nHere are the functions available in the trackref module.\n\nclass scrapy.utils.trackref.object_ref\u00b6\nInherit from this class (instead of object) if you want to track live\ninstances with the trackref module.\n\nscrapy.utils.trackref.print_live_refs(class_name, ignore=NoneType)\u00b6\nPrint a report of live references, grouped by class name.\nParameters:ignore (class or classes tuple) \u2013 if given, all objects from the specified class (or tuple of\nclasses) will be ignored.\n\nscrapy.utils.trackref.get_oldest(class_name)\u00b6\nReturn the oldest object alive with the given class name, or None if\nnone is found. Use print_live_refs() first to get a list of all\ntracked live objects per class name.\n\nscrapy.utils.trackref.iter_all(class_name)\u00b6\nReturn an iterator over all objects alive with the given class name, or\nNone if none is found. Use print_live_refs() first to get a list\nof all tracked live objects per class name.\n\n\n\nDebugging memory leaks with Guppy\u00b6\ntrackref provides a very convenient mechanism for tracking down memory\nleaks, but it only keeps track of the objects that are more likely to cause\nmemory leaks (Requests, Responses, Items, and Selectors). However, there are\nother cases where the memory leaks could come from other (more or less obscure)\nobjects. If this is your case, and you can\u2019t find your leaks using trackref,\nyou still have another resource: the Guppy library.\nIf you use setuptools, you can install Guppy with the following command:\neasy_install guppy\n\nThe telnet console also comes with a built-in shortcut (hpy) for accessing\nGuppy heap objects. Here\u2019s an example to view all Python objects available in\nthe heap using Guppy:\n>>> x = hpy.heap()\n>>> x.bytype\nPartition of a set of 297033 objects. Total size = 52587824 bytes.\n Index  Count   %     Size   % Cumulative  % Type\n     0  22307   8 16423880  31  16423880  31 dict\n     1 122285  41 12441544  24  28865424  55 str\n     2  68346  23  5966696  11  34832120  66 tuple\n     3    227   0  5836528  11  40668648  77 unicode\n     4   2461   1  2222272   4  42890920  82 type\n     5  16870   6  2024400   4  44915320  85 function\n     6  13949   5  1673880   3  46589200  89 types.CodeType\n     7  13422   5  1653104   3  48242304  92 list\n     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern\n     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers\n<1676 more rows. Type e.g. '_.more' to view.>\n\n\nYou can see that most space is used by dicts. Then, if you want to see from\nwhich attribute those dicts are referenced, you could do:\n>>> x.bytype[0].byvia\nPartition of a set of 22307 objects. Total size = 16423880 bytes.\n Index  Count   %     Size   % Cumulative  % Referred Via:\n     0  10982  49  9416336  57   9416336  57 '.__dict__'\n     1   1820   8  2681504  16  12097840  74 '.__dict__', '.func_globals'\n     2   3097  14  1122904   7  13220744  80\n     3    990   4   277200   2  13497944  82 \"['cookies']\"\n     4    987   4   276360   2  13774304  84 \"['cache']\"\n     5    985   4   275800   2  14050104  86 \"['meta']\"\n     6    897   4   251160   2  14301264  87 '[2]'\n     7      1   0   196888   1  14498152  88 \"['moduleDict']\", \"['modules']\"\n     8    672   3   188160   1  14686312  89 \"['cb_kwargs']\"\n     9     27   0   155016   1  14841328  90 '[1]'\n<333 more rows. Type e.g. '_.more' to view.>\n\n\nAs you can see, the Guppy module is very powerful but also requires some deep\nknowledge about Python internals. For more info about Guppy, refer to the\nGuppy documentation.\n\n\nLeaks without leaks\u00b6\nSometimes, you may notice that the memory usage of your Scrapy process will\nonly increase, but never decrease. Unfortunately, this could happen even\nthough neither Scrapy nor your project are leaking memory. This is due to a\n(not so well) known problem of Python, which may not return released memory to\nthe operating system in some cases. For more information on this issue see:\nPython Memory Management\nPython Memory Management Part 2\nPython Memory Management Part 3\nThe improvements proposed by Evan Jones, which are detailed in this paper,\ngot merged in Python 2.5, but this only reduces the problem, it doesn\u2019t fix it\ncompletely. To quote the paper:\n\nUnfortunately, this patch can only free an arena if there are no more\nobjects allocated in it anymore. This means that fragmentation is a large\nissue. An application could have many megabytes of free memory, scattered\nthroughout all the arenas, but it will be unable to free any of it. This is\na problem experienced by all memory allocators. The only way to solve it is\nto move to a compacting garbage collector, which is able to move objects in\nmemory. This would require significant changes to the Python interpreter.\nThis problem will be fixed in future Scrapy releases, where we plan to adopt a\nnew process model and run spiders in a pool of recyclable sub-processes.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/leaks.html", "title": ["Debugging memory leaks \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDownloading Item Images\u00b6\nScrapy provides an item pipeline for downloading\nimages attached to a particular item, for example, when you scrape products and\nalso want to download their images locally.\nThis pipeline, called the Images Pipeline and implemented in the\nImagesPipeline class, provides a convenient way for\ndownloading and storing images locally with some additional features:\nConvert all downloaded images to a common format (JPG) and mode (RGB)\nAvoid re-downloading images which were downloaded recently\nThumbnail generation\nCheck images width/height to make sure they meet a minimum constraint\nThis pipeline also keeps an internal queue of those images which are currently\nbeing scheduled for download, and connects those items that arrive containing\nthe same image, to that queue. This avoids downloading the same image more than\nonce when it\u2019s shared by several items.\nThe Python Imaging Library is used for thumbnailing and normalizing images\nto JPEG/RGB format, so you need to install that library in order to use the\nimages pipeline.\n\nUsing the Images Pipeline\u00b6\nThe typical workflow, when using the ImagesPipeline goes like\nthis:\nIn a Spider, you scrape an item and put the URLs of its images into a\nimage_urls field.\nThe item is returned from the spider and goes to the item pipeline.\nWhen the item reaches the ImagesPipeline, the URLs in the\nimage_urls field are scheduled for download using the standard\nScrapy scheduler and downloader (which means the scheduler and downloader\nmiddlewares are reused), but with a higher priority, processing them before other\npages are scraped. The item remains \u201clocked\u201d at that particular pipeline stage\nuntil the images have finish downloading (or fail for some reason).\nWhen the images are downloaded another field (images) will be populated\nwith the results. This field will contain a list of dicts with information\nabout the images downloaded, such as the downloaded path, the original\nscraped url (taken from the image_urls field) , and the image checksum.\nThe images in the list of the images field will retain the same order of\nthe original image_urls field. If some image failed downloading, an\nerror will be logged and the image won\u2019t be present in the images field.\n\n\nUsage example\u00b6\nIn order to use the image pipeline you just need to enable it and define an item with the image_urls and\nimages fields:\nfrom scrapy.item import Item\n\nclass MyItem(Item):\n\n    # ... other item fields ...\n    image_urls = Field()\n    images = Field()\n\n\nIf you need something more complex and want to override the custom images\npipeline behaviour, see Implementing your custom Images Pipeline.\n\n\nEnabling your Images Pipeline\u00b6\nTo enable your images pipeline you must first add it to your project\nITEM_PIPELINES setting:\nITEM_PIPELINES = ['scrapy.contrib.pipeline.images.ImagesPipeline']\n\n\nAnd set the IMAGES_STORE setting to a valid directory that will be\nused for storing the downloaded images. Otherwise the pipeline will remain\ndisabled, even if you include it in the ITEM_PIPELINES setting.\nFor example:\nIMAGES_STORE = '/path/to/valid/dir'\n\n\n\n\nImages Storage\u00b6\nFile system is currently the only officially supported storage, but there is\nalso (undocumented) support for Amazon S3.\n\nFile system storage\u00b6\nThe images are stored in files (one per image), using a SHA1 hash of their\nURLs for the file names.\nFor example, the following image URL:\nhttp://www.example.com/image.jpg\n\nWhose SHA1 hash is:\n3afec3b4765f8f0a07b78f98c07b83f013567a0a\n\nWill be downloaded and stored in the following file:\n<IMAGES_STORE>/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg\n\nWhere:\n<IMAGES_STORE> is the directory defined in IMAGES_STORE setting\nfull is a sub-directory to separate full images from thumbnails (if\nused). For more info see Thumbnail generation.\n\n\n\nAdditional features\u00b6\n\nImage expiration\u00b6\nThe Image Pipeline avoids downloading images that were downloaded recently. To\nadjust this retention delay use the IMAGES_EXPIRES setting, which\nspecifies the delay in number of days:\n# 90 days of delay for image expiration\nIMAGES_EXPIRES = 90\n\n\n\n\nThumbnail generation\u00b6\nThe Images Pipeline can automatically create thumbnails of the downloaded\nimages.\nIn order use this feature, you must set IMAGES_THUMBS to a dictionary\nwhere the keys are the thumbnail names and the values are their dimensions.\nFor example:\nIMAGES_THUMBS = {\n    'small': (50, 50),\n    'big': (270, 270),\n}\n\n\nWhen you use this feature, the Images Pipeline will create thumbnails of the\neach specified size with this format:\n<IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg\n\nWhere:\n<size_name> is the one specified in the IMAGES_THUMBS\ndictionary keys (small, big, etc)\n<image_id> is the SHA1 hash of the image url\nExample of image files stored using small and big thumbnail names:\n<IMAGES_STORE>/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg\n<IMAGES_STORE>/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg\n<IMAGES_STORE>/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg\n\nThe first one is the full image, as downloaded from the site.\n\n\nFiltering out small images\u00b6\nYou can drop images which are too small, by specifying the minimum allowed size\nin the IMAGES_MIN_HEIGHT and IMAGES_MIN_WIDTH settings.\nFor example:\nIMAGES_MIN_HEIGHT = 110\nIMAGES_MIN_WIDTH = 110\n\n\nNote: these size constraints don\u2019t affect thumbnail generation at all.\nBy default, there are no size constraints, so all images are processed.\n\n\n\nImplementing your custom Images Pipeline\u00b6\nHere are the methods that you should override in your custom Images Pipeline:\n\nclass scrapy.contrib.pipeline.images.ImagesPipeline\u00b6\n\nget_media_requests(item, info)\u00b6\nAs seen on the workflow, the pipeline will get the URLs of the images to\ndownload from the item. In order to do this, you must override the\nget_media_requests() method and return a Request for each\nimage URL:\ndef get_media_requests(self, item, info):\n    for image_url in item['image_urls']:\n        yield Request(image_url)\n\n\nThose requests will be processed by the pipeline and, when they have finished\ndownloading, the results will be sent to the\nitem_completed() method, as a list of 2-element tuples.\nEach tuple will contain (success, image_info_or_failure) where:\nsuccess is a boolean which is True if the image was downloaded\nsuccessfully or False if it failed for some reason\nimage_info_or_error is a dict containing the following keys (if success\nis True) or a Twisted Failure if there was a problem.url - the url where the image was downloaded from. This is the url of\nthe request returned from the get_media_requests()\nmethod.\npath - the path (relative to IMAGES_STORE) where the image\nwas stored\nchecksum - a MD5 hash of the image contents\n\nThe list of tuples received by item_completed() is\nguaranteed to retain the same order of the requests returned from the\nget_media_requests() method.\nHere\u2019s a typical value of the results argument:\n[(True,\n  {'checksum': '2b00042f7481c7b056c4b410d28f33cf',\n   'path': 'full/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg',\n   'url': 'http://www.example.com/images/product1.jpg'}),\n (True,\n  {'checksum': 'b9628c4ab9b595f72f280b90c4fd093d',\n   'path': 'full/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg',\n   'url': 'http://www.example.com/images/product2.jpg'}),\n (False,\n  Failure(...))]\n\n\nBy default the get_media_requests() method returns None which\nmeans there are no images to download for the item.\n\nitem_completed(results, items, info)\u00b6\nThe ImagesPipeline.item_completed() method called when all image\nrequests for a single item have completed (either finished downloading, or\nfailed for some reason).\nThe item_completed() method must return the\noutput that will be sent to subsequent item pipeline stages, so you must\nreturn (or drop) the item, as you would in any pipeline.\nHere is an example of the item_completed() method where we\nstore the downloaded image paths (passed in results) in the image_paths\nitem field, and we drop the item if it doesn\u2019t contain any images:\nfrom scrapy.exceptions import DropItem\n\ndef item_completed(self, results, item, info):\n    image_paths = [x['path'] for ok, x in results if ok]\n    if not image_paths:\n        raise DropItem(\"Item contains no images\")\n    item['image_paths'] = image_paths\n    return item\n\n\nBy default, the item_completed() method returns the item.\n\n\nCustom Images pipeline example\u00b6\nHere is a full example of the Images Pipeline whose methods are examplified\nabove:\nfrom scrapy.contrib.pipeline.images import ImagesPipeline\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request\n\nclass MyImagesPipeline(ImagesPipeline):\n\n    def get_media_requests(self, item, info):\n        for image_url in item['image_urls']:\n            yield Request(image_url)\n\n    def item_completed(self, results, item, info):\n        image_paths = [x['path'] for ok, x in results if ok]\n        if not image_paths:\n            raise DropItem(\"Item contains no images\")\n        item['image_paths'] = image_paths\n        return item\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/images.html", "title": ["Downloading Item Images \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nUbuntu packages\u00b6\n\nNew in version 0.10.\nScrapinghub publishes apt-gettable packages which are generally fresher than\nthose in Ubuntu, and more stable too since they\u2019re continuously built from\nGithub repo (master & stable branches) and so they contain the latest bug\nfixes.\nTo use the packages, just add the following line to your\n/etc/apt/sources.list, and then run aptitude update and aptitude\ninstall scrapy-0.13:\ndeb http://archive.scrapy.org/ubuntu DISTRO main\n\nReplacing DISTRO with the name of your Ubuntu release, which you can get\nwith command:\nlsb_release -cs\n\n\nSupported Ubuntu releases are: karmic, lucid, maverick, natty,\noneiric, precise.\nFor Ubuntu Precise (12.04):\ndeb http://archive.scrapy.org/ubuntu precise main\n\nFor Ubuntu Oneiric (11.10):\ndeb http://archive.scrapy.org/ubuntu oneiric main\n\nFor Ubuntu Natty (11.04):\ndeb http://archive.scrapy.org/ubuntu natty main\n\nFor Ubuntu Maverick (10.10):\ndeb http://archive.scrapy.org/ubuntu maverick main\n\nFor Ubuntu Lucid (10.04):\ndeb http://archive.scrapy.org/ubuntu lucid main\n\nFor Ubuntu Karmic (9.10):\ndeb http://archive.scrapy.org/ubuntu karmic main\n\n\nWarning\nPlease note that these packages are updated frequently, and so if\nyou find you can\u2019t download the packages, try updating your apt package\nlists first, e.g., with apt-get update or aptitude update.\n\nThe public GPG key used to sign these packages can be imported into you APT\nkeyring as follows:\ncurl -s http://archive.scrapy.org/ubuntu/archive.key | sudo apt-key add -\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/ubuntu.html", "title": ["Ubuntu packages \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nScrapy Service (scrapyd)\u00b6\n\nNew in version 0.10.\nScrapy comes with a built-in service, called \u201cScrapyd\u201d, which allows you to\ndeploy (aka. upload) your projects and control their spiders using a JSON web\nservice.\n\nProjects and versions\u00b6\nScrapyd can manage multiple projects and each project can have multiple\nversions uploaded, but only the latest one will be used for launching new\nspiders.\nA common (and useful) convention to use for the version name is the revision\nnumber of the version control tool you\u2019re using to track your Scrapy project\ncode. For example: r23. The versions are not compared alphabetically but\nusing a smarter algorithm (the same distutils uses) so r10 compares\ngreater to r9, for example.\n\n\nHow Scrapyd works\u00b6\nScrapyd is an application (typically run as a daemon) that continually polls\nfor spiders that need to run.\nWhen a spider needs to run, a process is started to crawl the spider:\nscrapy crawl myspider\n\nScrapyd also runs multiple processes in parallel, allocating them in a fixed\nnumber of slots given by the max_proc and max_proc_per_cpu options,\nstarting as many processes as possible to handle the load.\nIn addition to dispatching and managing processes, Scrapyd provides a\nJSON web service to upload new project versions\n(as eggs) and schedule spiders. This feature is optional and can be disabled if\nyou want to implement your own custom Scrapyd. The components are pluggable and\ncan be changed, if you\u2019re familiar with the Twisted Application Framework\nwhich Scrapyd is implemented in.\nStarting from 0.11, Scrapyd also provides a minimal web interface.\n\n\nStarting Scrapyd\u00b6\nScrapyd is implemented using the standard Twisted Application Framework. To\nstart the service, use the extras/scrapyd.tac file provided in the Scrapy\ndistribution, like this:\ntwistd -ny extras/scrapyd.tac\n\nThat should get your Scrapyd started.\nOr, if you want to start Scrapyd from inside a Scrapy project you can use the\nserver command, like this:\nscrapy server\n\n\n\nInstalling Scrapyd\u00b6\nHow to deploy Scrapyd on your servers depends on the platform you\u2019re using.\nScrapy comes with Ubuntu packages for Scrapyd ready for deploying it as a\nsystem service, to ease the installation and administration, but you can create\npackages for other distribution or operating systems (including Windows). If\nyou do so, and want to contribute them, send a message to\nscrapy-developers@googlegroups.com and say hi. The community will appreciate\nit.\n\nInstalling Scrapyd in Ubuntu\u00b6\nWhen deploying Scrapyd, it\u2019s very useful to have a version already packaged for\nyour system. For this reason, Scrapyd comes with Ubuntu packages ready to use\nin your Ubuntu servers.\nSo, if you plan to deploy Scrapyd on a Ubuntu server, just add the Ubuntu\nrepositories as described in Ubuntu packages and then run:\naptitude install scrapyd-X.YY\n\nWhere X.YY is the Scrapy version, for example: 0.14.\nThis will install Scrapyd in your Ubuntu server creating a scrapy user\nwhich Scrapyd will run as. It will also create some directories and files that\nare listed below:\n\n/etc/scrapyd\u00b6\nScrapyd configuration files. See Scrapyd Configuration file.\n\n\n/var/log/scrapyd/scrapyd.log\u00b6\nScrapyd main log file.\n\n\n/var/log/scrapyd/scrapyd.out\u00b6\nThe standard output captured from Scrapyd process and any\nsub-process spawned from it.\n\n\n/var/log/scrapyd/scrapyd.err\u00b6\nThe standard error captured from Scrapyd and any sub-process spawned\nfrom it. Remember to check this file if you\u2019re having problems, as the errors\nmay not get logged to the scrapyd.log file.\n\n\n/var/log/scrapyd/project\u00b6\nBesides the main service log file, Scrapyd stores one log file per crawling\nprocess in:\n/var/log/scrapyd/PROJECT/SPIDER/ID.log\n\nWhere ID is a unique id for the run.\n\n\n/var/lib/scrapyd/\u00b6\nDirectory used to store data files (uploaded eggs and spider queues).\n\n\n\n\nScrapyd Configuration file\u00b6\nScrapyd searches for configuration files in the following locations, and parses\nthem in order with the latest ones taking more priority:\n/etc/scrapyd/scrapyd.conf (Unix)\nc:\\scrapyd\\scrapyd.conf (Windows)\n/etc/scrapyd/conf.d/* (in alphabetical order, Unix)\nscrapyd.conf\nThe configuration file supports the following options (see default values in\nthe example).\n\nhttp_port\u00b6\nThe TCP port where the HTTP JSON API will listen. Defaults to 6800.\n\n\nbind_address\u00b6\nThe IP address where the HTTP JSON API will listen. Defaults to 0.0.0.0 (all)\n\n\nmax_proc\u00b6\nThe maximum number of concurrent Scrapy process that will be started. If unset\nor 0 it will use the number of cpus available in the system mulitplied by\nthe value in max_proc_per_cpu option. Defaults to 0.\n\n\nmax_proc_per_cpu\u00b6\nThe maximum number of concurrent Scrapy process that will be started per cpu.\nDefaults to 4.\n\n\ndebug\u00b6\nWhether debug mode is enabled. Defaults to off. When debug mode is enabled\nthe full Python traceback will be returned (as plain text responses) when there\nis an error processing a JSON API call.\n\n\neggs_dir\u00b6\nThe directory where the project eggs will be stored.\n\n\ndbs_dir\u00b6\nThe directory where the project databases will be stored (this includes the\nspider queues).\n\n\nlogs_dir\u00b6\nThe directory where the Scrapy logs will be stored. If you want to disable\nstoring logs set this option empty, like this:\nlogs_dir =\n\n\n\nitems_dir\u00b6\n\nNew in version 0.15.\nThe directory where the Scrapy items will be stored. If you want to disable\nstoring feeds of scraped items (perhaps, because you use a database or other\nstorage) set this option empty, like this:\nitems_dir =\n\n\n\njobs_to_keep\u00b6\n\nNew in version 0.15.\nThe number of finished jobs to keep per spider. Defaults to 5. This\nincludes logs and items.\nThis setting was named logs_to_keep in previous versions.\n\n\nrunner\u00b6\nThe module that will be used for launching sub-processes. You can customize the\nScrapy processes launched from Scrapyd by using your own module.\n\n\napplication\u00b6\nA function that returns the (Twisted) Application object to use. This can be\nused if you want to extend Scrapyd by adding and removing your own components\nand services.\nFor more info see Twisted Application Framework\n\n\nExample configuration file\u00b6\nHere is an example configuration file with all the defaults:\n[scrapyd]\neggs_dir    = eggs\nlogs_dir    = logs\nitems_dir   = items\njobs_to_keep = 5\ndbs_dir     = dbs\nmax_proc    = 0\nmax_proc_per_cpu = 4\nfinished_to_keep = 100\nhttp_port   = 6800\ndebug       = off\nrunner      = scrapyd.runner\napplication = scrapyd.app.application\nlauncher    = scrapyd.launcher.Launcher\n\n[services]\nschedule.json     = scrapyd.webservice.Schedule\ncancel.json       = scrapyd.webservice.Cancel\naddversion.json   = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json  = scrapyd.webservice.ListSpiders\ndelproject.json   = scrapyd.webservice.DeleteProject\ndelversion.json   = scrapyd.webservice.DeleteVersion\nlistjobs.json     = scrapyd.webservice.ListJobs\n\n\n\n\n\nDeploying your project\u00b6\nDeploying your project into a Scrapyd server typically involves two steps:\nbuilding a Python egg of your project. This is called \u201ceggifying\u201d your\nproject. You\u2019ll need to install setuptools for this. See\nEgg caveats below.\nuploading the egg to the Scrapyd server\nThe simplest way to deploy your project is by using the deploy\ncommand, which automates the process of building the egg uploading it using the\nScrapyd HTTP JSON API.\nThe deploy command supports multiple targets (Scrapyd servers that\ncan host your project) and each target supports multiple projects.\nEach time you deploy a new version of a project, you can name it for later\nreference.\n\nShow and define targets\u00b6\nTo see all available targets type:\nscrapy deploy -l\n\nThis will return a list of available targets and their URLs. For example:\nscrapyd              http://localhost:6800/\n\nYou can define targets by adding them to your project\u2019s scrapy.cfg file,\nor any other supported location like ~/.scrapy.cfg, /etc/scrapy.cfg,\nor c:\\scrapy\\scrapy.cfg (in Windows).\nHere\u2019s an example of defining a new target scrapyd2 with restricted access\nthrough HTTP basic authentication:\n[deploy:scrapyd2]\nurl = http://scrapyd.mydomain.com/api/scrapyd/\nusername = john\npassword = secret\n\n\nNote\nThe deploy command also supports netrc for getting the\ncredentials.\n\nNow, if you type scrapy deploy -l you\u2019ll see:\nscrapyd              http://localhost:6800/\nscrapyd2             http://scrapyd.mydomain.com/api/scrapyd/\n\n\n\nSee available projects\u00b6\nTo see all available projects in a specific target use:\nscrapy deploy -L scrapyd\n\nIt would return something like this:\nproject1\nproject2\n\n\n\n\nDeploying a project\u00b6\nFinally, to deploy your project use:\nscrapy deploy scrapyd -p project1\n\nThis will eggify your project and upload it to the target, printing the JSON\nresponse returned from the Scrapyd server. If you have a setup.py file in\nyour project, that one will be used. Otherwise a setup.py file will be\ncreated automatically (based on a simple template) that you can edit later.\nAfter running that command you will see something like this, meaning your\nproject was uploaded successfully:\nDeploying myproject-1287453519 to http://localhost:6800/addversion.json\nServer response (200):\n{\"status\": \"ok\", \"spiders\": [\"spider1\", \"spider2\"]}\n\nBy default scrapy deploy uses the current timestamp for generating the\nproject version, as you can see in the output above. However, you can pass a\ncustom version with the --version option:\nscrapy deploy scrapyd -p project1 --version 54\n\nAlso, if you use Mercurial for tracking your project source code, you can use\nHG for the version which will be replaced by the current Mercurial\nrevision, for example r382:\nscrapy deploy scrapyd -p project1 --version HG\n\nAnd, if you use Git for tracking your project source code, you can use\nGIT for the version which will be replaced by the SHA1 of current Git\nrevision, for example b0582849179d1de7bd86eaa7201ea3cda4b5651f:\nscrapy deploy scrapyd -p project1 --version GIT\n\nSupport for other version discovery sources may be added in the future.\nFinally, if you don\u2019t want to specify the target, project and version every\ntime you run scrapy deploy you can define the defaults in the\nscrapy.cfg file. For example:\n[deploy]\nurl = http://scrapyd.mydomain.com/api/scrapyd/\nusername = john\npassword = secret\nproject = project1\nversion = HG\n\nThis way, you can deploy your project just by using:\nscrapy deploy\n\n\n\nLocal settings\u00b6\nSometimes, while your working on your projects, you may want to override your\ncertain settings with certain local settings that shouldn\u2019t be deployed to\nScrapyd, but only used locally to develop and debug your spiders.\nOne way to deal with this is to have a local_settings.py at the root of\nyour project (where the scrapy.cfg file resides) and add these lines to the\nend of your project settings:\ntry:\n    from local_settings import *\nexcept ImportError:\n    pass\n\n\nscrapy deploy won\u2019t deploy anything outside the project module so the\nlocal_settings.py file won\u2019t be deployed.\nHere\u2019s the directory structure, to illustrate:\nscrapy.cfg\nlocal_settings.py\nmyproject/\n    __init__.py\n    settings.py\n    spiders/\n        ...\n\n\n\nEgg caveats\u00b6\nThere are some things to keep in mind when building eggs of your Scrapy\nproject:\nmake sure no local development settings are included in the egg when you\nbuild it. The find_packages function may be picking up your custom\nsettings. In most cases you want to upload the egg with the default project\nsettings.\nyou shouldn\u2019t use __file__ in your project code as it doesn\u2019t play well\nwith eggs. Consider using pkgutil.get_data() instead.\nbe careful when writing to disk in your project (in any spider, extension or\nmiddleware) as Scrapyd will probably run with a different user which may not\nhave write access to certain directories. If you can, avoid writing to disk\nand always use tempfile for temporary files.\n\n\n\nScheduling a spider run\u00b6\nTo schedule a spider run:\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider2\n{\"status\": \"ok\", \"jobid\": \"26d1b1a6d6f111e0be5c001e648c57f8\"}\n\nFor more resources see: JSON API reference for more available resources.\n\n\nWeb Interface\u00b6\n\nNew in version 0.11.\nScrapyd comes with a minimal web interface (for monitoring running processes\nand accessing logs) which can be accessed at http://localhost:6800/\n\n\nJSON API reference\u00b6\nThe following section describes the available resources in Scrapyd JSON API.\n\naddversion.json\u00b6\nAdd a version to a project, creating the project if it doesn\u2019t exist.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\nversion (string, required) - the project version\negg (file, required) - a Python egg containing the project\u2019s code\n\nExample request:\n$ curl http://localhost:6800/addversion.json -F project=myproject -F version=r23 -F egg=@myproject.egg\n\nExample response:\n{\"status\": \"ok\", \"spiders\": 3}\n\n\n\n\nschedule.json\u00b6\nSchedule a spider run (also known as a job), returning the job id.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\nspider (string, required) - the spider name\nsetting (string, optional) - a scrapy setting to use when running the spider\nany other parameter is passed as spider argument\n\nExample request:\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider\n\nExample response:\n{\"status\": \"ok\", \"jobid\": \"6487ec79947edab326d6db28a2d86511e8247444\"}\n\n\nExample request passing a spider argument (arg1) and a setting\n(DOWNLOAD_DELAY):\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider -d setting=DOWNLOAD_DELAY=2 -d arg1=val1\n\n\n\ncancel.json\u00b6\n\nNew in version 0.15.\nCancel a spider run (aka. job). If the job is pending, it will be removed. If\nthe job is running, it will be terminated.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\njob (string, required) - the job id\n\nExample request:\n$ curl http://localhost:6800/cancel.json -d project=myproject -d job=6487ec79947edab326d6db28a2d86511e8247444\n\nExample response:\n{\"status\": \"ok\", \"prevstate\": \"running\"}\n\n\n\n\nlistprojects.json\u00b6\nGet the list of projects uploaded to this Scrapy server.\nSupported Request Methods: GET\nParameters: none\nExample request:\n$ curl http://localhost:6800/listprojects.json\n\nExample response:\n{\"status\": \"ok\", \"projects\": [\"myproject\", \"otherproject\"]}\n\n\n\n\nlistversions.json\u00b6\nGet the list of versions available for some project. The versions are returned\nin order, the last one is the currently used version.\nSupported Request Methods: GET\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/listversions.json?project=myproject\n\nExample response:\n{\"status\": \"ok\", \"versions\": [\"r99\", \"r156\"]}\n\n\n\n\nlistspiders.json\u00b6\nGet the list of spiders available in the last version of some project.\nSupported Request Methods: GET\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/listspiders.json?project=myproject\n\nExample response:\n{\"status\": \"ok\", \"spiders\": [\"spider1\", \"spider2\", \"spider3\"]}\n\n\n\n\nlistjobs.json\u00b6\n\nNew in version 0.15.\nGet the list of pending, running and finished jobs of some project.\nSupported Request Methods: GET\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/listjobs.json?project=myproject\n\nExample response:\n{\"status\": \"ok\",\n \"pending\": [{\"id\": \"78391cc0fcaf11e1b0090800272a6d06\", \"spider\": \"spider1\"}],\n \"running\": [{\"id\": \"422e608f9f28cef127b3d5ef93fe9399\", \"spider\": \"spider2\"}],\n \"finished\": [{\"id\": \"2f16646cfcaf11e1b0090800272a6d06\", \"spider\": \"spider3\", \"start_time\": \"2012-09-12 10:14:03.594664\", \"end_time\": \"2012-09-12 10:24:03.594664\"}]}\n\n\n\nNote\nAll job data is kept in memory and will be reset when the Scrapyd service is restarted. See issue 173.\n\n\n\ndelversion.json\u00b6\nDelete a project version. If there are no more versions available for a given\nproject, that project will be deleted too.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\nversion (string, required) - the project version\n\nExample request:\n$ curl http://localhost:6800/delversion.json -d project=myproject -d version=r99\n\nExample response:\n{\"status\": \"ok\"}\n\n\n\n\ndelproject.json\u00b6\nDelete a project and all its uploaded versions.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/delproject.json -d project=myproject\n\nExample response:\n{\"status\": \"ok\"}\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/scrapyd.html", "title": ["Scrapy Service (scrapyd) \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nAutoThrottle extension\u00b6\nThis is an extension for automatically throttling crawling speed based on load\nof both the Scrapy server and the website you are crawling.\n\nDesign goals\u00b6\nbe nicer to sites instead of using default download delay of zero\nautomatically adjust scrapy to the optimum crawling speed, so the user\ndoesn\u2019t have to tune the download delays and concurrent requests to find the\noptimum one. the user only needs to specify the maximum concurrent requests\nit allows, and the extension does the rest.\n\n\nHow it works\u00b6\nIn Scrapy, the download latency is measured as the time elapsed between\nestablishing the TCP connection and receiving the HTTP headers.\nNote that these latencies are very hard to measure accurately in a cooperative\nmultitasking environment because Scrapy may be busy processing a spider\ncallback, for example, and unable to attend downloads. However, these latencies\nshould still give a reasonable estimate of how busy Scrapy (and ultimately, the\nserver) is, and this extension builds on that premise.\n\n\nThrottling algorithm\u00b6\nThis adjusts download delays and concurrency based on the following rules:\nspiders always start with one concurrent request and a download delay of\nAUTOTHROTTLE_START_DELAY\nwhen a response is received, the download delay is adjusted to the\naverage of previous download delay and the latency of the response.\nafter AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD responses have\npassed, the average latency of this period is checked against the previous\none and:if the latency remained constant (within standard deviation limits), it is increased\nif the latency has increased (beyond standard deviation limits) and the concurrency is higher than 1, the concurrency is decreased\n\n\nNote\nThe AutoThrottle extension honours the standard Scrapy settings for\nconcurrency and delay. This means that it will never set a download delay\nlower than DOWNLOAD_DELAY or a concurrency higher than CONCURRENT_REQUESTS_PER_DOMAIN (or CONCURRENT_REQUESTS_PER_IP, depending on which one you use).\n\n\n\nSettings\u00b6\nThe settings used to control the AutoThrottle extension are:\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\nAUTOTHROTTLE_DEBUG\nDOWNLOAD_DELAY\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nFor more information see Throttling algorithm.\n\nAUTOTHROTTLE_ENABLED\u00b6\nDefault: False\nEnables the AutoThrottle extension.\n\n\nAUTOTHROTTLE_START_DELAY\u00b6\nDefault: 5.0\nThe initial download delay (in seconds).\n\n\nAUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\u00b6\nDefault: 10\nHow many responses should pass to perform concurrency adjustments.\n\n\nAUTOTHROTTLE_DEBUG\u00b6\nDefault: False\nEnable AutoThrottle debug mode which will display stats on every response\nreceived, so you can see how the throttling parameters are being adjusted in\nreal time.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/autothrottle.html", "title": ["AutoThrottle extension \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nJobs: pausing and resuming crawls\u00b6\nSometimes, for big sites, it\u2019s desirable to pause crawls and be able to resume\nthem later.\nScrapy supports this functionality out of the box by providing the following\nfacilities:\na scheduler that persists scheduled requests on disk\na duplicates filter that persists visited requests on disk\nan extension that keeps some spider state (key/value pairs) persistent\nbetween batches\n\nJob directory\u00b6\nTo enable persistence support you just need to define a job directory through\nthe JOBDIR setting. This directory will be for storing all required data to\nkeep the state of a single job (ie. a spider run).  It\u2019s important to note that\nthis directory must not be shared by different spiders, or even different\njobs/runs of the same spider, as it\u2019s meant to be used for storing the state of\na single job.\n\n\nHow to use it\u00b6\nTo start a spider with persistence supported enabled, run it like this:\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n\nThen, you can stop the spider safely at any time (by pressing Ctrl-C or sending\na signal), and resume it later by issuing the same command:\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n\n\n\nKeeping persitent state between batches\u00b6\nSometimes you\u2019ll want to keep some persistent spider state between pause/resume\nbatches. You can use the spider.state attribute for that, which should be a\ndict. There\u2019s a built-in extension that takes care of serializing, storing and\nloading that attribute from the job directory, when the spider starts and\nstops.\nHere\u2019s an example of a callback that uses the spider state (other spider code\nis omitted for brevity):\ndef parse_item(self, response):\n    # parse item here\n    self.state['items_count'] = self.state.get('items_count', 0) + 1\n\n\n\n\nPersistence gotchas\u00b6\nThere are a few things to keep in mind if you want to be able to use the Scrapy\npersistence support:\n\nCookies expiration\u00b6\nCookies may expire. So, if you don\u2019t resume your spider quickly the requests\nscheduled may no longer work. This won\u2019t be an issue if you spider doesn\u2019t rely\non cookies.\n\n\nRequest serialization\u00b6\nRequests must be serializable by the pickle module, in order for persistence\nto work, so you should make sure that your requests are serializable.\nThe most common issue here is to use lambda functions on request callbacks that\ncan\u2019t be persisted.\nSo, for example, this won\u2019t work:\ndef some_callback(self, response):\n    somearg = 'test'\n    return Request('http://www.example.com', callback=lambda r: self.other_callback(r, somearg))\n\ndef other_callback(self, response, somearg):\n    print \"the argument passed is:\", somearg\n\n\nBut this will:\ndef some_callback(self, response):\n    somearg = 'test'\n    return Request('http://www.example.com', meta={'somearg': somearg})\n\ndef other_callback(self, response):\n    somearg = response.meta['somearg']\n    print \"the argument passed is:\", somearg\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/jobs.html", "title": ["Jobs: pausing and resuming crawls \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDjangoItem\u00b6\nDjangoItem is a class of item that gets its fields definition from a\nDjango model, you simply create a DjangoItem and specify what Django\nmodel it relates to.\nBesides of getting the model fields defined on your item, DjangoItem\nprovides a method to create and populate a Django model instance with the item\ndata.\n\nUsing DjangoItem\u00b6\nDjangoItem works much like ModelForms in Django, you create a subclass\nand define its django_model attribute to be a valid Django model. With this\nyou will get an item with a field for each Django model field.\nIn addition, you can define fields that aren\u2019t present in the model and even\noverride fields that are present in the model defining them in the item.\nLet\u2019s see some examples:\nDjango model for the examples:\nclass Person(models.Model):\n    name = models.CharField(max_length=255)\n    age = models.IntegerField()\n\n\nDefining a basic DjangoItem:\nclass PersonItem(DjangoItem):\n    django_model = Person\n\n\nDjangoItem work just like Item:\np = PersonItem()\np['name'] = 'John'\np['age'] = '22'\n\n\nTo obtain the Django model from the item, we call the extra method\nsave() of the DjangoItem:\n>>> person = p.save()\n>>> person.name\n'John'\n>>> person.age\n'22'\n>>> person.id\n1\n\n\nAs you see the model is already saved when we call save(), we\ncan prevent this by calling it with commit=False. We can use\ncommit=False in save() method to obtain an unsaved model:\n>>> person = p.save(commit=False)\n>>> person.name\n'John'\n>>> person.age\n'22'\n>>> person.id\nNone\n\n\nAs said before, we can add other fields to the item:\nclass PersonItem(DjangoItem):\n    django_model = Person\n    sex = Field()\n\np = PersonItem()\np['name'] = 'John'\np['age'] = '22'\np['sex'] = 'M'\n\n\n\nNote\nfields added to the item won\u2019t be taken into account when doing a save()\n\nAnd we can override the fields of the model with your own:\nclass PersonItem(DjangoItem):\n    django_model = Person\n    name = Field(default='No Name')\n\n\nThis is useful to provide properties to the field, like a default or any other\nproperty that your project uses.\n\n\nDjangoItem caveats\u00b6\nDjangoItem is a rather convenient way to integrate Scrapy projects with Django\nmodels, but bear in mind that Django ORM may not scale well if you scrape a lot\nof items (ie. millions) with Scrapy. This is because a relational backend is\noften not a good choice for a write intensive application (such as a web\ncrawler), specially if the database is highly normalized and with many indices.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/djangoitem.html", "title": ["DjangoItem \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nArchitecture overview\u00b6\nThis document describes the architecture of Scrapy and how its components\ninteract.\n\nOverview\u00b6\nThe following diagram shows an overview of the Scrapy architecture with its\ncomponents and an outline of the data flow that takes place inside the system\n(shown by the green arrows). A brief description of the components is included\nbelow with links for more detailed information about them. The data flow is\nalso described below.\n\n\n\nComponents\u00b6\n\nScrapy Engine\u00b6\nThe engine is responsible for controlling the data flow between all components\nof the system, and triggering events when certain actions occur. See the Data\nFlow section below for more details.\n\n\nScheduler\u00b6\nThe Scheduler receives requests from the engine and enqueues them for feeding\nthem later (also to the engine) when the engine requests them.\n\n\nDownloader\u00b6\nThe Downloader is responsible for fetching web pages and feeding them to the\nengine which, in turn, feeds them to the spiders.\n\n\nSpiders\u00b6\nSpiders are custom classes written by Scrapy users to parse responses and\nextract items (aka scraped items) from them or additional URLs (requests) to\nfollow. Each spider is able to handle a specific domain (or group of domains).\nFor more information see Spiders.\n\n\nItem Pipeline\u00b6\nThe Item Pipeline is responsible for processing the items once they have been\nextracted (or scraped) by the spiders. Typical tasks include cleansing,\nvalidation and persistence (like storing the item in a database). For more\ninformation see Item Pipeline.\n\n\nDownloader middlewares\u00b6\nDownloader middlewares are specific hooks that sit between the Engine and the\nDownloader and process requests when they pass from the Engine to the\nDownloader, and responses that pass from Downloader to the Engine. They provide\na convenient mechanism for extending Scrapy functionality by plugging custom\ncode. For more information see Downloader Middleware.\n\n\nSpider middlewares\u00b6\nSpider middlewares are specific hooks that sit between the Engine and the\nSpiders and are able to process spider input (responses) and output (items and\nrequests). They provide a convenient mechanism for extending Scrapy\nfunctionality by plugging custom code. For more information see\nSpider Middleware.\n\n\n\nData flow\u00b6\nThe data flow in Scrapy is controlled by the execution engine, and goes like\nthis:\nThe Engine opens a domain, locates the Spider that handles that domain, and\nasks the spider for the first URLs to crawl.\nThe Engine gets the first URLs to crawl from the Spider and schedules them\nin the Scheduler, as Requests.\nThe Engine asks the Scheduler for the next URLs to crawl.\nThe Scheduler returns the next URLs to crawl to the Engine and the Engine\nsends them to the Downloader, passing through the Downloader Middleware\n(request direction).\nOnce the page finishes downloading the Downloader generates a Response (with\nthat page) and sends it to the Engine, passing through the Downloader\nMiddleware (response direction).\nThe Engine receives the Response from the Downloader and sends it to the\nSpider for processing, passing through the Spider Middleware (input direction).\nThe Spider processes the Response and returns scraped Items and new Requests\n(to follow) to the Engine.\nThe Engine sends scraped Items (returned by the Spider) to the Item Pipeline\nand Requests (returned by spider) to the Scheduler\nThe process repeats (from step 2) until there are no more requests from the\nScheduler, and the Engine closes the domain.\n\n\nEvent-driven networking\u00b6\nScrapy is written with Twisted, a popular event-driven networking framework\nfor Python. Thus, it\u2019s implemented using a non-blocking (aka asynchronous) code\nfor concurrency.\nFor more information about asynchronous programming and Twisted see these\nlinks:\nAsynchronous Programming with Twisted\nTwisted - hello, asynchronous programming\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/architecture.html", "title": ["Architecture overview \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDownloader Middleware\u00b6\nThe downloader middleware is a framework of hooks into Scrapy\u2019s\nrequest/response processing.  It\u2019s a light, low-level system for globally\naltering Scrapy\u2019s requests and responses.\n\nActivating a downloader middleware\u00b6\nTo activate a downloader middleware component, add it to the\nDOWNLOADER_MIDDLEWARES setting, which is a dict whose keys are the\nmiddleware class paths and their values are the middleware orders.\nHere\u2019s an example:\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.middlewares.CustomDownloaderMiddleware': 543,\n}\n\n\nThe DOWNLOADER_MIDDLEWARES setting is merged with the\nDOWNLOADER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to\nbe overridden) and then sorted by order to get the final sorted list of enabled\nmiddlewares: the first middleware is the one closer to the engine and the last\nis the one closer to the downloader.\nTo decide which order to assign to your middleware see the\nDOWNLOADER_MIDDLEWARES_BASE setting and pick a value according to\nwhere you want to insert the middleware. The order does matter because each\nmiddleware performs a different action and your middleware could depend on some\nprevious (or subsequent) middleware being applied.\nIf you want to disable a built-in middleware (the ones defined in\nDOWNLOADER_MIDDLEWARES_BASE and enabled by default) you must define it\nin your project\u2019s DOWNLOADER_MIDDLEWARES setting and assign None\nas its value.  For example, if you want to disable the off-site middleware:\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.middlewares.CustomDownloaderMiddleware': 543,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n}\n\n\nFinally, keep in mind that some middlewares may need to be enabled through a\nparticular setting. See each middleware documentation for more info.\n\n\nWriting your own downloader middleware\u00b6\nWriting your own downloader middleware is easy. Each middleware component is a\nsingle Python class that defines one or more of the following methods:\n\nclass scrapy.contrib.downloadermiddleware.DownloaderMiddleware\u00b6\n\nprocess_request(request, spider)\u00b6\nThis method is called for each request that goes through the download\nmiddleware.\nprocess_request() should return either None, a\nResponse object, or a Request\nobject.\nIf it returns None, Scrapy will continue processing this request, executing all\nother middlewares until, finally, the appropriate downloader handler is called\nthe request performed (and its response downloaded).\nIf it returns a Response object, Scrapy won\u2019t bother\ncalling ANY other request or exception middleware, or the appropriate\ndownload function; it\u2019ll return that Response. Response middleware is\nalways called on every Response.\nIf it returns a Request object, the returned request will be\nrescheduled (in the Scheduler) to be downloaded in the future. The callback of\nthe original request will always be called. If the new request has a callback\nit will be called with the response downloaded, and the output of that callback\nwill then be passed to the original callback. If the new request doesn\u2019t have a\ncallback, the response downloaded will be just passed to the original request\ncallback.\nIf it returns an IgnoreRequest exception, the\nentire request will be dropped completely and its callback never called.\nParameters:request (Request object) \u2013 the request being processed\nspider (BaseSpider object) \u2013 the spider for which this request is intended\n\n\nprocess_response(request, response, spider)\u00b6\nprocess_response() should return a Response\nobject or raise a IgnoreRequest exception.\nIf it returns a Response (it could be the same given\nresponse, or a brand-new one), that response will continue to be processed\nwith the process_response() of the next middleware in the pipeline.\nIf it returns an IgnoreRequest exception, the\nresponse will be dropped completely and its callback never called.\nParameters:request (is a Request object) \u2013 the request that originated the response\nresponse (Response object) \u2013 the response being processed\nspider (BaseSpider object) \u2013 the spider for which this response is intended\n\n\nprocess_exception(request, exception, spider)\u00b6\nScrapy calls process_exception() when a download handler\nor a process_request() (from a downloader middleware) raises an\nexception.\nprocess_exception() should return either None,\nResponse or Request object.\nIf it returns None, Scrapy will continue processing this exception,\nexecuting any other exception middleware, until no middleware is left and\nthe default exception handling kicks in.\nIf it returns a Response object, the response middleware\nkicks in, and won\u2019t bother calling any other exception middleware.\nIf it returns a Request object, the returned request is\nused to instruct an immediate redirection.\nThe original request won\u2019t finish until the redirected\nrequest is completed. This stops the process_exception()\nmiddleware the same as returning Response would do.\nParameters:request (is a Request object) \u2013 the request that generated the exception\nexception (an Exception object) \u2013 the raised exception\nspider (BaseSpider object) \u2013 the spider for which this request is intended\n\n\n\nBuilt-in downloader middleware reference\u00b6\nThis page describes all downloader middleware components that come with\nScrapy. For information on how to use them and how to write your own downloader\nmiddleware, see the downloader middleware usage guide.\nFor a list of the components enabled by default (and their orders) see the\nDOWNLOADER_MIDDLEWARES_BASE setting.\n\nCookiesMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware\u00b6\nThis middleware enables working with sites that require cookies, such as\nthose that use sessions. It keeps track of cookies sent by web servers, and\nsend them back on subsequent requests (from that spider), just like web\nbrowsers do.\nThe following settings can be used to configure the cookie middleware:\nCOOKIES_ENABLED\nCOOKIES_DEBUG\n\nMultiple cookie sessions per spider\u00b6\n\nNew in version 0.15.\nThere is support for keeping multiple cookie sessions per spider by using the\ncookiejar Request meta key. By default it uses a single cookie jar\n(session), but you can pass an identifier to use different ones.\nFor example:\nfor i, url in enumerate(urls):\n    yield Request(\"http://www.example.com\", meta={'cookiejar': i},\n        callback=self.parse_page)\n\n\nKeep in mind that the cookiejar meta key is not \u201csticky\u201d. You need to keep\npassing it along on subsequent requests. For example:\ndef parse_page(self, response):\n    # do some processing\n    return Request(\"http://www.example.com/otherpage\",\n        meta={'cookiejar': response.meta['cookiejar']},\n        callback=self.parse_other_page)\n\n\n\n\nCOOKIES_ENABLED\u00b6\nDefault: True\nWhether to enable the cookies middleware. If disabled, no cookies will be sent\nto web servers.\n\n\nCOOKIES_DEBUG\u00b6\nDefault: False\nIf enabled, Scrapy will log all cookies sent in requests (ie. Cookie\nheader) and all cookies received in responses (ie. Set-Cookie header).\nHere\u2019s an example of a log with COOKIES_DEBUG enabled:\n2011-04-06 14:35:10-0300 [diningcity] INFO: Spider opened\n2011-04-06 14:35:10-0300 [diningcity] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>\n        Cookie: clientlanguage_nl=en_EN\n2011-04-06 14:35:14-0300 [diningcity] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>\n        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/\n        Set-Cookie: ip_isocode=US\n        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/\n2011-04-06 14:49:50-0300 [diningcity] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)\n[...]\n\n\n\n\nDefaultHeadersMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware\u00b6\nThis middleware sets all default requests headers specified in the\nDEFAULT_REQUEST_HEADERS setting.\n\n\nDownloadTimeoutMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware\u00b6\nThis middleware sets the download timeout for requests specified in the\nDOWNLOAD_TIMEOUT setting.\n\n\nHttpAuthMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware\u00b6\nThis middleware authenticates all requests generated from certain spiders\nusing Basic access authentication (aka. HTTP auth).\nTo enable HTTP authentication from certain spiders, set the http_user\nand http_pass attributes of those spiders.\nExample:\nclass SomeIntranetSiteSpider(CrawlSpider):\n\n    http_user = 'someuser'\n    http_pass = 'somepass'\n    name = 'intranet.example.com'\n\n    # .. rest of the spider code omitted ...\n\n\n\n\nHttpCacheMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\u00b6\nThis middleware provides low-level cache to all HTTP requests and responses.\nEvery request and its corresponding response are cached. When the same\nrequest is seen again, the response is returned without transferring\nanything from the Internet.\nThe HTTP cache is useful for testing spiders faster (without having to wait for\ndownloads every time) and for trying your spider offline, when an Internet\nconnection is not available.\nScrapy ships with two storage backends for the HTTP cache middleware:\nDBM storage backend (default)\nFile system backend\nYou can change the storage backend with the HTTPCACHE_STORAGE\nsetting. Or you can also implement your own backend.\n\nDBM storage backend (default)\u00b6\n\nNew in version 0.13.\nA DBM storage backend is available for the HTTP cache middleware. To use it\n(note: it is the default storage backend) set HTTPCACHE_STORAGE\nto scrapy.contrib.httpcache.DbmCacheStorage.\nBy default, it uses the anydbm module, but you can change it with the\nHTTPCACHE_DBM_MODULE setting.\n\n\nFile system backend\u00b6\nA file system storage backend is also available for the HTTP cache middleware.\nTo use it (instead of the default DBM storage backend) set HTTPCACHE_STORAGE\nto scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage.\nEach request/response pair is stored in a different directory containing\nthe following files:\n\nrequest_body - the plain request body\nrequest_headers - the request headers (in raw HTTP format)\nresponse_body - the plain response body\nresponse_headers - the request headers (in raw HTTP format)\nmeta - some metadata of this cache resource in Python repr() format\n(grep-friendly format)\npickled_meta - the same metadata in meta but pickled for more\nefficient deserialization\n\nThe directory name is made from the request fingerprint (see\nscrapy.utils.request.fingerprint), and one level of subdirectories is\nused to avoid creating too many files into the same directory (which is\ninefficient in many file systems). An example directory could be:\n/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7\n\n\n\nHTTPCache middleware settings\u00b6\nThe HttpCacheMiddleware can be configured through the following\nsettings:\n\nHTTPCACHE_ENABLED\u00b6\n\nNew in version 0.11.\nDefault: False\nWhether the HTTP cache will be enabled.\n\nChanged in version 0.11: Before 0.11, HTTPCACHE_DIR was used to enable cache.\n\n\nHTTPCACHE_EXPIRATION_SECS\u00b6\nDefault: 0\nExpiration time for cached requests, in seconds.\nCached requests older than this time will be re-downloaded. If zero, cached\nrequests will never expire.\n\nChanged in version 0.11: Before 0.11, zero meant cached requests always expire.\n\n\nHTTPCACHE_DIR\u00b6\nDefault: 'httpcache'\nThe directory to use for storing the (low-level) HTTP cache. If empty, the HTTP\ncache will be disabled. If a relative path is given, is taken relative to the\nproject data dir. For more info see: Default structure of Scrapy projects.\n\n\nHTTPCACHE_IGNORE_HTTP_CODES\u00b6\n\nNew in version 0.10.\nDefault: []\nDon\u2019t cache response with these HTTP codes.\n\n\nHTTPCACHE_IGNORE_MISSING\u00b6\nDefault: False\nIf enabled, requests not found in the cache will be ignored instead of downloaded.\n\n\nHTTPCACHE_IGNORE_SCHEMES\u00b6\n\nNew in version 0.10.\nDefault: ['file']\nDon\u2019t cache responses with these URI schemes.\n\n\nHTTPCACHE_STORAGE\u00b6\nDefault: 'scrapy.contrib.httpcache.DbmCacheStorage'\nThe class which implements the cache storage backend.\n\n\nHTTPCACHE_DBM_MODULE\u00b6\n\nNew in version 0.13.\nDefault: 'anydbm'\nThe database module to use in the DBM storage backend. This setting is specific to the DBM backend.\n\n\n\n\nHttpCompressionMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware\u00b6\nThis middleware allows compressed (gzip, deflate) traffic to be\nsent/received from web sites.\n\n\nChunkedTransferMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware\u00b6\nThis middleware adds support for chunked transfer encoding\n\n\nHttpProxyMiddleware\u00b6\n\nNew in version 0.8.\n\nclass scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware\u00b6\nThis middleware sets the HTTP proxy to use for requests, by setting the\nproxy meta value to Request objects.\nLike the Python standard library modules urllib and urllib2, it obeys\nthe following environment variables:\nhttp_proxy\nhttps_proxy\nno_proxy\n\n\nRedirectMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware\u00b6\nThis middleware handles redirection of requests based on response status and\nmeta-refresh html tag.\nThe urls which the request goes through (while being redirected) can be found\nin the redirect_urls Request.meta key.\nThe RedirectMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\nREDIRECT_ENABLED\nREDIRECT_MAX_TIMES\nREDIRECT_MAX_METAREFRESH_DELAY\nIf Request.meta contains the\ndont_redirect key, the request will be ignored by this middleware.\n\nRedirectMiddleware settings\u00b6\n\nREDIRECT_ENABLED\u00b6\n\nNew in version 0.13.\nDefault: True\nWhether the Redirect middleware will be enabled.\n\n\nREDIRECT_MAX_TIMES\u00b6\nDefault: 20\nThe maximum number of redirections that will be follow for a single request.\n\n\nREDIRECT_MAX_METAREFRESH_DELAY\u00b6\nDefault: 100\nThe maximum meta-refresh delay (in seconds) to follow the redirection.\n\n\n\n\nRetryMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.retry.RetryMiddleware\u00b6\nA middlware to retry failed requests that are potentially caused by\ntemporary problems such as a connection timeout or HTTP 500 error.\nFailed pages are collected on the scraping process and rescheduled at the\nend, once the spider has finished crawling all regular (non failed) pages.\nOnce there are no more failed pages to retry, this middleware sends a signal\n(retry_complete), so other extensions could connect to that signal.\nThe RetryMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\nRETRY_ENABLED\nRETRY_TIMES\nRETRY_HTTP_CODES\nAbout HTTP errors to consider:\nYou may want to remove 400 from RETRY_HTTP_CODES, if you stick to the\nHTTP protocol. It\u2019s included by default because it\u2019s a common code used\nto indicate server overload, which would be something we want to retry.\nIf Request.meta contains the dont_retry\nkey, the request will be ignored by this middleware.\n\nRetryMiddleware Settings\u00b6\n\nRETRY_ENABLED\u00b6\n\nNew in version 0.13.\nDefault: True\nWhether the Retry middleware will be enabled.\n\n\nRETRY_TIMES\u00b6\nDefault: 2\nMaximum number of times to retry, in addition to the first download.\n\n\nRETRY_HTTP_CODES\u00b6\nDefault: [500, 503, 504, 400, 408]\nWhich HTTP response codes to retry. Other errors (DNS lookup issues,\nconnections lost, etc) are always retried.\n\n\n\n\nRobotsTxtMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware\u00b6\nThis middleware filters out requests forbidden by the robots.txt exclusion\nstandard.\nTo make sure Scrapy respects robots.txt make sure the middleware is enabled\nand the ROBOTSTXT_OBEY setting is enabled.\n\nWarning\nKeep in mind that, if you crawl using multiple concurrent\nrequests per domain, Scrapy could still  download some forbidden pages\nif they were requested before the robots.txt file was downloaded. This\nis a known limitation of the current robots.txt middleware and will\nbe fixed in the future.\n\n\n\nDownloaderStats\u00b6\n\nclass scrapy.contrib.downloadermiddleware.stats.DownloaderStats\u00b6\nMiddleware that stores stats of all requests, responses and exceptions that\npass through it.\nTo use this middleware you must enable the DOWNLOADER_STATS\nsetting.\n\n\nUserAgentMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware\u00b6\nMiddleware that allows spiders to override the default user agent.\nIn order for a spider to override the default user agent, its user_agent\nattribute must be set.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/downloader-middleware.html", "title": ["Downloader Middleware \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSpider Middleware\u00b6\nThe spider middleware is a framework of hooks into Scrapy\u2019s spider processing\nmechanism where you can plug custom functionality to process the requests that\nare sent to Spiders for processing and to process the responses\nand items that are generated from spiders.\n\nActivating a spider middleware\u00b6\nTo activate a spider middleware component, add it to the\nSPIDER_MIDDLEWARES setting, which is a dict whose keys are the\nmiddleware class path and their values are the middleware orders.\nHere\u2019s an example:\nSPIDER_MIDDLEWARES = {\n    'myproject.middlewares.CustomSpiderMiddleware': 543,\n}\n\n\nThe SPIDER_MIDDLEWARES setting is merged with the\nSPIDER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to\nbe overridden) and then sorted by order to get the final sorted list of enabled\nmiddlewares: the first middleware is the one closer to the engine and the last\nis the one closer to the spider.\nTo decide which order to assign to your middleware see the\nSPIDER_MIDDLEWARES_BASE setting and pick a value according to where\nyou want to insert the middleware. The order does matter because each\nmiddleware performs a different action and your middleware could depend on some\nprevious (or subsequent) middleware being applied.\nIf you want to disable a builtin middleware (the ones defined in\nSPIDER_MIDDLEWARES_BASE, and enabled by default) you must define it\nin your project SPIDER_MIDDLEWARES setting and assign None as its\nvalue.  For example, if you want to disable the off-site middleware:\nSPIDER_MIDDLEWARES = {\n    'myproject.middlewares.CustomSpiderMiddleware': 543,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': None,\n}\n\n\nFinally, keep in mind that some middlewares may need to be enabled through a\nparticular setting. See each middleware documentation for more info.\n\n\nWriting your own spider middleware\u00b6\nWriting your own spider middleware is easy. Each middleware component is a\nsingle Python class that defines one or more of the following methods:\n\nclass scrapy.contrib.spidermiddleware.SpiderMiddleware\u00b6\n\nprocess_spider_input(response, spider)\u00b6\nThis method is called for each response that goes through the spider\nmiddleware and into the spider, for processing.\nprocess_spider_input() should return None or raise an\nexception.\nIf it returns None, Scrapy will continue processing this response,\nexecuting all other middlewares until, finally, the response is handled\nto the spider for processing.\nIf it raises an exception, Scrapy won\u2019t bother calling any other spider\nmiddleware process_spider_input() and will call the request\nerrback.  The output of the errback is chained back in the other\ndirection for process_spider_output() to process it, or\nprocess_spider_exception() if it raised an exception.\nParameters:response (Response object) \u2013 the response being processed\nspider (BaseSpider object) \u2013 the spider for which this response is intended\n\n\nprocess_spider_output(response, result, spider)\u00b6\nThis method is called with the results returned from the Spider, after\nit has processed the response.\nprocess_spider_output() must return an iterable of\nRequest or Item objects.\nParameters:response (class:~scrapy.http.Response object) \u2013 the response which generated this output from the\nspider\nresult (an iterable of Request or\nItem objects) \u2013 the result returned by the spider\nspider (BaseSpider object) \u2013 the spider whose result is being processed\n\n\nprocess_spider_exception(response, exception, spider)\u00b6\nThis method is called when when a spider or process_spider_input()\nmethod (from other spider middleware) raises an exception.\nprocess_spider_exception() should return either None or an\niterable of Response or\nItem objects.\nIf it returns None, Scrapy will continue processing this exception,\nexecuting any other process_spider_exception() in the following\nmiddleware components, until no middleware components are left and the\nexception reaches the engine (where it\u2019s logged and discarded).\nIf it returns an iterable the process_spider_output() pipeline\nkicks in, and no other process_spider_exception() will be called.\nParameters:response (Response object) \u2013 the response being processed when the exception was\nraised\nexception (Exception object) \u2013 the exception raised\nspider (scrapy.spider.BaseSpider object) \u2013 the spider which raised the exception\n\n\nprocess_start_requests(start_requests, spider)\u00b6\n\nNew in version 0.15.\nThis method is called with the start requests of the spider, and works\nsimilarly to the process_spider_output() method, except that it\ndoesn\u2019t have a response associated and must return only requests (not\nitems).\nIt receives an iterable (in the start_requests parameter) and must\nreturn another iterable of Request objects.\n\nNote\nWhen implementing this method in your spider middleware, you\nshould always return an iterable (that follows the input one) and\nnot consume all start_requests iterator because it can be very\nlarge (or even unbounded) and cause a memory overflow. The Scrapy\nengine is designed to pull start requests while it has capacity to\nprocess them, so the start requests iterator can be effectively\nendless where there is some other condition for stopping the spider\n(like a time limit or item/page count).\n\nParameters:start_requests (an iterable of Request) \u2013 the start requests\nspider (BaseSpider object) \u2013 the spider to whom the start requests belong\n\n\n\nBuilt-in spider middleware reference\u00b6\nThis page describes all spider middleware components that come with Scrapy. For\ninformation on how to use them and how to write your own spider middleware, see\nthe spider middleware usage guide.\nFor a list of the components enabled by default (and their orders) see the\nSPIDER_MIDDLEWARES_BASE setting.\n\nDepthMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.depth.DepthMiddleware\u00b6\nDepthMiddleware is a scrape middleware used for tracking the depth of each\nRequest inside the site being scraped. It can be used to limit the maximum\ndepth to scrape or things like that.\nThe DepthMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\n\nDEPTH_LIMIT - The maximum depth that will be allowed to\ncrawl for any site. If zero, no limit will be imposed.\nDEPTH_STATS - Whether to collect depth stats.\nDEPTH_PRIORITY - Whether to prioritize the requests based on\ntheir depth.\n\n\n\nHttpErrorMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware\u00b6\nFilter out unsuccessful (erroneous) HTTP responses so that spiders don\u2019t\nhave to deal with them, which (most of the time) imposes an overhead,\nconsumes more resources, and makes the spider logic more complex.\nAccording to the HTTP standard, successful responses are those whose\nstatus codes are in the 200-300 range.\nIf you still want to process response codes outside that range, you can\nspecify which response codes the spider is able to handle using the\nhandle_httpstatus_list spider attribute.\nFor example, if you want your spider to handle 404 responses you can do\nthis:\nclass MySpider(CrawlSpider):\n    handle_httpstatus_list = [404]\n\n\nThe handle_httpstatus_list key of Request.meta can also be used to specify which response codes to\nallow on a per-request basis.\nKeep in mind, however, that it\u2019s usually a bad idea to handle non-200\nresponses, unless you really know what you\u2019re doing.\nFor more information see: HTTP Status Code Definitions.\n\n\nOffsiteMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware\u00b6\nFilters out Requests for URLs outside the domains covered by the spider.\nThis middleware filters out every request whose host names aren\u2019t in the\nspider\u2019s allowed_domains attribute.\nWhen your spider returns a request for a domain not belonging to those\ncovered by the spider, this middleware will log a debug message similar to\nthis one:\nDEBUG: Filtered offsite request to 'www.othersite.com': <GET http://www.othersite.com/some/page.html>\n\nTo avoid filling the log with too much noise, it will only print one of\nthese messages for each new domain filtered. So, for example, if another\nrequest for www.othersite.com is filtered, no log message will be\nprinted. But if a request for someothersite.com is filtered, a message\nwill be printed (but only for the first request filtered).\nIf the spider doesn\u2019t define an\nallowed_domains attribute, or the\nattribute is empty, the offsite middleware will allow all requests.\nIf the request has the dont_filter attribute\nset, the offsite middleware will allow the request even if its domain is not\nlisted in allowed domains.\n\n\nRefererMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.referer.RefererMiddleware\u00b6\nPopulates Request referer field, based on the Response which originated it.\n\nRefererMiddleware settings\u00b6\n\nREFERER_ENABLED\u00b6\n\nNew in version 0.15.\nDefault: True\nWhether to enable referer middleware.\n\n\n\n\nUrlLengthMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware\u00b6\nFilters out requests with URLs longer than URLLENGTH_LIMIT\nThe UrlLengthMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\n\nURLLENGTH_LIMIT - The maximum URL length to allow for crawled URLs.\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/spider-middleware.html", "title": ["Spider Middleware \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExtensions\u00b6\nThe extensions framework provides a mechanism for inserting your own\ncustom functionality into Scrapy.\nExtensions are just regular classes that are instantiated at Scrapy startup,\nwhen extensions are initialized.\n\nExtension settings\u00b6\nExtensions use the Scrapy settings to manage their\nsettings, just like any other Scrapy code.\nIt is customary for extensions to prefix their settings with their own name, to\navoid collision with existing (and future) extensions. For example, an\nhypothetic extension to handle Google Sitemaps would use settings like\nGOOGLESITEMAP_ENABLED, GOOGLESITEMAP_DEPTH, and so on.\n\n\nLoading & activating extensions\u00b6\nExtensions are loaded and activated at startup by instantiating a single\ninstance of the extension class. Therefore, all the extension initialization\ncode must be performed in the class constructor (__init__ method).\nTo make an extension available, add it to the EXTENSIONS setting in\nyour Scrapy settings. In EXTENSIONS, each extension is represented\nby a string: the full Python path to the extension\u2019s class name. For example:\nEXTENSIONS = {\n    'scrapy.contrib.corestats.CoreStats': 500,\n    'scrapy.webservice.WebService': 500,\n    'scrapy.telnet.TelnetConsole': 500,\n}\n\n\nAs you can see, the EXTENSIONS setting is a dict where the keys are\nthe extension paths, and their values are the orders, which define the\nextension loading order. Extensions orders are not as important as middleware\norders though, and they are typically irrelevant, ie. it doesn\u2019t matter in\nwhich order the extensions are loaded because they don\u2019t depend on each other\n[1].\nHowever, this feature can be exploited if you need to add an extension which\ndepends on other extensions already loaded.\n[1] This is is why the EXTENSIONS_BASE setting in Scrapy (which\ncontains all built-in extensions enabled by default) defines all the extensions\nwith the same order (500).\n\n\nAvailable, enabled and disabled extensions\u00b6\nNot all available extensions will be enabled. Some of them usually depend on a\nparticular setting. For example, the HTTP Cache extension is available by default\nbut disabled unless the HTTPCACHE_ENABLED setting is set.\n\n\nDisabling an extension\u00b6\nIn order to disable an extension that comes enabled by default (ie. those\nincluded in the EXTENSIONS_BASE setting) you must set its order to\nNone. For example:\nEXTENSIONS = {\n    'scrapy.contrib.corestats.CoreStats': None,\n}\n\n\n\n\nWriting your own extension\u00b6\nWriting your own extension is easy. Each extension is a single Python class\nwhich doesn\u2019t need to implement any particular method.\nThe main entry point for a Scrapy extension (this also includes middlewares and\npipelines) is the from_crawler class method which receives a\nCrawler instance which is the main object controlling the Scrapy crawler.\nThrough that object you can access settings, signals, stats, and also control\nthe crawler behaviour, if your extension needs to such thing.\nTypically, extensions connect to signals and perform\ntasks triggered by them.\nFinally, if the from_crawler method raises the\nNotConfigured exception, the extension will be\ndisabled. Otherwise, the extension will be enabled.\n\nSample extension\u00b6\nHere we will implement a simple extension to illustrate the concepts described\nin the previous section. This extension will log a message every time:\na spider is opened\na spider is closed\na specific number of items are scraped\nThe extension will be enabled through the MYEXT_ENABLED setting and the\nnumber of items will be specified through the MYEXT_ITEMCOUNT setting.\nHere is the code of such extension:\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\n\nclass SpiderOpenCloseLogging(object):\n\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool('MYEXT_ENABLED'):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint('MYEXT_ITEMCOUNT', 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        spider.log(\"opened spider %s\" % spider.name)\n\n    def spider_closed(self, spider):\n        spider.log(\"closed spider %s\" % spider.name)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped == self.item_count:\n            spider.log(\"scraped %d items, resetting counter\" % self.items_scraped)\n            self.item_count = 0\n\n\n\n\n\nBuilt-in extensions reference\u00b6\n\nGeneral purpose extensions\u00b6\n\nLog Stats extension\u00b6\n\nclass scrapy.contrib.logstats.LogStats\u00b6\nLog basic stats like crawled pages and scraped items.\n\n\nCore Stats extension\u00b6\n\nclass scrapy.contrib.corestats.CoreStats\u00b6\nEnable the collection of core statistics, provided the stats collection is\nenabled (see Stats Collection).\n\n\nWeb service extension\u00b6\n\nclass scrapy.webservice.WebService\u00b6\nSee topics-webservice.\n\n\nTelnet console extension\u00b6\n\nclass scrapy.telnet.TelnetConsole\u00b6\nProvides a telnet console for getting into a Python interpreter inside the\ncurrently running Scrapy process, which can be very useful for debugging.\nThe telnet console must be enabled by the TELNETCONSOLE_ENABLED\nsetting, and the server will listen in the port specified in\nTELNETCONSOLE_PORT.\n\n\nMemory usage extension\u00b6\n\nclass scrapy.contrib.memusage.MemoryUsage\u00b6\n\nNote\nThis extension does not work in Windows.\n\nMonitors the memory used by the Scrapy process that runs the spider and:\n1, sends a notification e-mail when it exceeds a certain value\n2. closes the spider when it exceeds a certain value\nThe notification e-mails can be triggered when a certain warning value is\nreached (MEMUSAGE_WARNING_MB) and when the maximum value is reached\n(MEMUSAGE_LIMIT_MB) which will also cause the spider to be closed\nand the Scrapy process to be terminated.\nThis extension is enabled by the MEMUSAGE_ENABLED setting and\ncan be configured with the following settings:\nMEMUSAGE_LIMIT_MB\nMEMUSAGE_WARNING_MB\nMEMUSAGE_NOTIFY_MAIL\nMEMUSAGE_REPORT\n\n\nMemory debugger extension\u00b6\n\nclass scrapy.contrib.memdebug.MemoryDebugger\u00b6\nAn extension for debugging memory usage. It collects information about:\nobjects uncollected by the Python garbage collector\nlibxml2 memory leaks\nobjects left alive that shouldn\u2019t. For more info, see Debugging memory leaks with trackref\nTo enable this extension, turn on the MEMDEBUG_ENABLED setting. The\ninfo will be stored in the stats.\n\n\nClose spider extension\u00b6\n\nclass scrapy.contrib.closespider.CloseSpider\u00b6\nCloses a spider automatically when some conditions are met, using a specific\nclosing reason for each condition.\nThe conditions for closing a spider can be configured through the following\nsettings:\nCLOSESPIDER_TIMEOUT\nCLOSESPIDER_ITEMCOUNT\nCLOSESPIDER_PAGECOUNT\nCLOSESPIDER_ERRORCOUNT\n\nCLOSESPIDER_TIMEOUT\u00b6\nDefault: 0\nAn integer which specifies a number of seconds. If the spider remains open for\nmore than that number of second, it will be automatically closed with the\nreason closespider_timeout. If zero (or non set), spiders won\u2019t be closed by\ntimeout.\n\n\nCLOSESPIDER_ITEMCOUNT\u00b6\nDefault: 0\nAn integer which specifies a number of items. If the spider scrapes more than\nthat amount if items and those items are passed by the item pipeline, the\nspider will be closed with the reason closespider_itemcount. If zero (or\nnon set), spiders won\u2019t be closed by number of passed items.\n\n\nCLOSESPIDER_PAGECOUNT\u00b6\n\nNew in version 0.11.\nDefault: 0\nAn integer which specifies the maximum number of responses to crawl. If the spider\ncrawls more than that, the spider will be closed with the reason\nclosespider_pagecount. If zero (or non set), spiders won\u2019t be closed by\nnumber of crawled responses.\n\n\nCLOSESPIDER_ERRORCOUNT\u00b6\n\nNew in version 0.11.\nDefault: 0\nAn integer which specifies the maximum number of errors to receive before\nclosing the spider. If the spider generates more than that number of errors,\nit will be closed with the reason closespider_errorcount. If zero (or non\nset), spiders won\u2019t be closed by number of errors.\n\n\n\nStatsMailer extension\u00b6\n\nclass scrapy.contrib.statsmailer.StatsMailer\u00b6\nThis simple extension can be used to send a notification e-mail every time a\ndomain has finished scraping, including the Scrapy stats collected. The email\nwill be sent to all recipients specified in the STATSMAILER_RCPTS\nsetting.\n\n\n\nDebugging extensions\u00b6\n\nStack trace dump extension\u00b6\n\nclass scrapy.contrib.debug.StackTraceDump\u00b6\nDumps information about the running process when a SIGQUIT or SIGUSR2\nsignal is received. The information dumped is the following:\nengine status (using scrapy.utils.engine.get_engine_status())\nlive references (see Debugging memory leaks with trackref)\nstack trace of all threads\nAfter the stack trace and engine status is dumped, the Scrapy process continues\nrunning normally.\nThis extension only works on POSIX-compliant platforms (ie. not Windows),\nbecause the SIGQUIT and SIGUSR2 signals are not available on Windows.\nThere are at least two ways to send Scrapy the SIGQUIT signal:\nBy pressing Ctrl-while a Scrapy process is running (Linux only?)\n\nBy running this command (assuming <pid> is the process id of the Scrapy\nprocess):\nkill -QUIT <pid>\n\n\n\n\nDebugger extension\u00b6\n\nclass scrapy.contrib.debug.Debugger\u00b6\nInvokes a Python debugger inside a running Scrapy process when a SIGUSR2\nsignal is received. After the debugger is exited, the Scrapy process continues\nrunning normally.\nFor more info see Debugging in Python.\nThis extension only works on POSIX-compliant platforms (ie. not Windows).\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/extensions.html", "title": ["Extensions \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nCore API\u00b6\n\nNew in version 0.15.\nThis section documents the Scrapy core API, and it\u2019s intended for developers of\nextensions and middlewares.\n\nCrawler API\u00b6\nThe main entry point to Scrapy API is the Crawler\nobject, passed to extensions through the from_crawler class method. This\nobject provides access to all Scrapy core components, and it\u2019s the only way for\nextensions to access them and hook their functionality into Scrapy.\nThe Extension Manager is responsible for loading and keeping track of installed\nextensions and it\u2019s configured through the EXTENSIONS setting which\ncontains a dictionary of all available extensions and their order similar to\nhow you configure the downloader middlewares.\n\nclass scrapy.crawler.Crawler(settings)\u00b6\nThe Crawler object must be instantiated with a\nscrapy.settings.Settings object.\n\nsettings\u00b6\nThe settings manager of this crawler.\nThis is used by extensions & middlewares to access the Scrapy settings\nof this crawler.\nFor an introduction on Scrapy settings see Settings.\nFor the API see Settings class.\n\nsignals\u00b6\nThe signals manager of this crawler.\nThis is used by extensions & middlewares to hook themselves into Scrapy\nfunctionality.\nFor an introduction on signals see Signals.\nFor the API see SignalManager class.\n\nstats\u00b6\nThe stats collector of this crawler.\nThis is used from extensions & middlewares to record stats of their\nbehaviour, or access stats collected by other extensions.\nFor an introduction on stats collection see Stats Collection.\nFor the API see StatsCollector class.\n\nextensions\u00b6\nThe extension manager that keeps track of enabled extensions.\nMost extensions won\u2019t need to access this attribute.\nFor an introduction on extensions and a list of available extensions on\nScrapy see Extensions.\n\nspiders\u00b6\nThe spider manager which takes care of loading and instantiating\nspiders.\nMost extensions won\u2019t need to access this attribute.\n\nengine\u00b6\nThe execution engine, which coordinates the core crawling logic\nbetween the scheduler, downloader and spiders.\nSome extension may want to access the Scrapy engine, to modify inspect\nor modify the downloader and scheduler behaviour, although this is an\nadvanced use and this API is not yet stable.\n\nconfigure()\u00b6\nConfigure the crawler.\nThis loads extensions, middlewares and spiders, leaving the crawler\nready to be started. It also configures the execution engine.\n\nstart()\u00b6\nStart the crawler. This calls configure() if it hasn\u2019t been called yet.\n\n\nSettings API\u00b6\n\nclass scrapy.settings.Settings\u00b6\nThis object that provides access to Scrapy settings.\n\noverrides\u00b6\nGlobal overrides are the ones that take most precedence, and are usually\npopulated by command-line options.\nOverrides should be populated before configuring the Crawler object\n(through the configure() method),\notherwise they won\u2019t have any effect. You don\u2019t typically need to worry\nabout overrides unless you are implementing your own Scrapy command.\n\nget(name, default=None)\u00b6\nGet a setting value without affecting its original type.\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetbool(name, default=False)\u00b6\nGet a setting value as a boolean. For example, both 1 and '1', and\nTrue return True, while 0, '0', False and None\nreturn False``\nFor example, settings populated through environment variables set to '0'\nwill return False when using this method.\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetint(name, default=0)\u00b6\nGet a setting value as an int\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetfloat(name, default=0.0)\u00b6\nGet a setting value as a float\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetlist(name, default=None)\u00b6\nGet a setting value as a list. If the setting original type is a list it\nwill be returned verbatim. If it\u2019s a string it will be split by \u201d,\u201d.\nFor example, settings populated through environment variables set to\n'one,two' will return a list [\u2018one\u2019, \u2018two\u2019] when using this method.\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\n\nSignals API\u00b6\n\nclass scrapy.signalmanager.SignalManager\u00b6\n\nconnect(receiver, signal)\u00b6\nConnect a receiver function to a signal.\nThe signal can be any object, although Scrapy comes with some\npredefined signals that are documented in the Signals\nsection.\nParameters:receiver (callable) \u2013 the function to be connected\nsignal (object) \u2013 the signal to connect to\n\n\nsend_catch_log(signal, **kwargs)\u00b6\nSend a signal, catch exceptions and log them.\nThe keyword arguments are passed to the signal handlers (connected\nthrough the connect() method).\n\nsend_catch_log_deferred(signal, **kwargs)\u00b6\nLike send_catch_log() but supports returning deferreds from\nsignal handlers.\nReturns a deferred that gets fired once all signal handlers\ndeferreds were fired. Send a signal, catch exceptions and log them.\nThe keyword arguments are passed to the signal handlers (connected\nthrough the connect() method).\n\ndisconnect(receiver, signal)\u00b6\nDisconnect a receiver function from a signal. This has the opposite\neffect of the connect() method, and the arguments are the same.\n\ndisconnect_all(signal)\u00b6\nDisconnect all receivers from the given signal.\nParameters:signal (object) \u2013 the signal to disconnect from\n\n\nStats Collector API\u00b6\nThere are several Stats Collectors available under the\nscrapy.statscol module and they all implement the Stats\nCollector API defined by the StatsCollector\nclass (which they all inherit from).\n\nclass scrapy.statscol.StatsCollector\u00b6\n\nget_value(key, default=None)\u00b6\nReturn the value for the given stats key or default if it doesn\u2019t exist.\n\nget_stats()\u00b6\nGet all stats from the currently running spider as a dict.\n\nset_value(key, value)\u00b6\nSet the given value for the given stats key.\n\nset_stats(stats)\u00b6\nOverride the current stats with the dict passed in stats argument.\n\ninc_value(key, count=1, start=0)\u00b6\nIncrement the value of the given stats key, by the given count,\nassuming the start value given (when it\u2019s not set).\n\nmax_value(key, value)\u00b6\nSet the given value for the given key only if current value for the\nsame key is lower than value. If there is no current value for the\ngiven key, the value is always set.\n\nmin_value(key, value)\u00b6\nSet the given value for the given key only if current value for the\nsame key is greater than value. If there is no current value for the\ngiven key, the value is always set.\n\nclear_stats()\u00b6\nClear all stats.\nThe following methods are not part of the stats collection api but instead\nused when implementing custom stats collectors:\n\nopen_spider(spider)\u00b6\nOpen the given spider for stats collection.\n\nclose_spider(spider)\u00b6\nClose the given spider. After this is called, no more specific stats\ncan be accessed or collected.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/api.html", "title": ["Core API \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nRequests and Responses\u00b6\nScrapy uses Request and Response objects for crawling web\nsites.\nTypically, Request objects are generated in the spiders and pass\nacross the system until they reach the Downloader, which executes the request\nand returns a Response object which travels back to the spider that\nissued the request.\nBoth Request and Response classes have subclasses which add\nfunctionality not required in the base classes. These are described\nbelow in Request subclasses and\nResponse subclasses.\n\nRequest objects\u00b6\n\nclass scrapy.http.Request(url[, method='GET', body, headers, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, callback, errback])\u00b6\nA Request object represents an HTTP request, which is usually\ngenerated in the Spider and executed by the Downloader, and thus generating\na Response.\nParameters:url (string) \u2013 the URL of this request\nmethod (string) \u2013 the HTTP method of this request. Defaults to 'GET'.\nmeta (dict) \u2013 the initial values for the Request.meta attribute. If\ngiven, the dict passed in this parameter will be shallow copied.\nbody (str or unicode) \u2013 the request body. If a unicode is passed, then it\u2019s encoded to\nstr using the encoding passed (which defaults to utf-8). If\nbody is not given,, an empty string is stored. Regardless of the\ntype of this argument, the final value stored will be a str` (never\nunicode or None).\nheaders (dict) \u2013 the headers of this request. The dict values can be strings\n(for single valued headers) or lists (for multi-valued headers).\ncookies (dict or list) \u2013 the request cookies. These can be sent in two forms.\nUsing a dict:request_with_cookies = Request(url=\"http://www.example.com\",\n                               cookies={'currency': 'USD', 'country': 'UY'})\n\n\n\nUsing a list of dicts:request_with_cookies = Request(url=\"http://www.example.com\",\n                               cookies=[{'name': 'currency',\n                                        'value': 'USD',\n                                        'domain': 'example.com',\n                                        'path': '/currency'}])\n\n\n\nThe latter form allows for customizing the domain and path\nattributes of the cookie. These is only useful if the cookies are saved\nfor later requests.\nWhen some site returns cookies (in a response) those are stored in the\ncookies for that domain and will be sent again in future requests. That\u2019s\nthe typical behaviour of any regular web browser. However, if, for some\nreason, you want to avoid merging with existing cookies you can instruct\nScrapy to do so by setting the dont_merge_cookies key in the\nRequest.meta.\nExample of request without merging cookies:\nrequest_with_cookies = Request(url=\"http://www.example.com\",\n                               cookies={'currency': 'USD', 'country': 'UY'},\n                               meta={'dont_merge_cookies': True})\n\n\nFor more info see CookiesMiddleware.\n\nencoding (string) \u2013 the encoding of this request (defaults to 'utf-8').\nThis encoding will be used to percent-encode the URL and to convert the\nbody to str (if given as unicode).\npriority (int) \u2013 the priority of this request (defaults to 0).\nThe priority is used by the scheduler to define the order used to process\nrequests.\ndont_filter (boolean) \u2013 indicates that this request should not be filtered by\nthe scheduler. This is used when you want to perform an identical\nrequest multiple times, to ignore the duplicates filter. Use it with\ncare, or you will get into crawling loops. Default to False.\ncallback (callable) \u2013 the function that will be called with the response of this\nrequest (once its downloaded) as its first parameter. For more information\nsee Passing additional data to callback functions below.\nIf a Request doesn\u2019t specify a callback, the spider\u2019s\nparse() method will be used.\nerrback (callable) \u2013 a function that will be called if any exception was\nraised while processing the request. This includes pages that failed\nwith 404 HTTP errors and such. It receives a Twisted Failure instance\nas first parameter.\n\n\nurl\u00b6\nA string containing the URL of this request. Keep in mind that this\nattribute contains the escaped URL, so it can differ from the URL passed in\nthe constructor.\nThis attribute is read-only. To change the URL of a Request use\nreplace().\n\nmethod\u00b6\nA string representing the HTTP method in the request. This is guaranteed to\nbe uppercase. Example: \"GET\", \"POST\", \"PUT\", etc\n\nheaders\u00b6\nA dictionary-like object which contains the request headers.\n\nbody\u00b6\nA str that contains the request body.\nThis attribute is read-only. To change the body of a Request use\nreplace().\n\nmeta\u00b6\nA dict that contains arbitrary metadata for this request. This dict is\nempty for new Requests, and is usually  populated by different Scrapy\ncomponents (extensions, middlewares, etc). So the data contained in this\ndict depends on the extensions you have enabled.\nSee Request.meta special keys for a list of special meta keys\nrecognized by Scrapy.\nThis dict is shallow copied when the request is cloned using the\ncopy() or replace() methods, and can also be accessed, in your\nspider, from the response.meta attribute.\n\ncopy()\u00b6\nReturn a new Request which is a copy of this Request. See also:\nPassing additional data to callback functions.\n\nreplace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])\u00b6\nReturn a Request object with the same members, except for those members\ngiven new values by whichever keyword arguments are specified. The\nattribute Request.meta is copied by default (unless a new value\nis given in the meta argument). See also\nPassing additional data to callback functions.\n\nPassing additional data to callback functions\u00b6\nThe callback of a request is a function that will be called when the response\nof that request is downloaded. The callback function will be called with the\ndownloaded Response object as its first argument.\nExample:\ndef parse_page1(self, response):\n    return Request(\"http://www.example.com/some_page.html\",\n                      callback=self.parse_page2)\n\ndef parse_page2(self, response):\n    # this would log http://www.example.com/some_page.html\n    self.log(\"Visited %s\" % response.url)\n\n\nIn some cases you may be interested in passing arguments to those callback\nfunctions so you can receive the arguments later, in the second callback. You\ncan use the Request.meta attribute for that.\nHere\u2019s an example of how to pass an item using this mechanism, to populate\ndifferent fields from different pages:\ndef parse_page1(self, response):\n    item = MyItem()\n    item['main_url'] = response.url\n    request = Request(\"http://www.example.com/some_page.html\",\n                      callback=self.parse_page2)\n    request.meta['item'] = item\n    return request\n\ndef parse_page2(self, response):\n    item = response.meta['item']\n    item['other_url'] = response.url\n    return item\n\n\n\n\n\nRequest.meta special keys\u00b6\nThe Request.meta attribute can contain any arbitrary data, but there\nare some special keys recognized by Scrapy and its built-in extensions.\nThose are:\ndont_redirect\ndont_retry\nhandle_httpstatus_list\ndont_merge_cookies (see cookies parameter of Request constructor)\ncookiejar\nredirect_urls\n\n\nRequest subclasses\u00b6\nHere is the list of built-in Request subclasses. You can also subclass\nit to implement your own custom functionality.\n\nFormRequest objects\u00b6\nThe FormRequest class extends the base Request with functionality for\ndealing with HTML forms. It uses lxml.html forms  to pre-populate form\nfields with form data from Response objects.\n\nclass scrapy.http.FormRequest(url[, formdata, ...])\u00b6\nThe FormRequest class adds a new argument to the constructor. The\nremaining arguments are the same as for the Request class and are\nnot documented here.\nParameters:formdata (dict or iterable of tuples) \u2013 is a dictionary (or iterable of (key, value) tuples)\ncontaining HTML Form data which will be url-encoded and assigned to the\nbody of the request.\nThe FormRequest objects support the following class method in\naddition to the standard Request methods:\n\nclassmethod from_response(response[, formname=None, formnumber=0, formdata=None, dont_click=False, ...])\u00b6\nReturns a new FormRequest object with its form field values\npre-populated with those found in the HTML <form> element contained\nin the given response. For an example see\nUsing FormRequest.from_response() to simulate a user login.\nThe policy is to automatically simulate a click, by default, on any form\ncontrol that looks clickable, like a <input type=\"submit\">.  Even\nthough this is quite convenient, and often the desired behaviour,\nsometimes it can cause problems which could be hard to debug. For\nexample, when working with forms that are filled and/or submitted using\njavascript, the default from_response() behaviour may not be the\nmost appropriate. To disable this behaviour you can set the\ndont_click argument to True. Also, if you want to change the\ncontrol clicked (instead of disabling it) you can also use the\nclickdata argument.\nParameters:response (Response object) \u2013 the response containing a HTML form which will be used\nto pre-populate the form fields\nformname (string) \u2013 if given, the form with name attribute set to this value\nwill be used. Otherwise, formnumber will be used for selecting\nthe form.\nformnumber (integer) \u2013 the number of form to use, when the response contains\nmultiple forms. The first one (and also the default) is 0.\nformdata (dict) \u2013 fields to override in the form data. If a field was\nalready present in the response <form> element, its value is\noverridden by the one passed in this parameter.\ndont_click (boolean) \u2013 If True, the form data will be submitted without\nclicking in any element.\n\nThe other parameters of this class method are passed directly to the\nFormRequest constructor.\n\nNew in version 0.10.3: The formname parameter.\n\n\nRequest usage examples\u00b6\n\nUsing FormRequest to send data via HTTP POST\u00b6\nIf you want to simulate a HTML Form POST in your spider and send a couple of\nkey-value fields, you can return a FormRequest object (from your\nspider) like this:\nreturn [FormRequest(url=\"http://www.example.com/post/action\",\n                    formdata={'name': 'John Doe', age: '27'},\n                    callback=self.after_post)]\n\n\n\n\nUsing FormRequest.from_response() to simulate a user login\u00b6\nIt is usual for web sites to provide pre-populated form fields through <input\ntype=\"hidden\"> elements, such as session related data or authentication\ntokens (for login pages). When scraping, you\u2019ll want these fields to be\nautomatically pre-populated and only override a couple of them, such as the\nuser name and password. You can use the FormRequest.from_response()\nmethod for this job. Here\u2019s an example spider which uses it:\nclass LoginSpider(BaseSpider):\n    name = 'example.com'\n    start_urls = ['http://www.example.com/users/login.php']\n\n    def parse(self, response):\n        return [FormRequest.from_response(response,\n                    formdata={'username': 'john', 'password': 'secret'},\n                    callback=self.after_login)]\n\n    def after_login(self, response):\n        # check login succeed before going on\n        if \"authentication failed\" in response.body:\n            self.log(\"Login failed\", level=log.ERROR)\n            return\n\n        # continue scraping with authenticated session...\n\n\n\n\n\n\nResponse objects\u00b6\n\nclass scrapy.http.Response(url[, status=200, headers, body, flags])\u00b6\nA Response object represents an HTTP response, which is usually\ndownloaded (by the Downloader) and fed to the Spiders for processing.\nParameters:url (string) \u2013 the URL of this response\nheaders (dict) \u2013 the headers of this response. The dict values can be strings\n(for single valued headers) or lists (for multi-valued headers).\nstatus (integer) \u2013 the HTTP status of the response. Defaults to 200.\nbody (str) \u2013 the response body. It must be str, not unicode, unless you\u2019re\nusing a encoding-aware Response subclass, such as\nTextResponse.\nmeta (dict) \u2013 the initial values for the Response.meta attribute. If\ngiven, the dict will be shallow copied.\nflags (list) \u2013 is a list containing the initial values for the\nResponse.flags attribute. If given, the list will be shallow\ncopied.\n\n\nurl\u00b6\nA string containing the URL of the response.\nThis attribute is read-only. To change the URL of a Response use\nreplace().\n\nstatus\u00b6\nAn integer representing the HTTP status of the response. Example: 200,\n404.\n\nheaders\u00b6\nA dictionary-like object which contains the response headers.\n\nbody\u00b6\nA str containing the body of this Response. Keep in mind that Reponse.body\nis always a str. If you want the unicode version use\nTextResponse.body_as_unicode() (only available in\nTextResponse and subclasses).\nThis attribute is read-only. To change the body of a Response use\nreplace().\n\nrequest\u00b6\nThe Request object that generated this response. This attribute is\nassigned in the Scrapy engine, after the response and the request have passed\nthrough all Downloader Middlewares.\nIn particular, this means that:\nHTTP redirections will cause the original request (to the URL before\nredirection) to be assigned to the redirected response (with the final\nURL after redirection).\nResponse.request.url doesn\u2019t always equal Response.url\nThis attribute is only available in the spider code, and in the\nSpider Middlewares, but not in\nDownloader Middlewares (although you have the Request available there by\nother means) and handlers of the response_downloaded signal.\n\nmeta\u00b6\nA shortcut to the Request.meta attribute of the\nResponse.request object (ie. self.request.meta).\nUnlike the Response.request attribute, the Response.meta\nattribute is propagated along redirects and retries, so you will get\nthe original Request.meta sent from your spider.\n\nSee also\nRequest.meta attribute\n\n\nflags\u00b6\nA list that contains flags for this response. Flags are labels used for\ntagging Responses. For example: \u2018cached\u2019, \u2018redirected\u2018, etc. And\nthey\u2019re shown on the string representation of the Response (__str__\nmethod) which is used by the engine for logging.\n\ncopy()\u00b6\nReturns a new Response which is a copy of this Response.\n\nreplace([url, status, headers, body, meta, flags, cls])\u00b6\nReturns a Response object with the same members, except for those members\ngiven new values by whichever keyword arguments are specified. The\nattribute Response.meta is copied by default (unless a new value\nis given in the meta argument).\n\n\nResponse subclasses\u00b6\nHere is the list of available built-in Response subclasses. You can also\nsubclass the Response class to implement your own functionality.\n\nTextResponse objects\u00b6\n\nclass scrapy.http.TextResponse(url[, encoding[, ...]])\u00b6\nTextResponse objects adds encoding capabilities to the base\nResponse class, which is meant to be used only for binary data,\nsuch as images, sounds or any media file.\nTextResponse objects support a new constructor argument, in\naddition to the base Response objects. The remaining functionality\nis the same as for the Response class and is not documented here.\nParameters:encoding (string) \u2013 is a string which contains the encoding to use for this\nresponse. If you create a TextResponse object with a unicode\nbody, it will be encoded using this encoding (remember the body attribute\nis always a string). If encoding is None (default value), the\nencoding will be looked up in the response headers and body instead.\nTextResponse objects support the following attributes in addition\nto the standard Response ones:\n\nencoding\u00b6\nA string with the encoding of this response. The encoding is resolved by\ntrying the following mechanisms, in order:\nthe encoding passed in the constructor encoding argument\nthe encoding declared in the Content-Type HTTP header. If this\nencoding is not valid (ie. unknown), it is ignored and the next\nresolution mechanism is tried.\nthe encoding declared in the response body. The TextResponse class\ndoesn\u2019t provide any special functionality for this. However, the\nHtmlResponse and XmlResponse classes do.\nthe encoding inferred by looking at the response body. This is the more\nfragile method but also the last one tried.\nTextResponse objects support the following methods in addition to\nthe standard Response ones:\n\nbody_as_unicode()\u00b6\nReturns the body of the response as unicode. This is equivalent to:\nresponse.body.decode(response.encoding)\n\n\nBut not equivalent to:\nunicode(response.body)\n\n\nSince, in the latter case, you would be using you system default encoding\n(typically ascii) to convert the body to uniode, instead of the response\nencoding.\n\n\nHtmlResponse objects\u00b6\n\nclass scrapy.http.HtmlResponse(url[, ...])\u00b6\nThe HtmlResponse class is a subclass of TextResponse\nwhich adds encoding auto-discovering support by looking into the HTML meta\nhttp-equiv attribute.  See TextResponse.encoding.\n\n\nXmlResponse objects\u00b6\n\nclass scrapy.http.XmlResponse(url[, ...])\u00b6\nThe XmlResponse class is a subclass of TextResponse which\nadds encoding auto-discovering support by looking into the XML declaration\nline.  See TextResponse.encoding.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/request-response.html", "title": ["Requests and Responses \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSettings\u00b6\nThe Scrapy settings allows you to customize the behaviour of all Scrapy\ncomponents, including the core, extensions, pipelines and spiders themselves.\nThe infrastructure of the settings provides a global namespace of key-value mappings\nthat the code can use to pull configuration values from. The settings can be\npopulated through different mechanisms, which are described below.\nThe settings are also the mechanism for selecting the currently active Scrapy\nproject (in case you have many).\nFor a list of available built-in settings see: Built-in settings reference.\n\nDesignating the settings\u00b6\nWhen you use Scrapy, you have to tell it which settings you\u2019re using. You can\ndo this by using an environment variable, SCRAPY_SETTINGS_MODULE.\nThe value of SCRAPY_SETTINGS_MODULE should be in Python path syntax, e.g.\nmyproject.settings. Note that the settings module should be on the\nPython import search path.\n\n\nPopulating the settings\u00b6\nSettings can be populated using different mechanisms, each of which having a\ndifferent precedence. Here is the list of them in decreasing order of\nprecedence:\n\nGlobal overrides (most precedence)\nProject settings module\nDefault settings per-command\nDefault global settings (less precedence)\n\nThese mechanisms are described in more detail below.\n\n1. Global overrides\u00b6\nGlobal overrides are the ones that take most precedence, and are usually\npopulated by command-line options. You can also override one (or more) settings\nfrom command line using the -s (or --set) command line option.\nFor more information see the overrides\nSettings attribute.\nExample:\nscrapy crawl domain.com -s LOG_FILE=scrapy.log\n\n\n\n\n2. Project settings module\u00b6\nThe project settings module is the standard configuration file for your Scrapy\nproject.  It\u2019s where most of your custom settings will be populated. For\nexample:: myproject.settings.\n\n\n3. Default settings per-command\u00b6\nEach Scrapy tool command can have its own default\nsettings, which override the global default settings. Those custom command\nsettings are specified in the default_settings attribute of the command\nclass.\n\n\n4. Default global settings\u00b6\nThe global defaults are located in the scrapy.settings.default_settings\nmodule and documented in the Built-in settings reference section.\n\n\n\nHow to access settings\u00b6\nSettings can be accessed through the scrapy.crawler.Crawler.settings\nattribute of the Crawler that is passed to from_crawler method in\nextensions and middlewares:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\nIn other words, settings can be accessed like a dict, but it\u2019s usually preferred\nto extract the setting in the format you need it to avoid type errors. In order\nto do that you\u2019ll have to use one of the methods provided the\nSettings API.\n\n\nRationale for setting names\u00b6\nSetting names are usually prefixed with the component that they configure. For\nexample, proper setting names for a fictional robots.txt extension would be\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR, etc.\n\n\nBuilt-in settings reference\u00b6\nHere\u2019s a list of all available Scrapy settings, in alphabetical order, along\nwith their default values and the scope where they apply.\nThe scope, where available, shows where the setting is being used, if it\u2019s tied\nto any particular component. In that case the module of that component will be\nshown, typically an extension, middleware or pipeline. It also means that the\ncomponent must be enabled in order for the setting to have any effect.\n\nAWS_ACCESS_KEY_ID\u00b6\nDefault: None\nThe AWS access key used by code that requires access to Amazon Web services,\nsuch as the S3 feed storage backend.\n\n\nAWS_SECRET_ACCESS_KEY\u00b6\nDefault: None\nThe AWS secret key used by code that requires access to Amazon Web services,\nsuch as the S3 feed storage backend.\n\n\nBOT_NAME\u00b6\nDefault: 'scrapybot'\nThe name of the bot implemented by this Scrapy project (also known as the\nproject name). This will be used to construct the User-Agent by default, and\nalso for logging.\nIt\u2019s automatically populated with your project name when you create your\nproject with the startproject command.\n\n\nCONCURRENT_ITEMS\u00b6\nDefault: 100\nMaximum number of concurrent items (per response) to process in parallel in the\nItem Processor (also known as the Item Pipeline).\n\n\nCONCURRENT_REQUESTS\u00b6\nDefault: 16\nThe maximum number of concurrent (ie. simultaneous) requests that will be\nperformed by the Scrapy downloader.\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN\u00b6\nDefault: 8\nThe maximum number of concurrent (ie. simultaneous) requests that will be\nperformed to any single domain.\n\n\nCONCURRENT_REQUESTS_PER_IP\u00b6\nDefault: 0\nThe maximum number of concurrent (ie. simultaneous) requests that will be\nperformed to any single IP. If non-zero, the\nCONCURRENT_REQUESTS_PER_DOMAIN setting is ignored, and this one is\nused instead. In other words, concurrency limits will be applied per IP, not\nper domain.\n\n\nDEFAULT_ITEM_CLASS\u00b6\nDefault: 'scrapy.item.Item'\nThe default class that will be used for instantiating items in the the\nScrapy shell.\n\n\nDEFAULT_REQUEST_HEADERS\u00b6\nDefault:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nThe default headers used for Scrapy HTTP Requests. They\u2019re populated in the\nDefaultHeadersMiddleware.\n\n\nDEPTH_LIMIT\u00b6\nDefault: 0\nThe maximum depth that will be allowed to crawl for any site. If zero, no limit\nwill be imposed.\n\n\nDEPTH_PRIORITY\u00b6\nDefault: 0\nAn integer that is used to adjust the request priority based on its depth.\nIf zero, no priority adjustment is made from depth.\n\n\nDEPTH_STATS\u00b6\nDefault: True\nWhether to collect maximum depth stats.\n\n\nDEPTH_STATS_VERBOSE\u00b6\nDefault: False\nWhether to collect verbose depth stats. If this is enabled, the number of\nrequests for each depth is collected in the stats.\n\n\nDNSCACHE_ENABLED\u00b6\nDefault: True\nWhether to enable DNS in-memory cache.\n\n\nDOWNLOADER_DEBUG\u00b6\nDefault: False\nWhether to enable the Downloader debugging mode.\n\n\nDOWNLOADER_MIDDLEWARES\u00b6\nDefault:: {}\nA dict containing the downloader middlewares enabled in your project, and their\norders. For more info see Activating a downloader middleware.\n\n\nDOWNLOADER_MIDDLEWARES_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\nA dict containing the downloader middlewares enabled by default in Scrapy. You\nshould never modify this setting in your project, modify\nDOWNLOADER_MIDDLEWARES instead.  For more info see\nActivating a downloader middleware.\n\n\nDOWNLOADER_STATS\u00b6\nDefault: True\nWhether to enable downloader stats collection.\n\n\nDOWNLOAD_DELAY\u00b6\nDefault: 0\nThe amount of time (in secs) that the downloader should wait before downloading\nconsecutive pages from the same spider. This can be used to throttle the\ncrawling speed to avoid hitting servers too hard. Decimal numbers are\nsupported.  Example:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\nThis setting is also affected by the RANDOMIZE_DOWNLOAD_DELAY\nsetting (which is enabled by default). By default, Scrapy doesn\u2019t wait a fixed\namount of time between requests, but uses a random interval between 0.5 and 1.5\n* DOWNLOAD_DELAY.\nYou can also change this setting per spider.\n\n\nDOWNLOAD_HANDLERS\u00b6\nDefault: {}\nA dict containing the request downloader handlers enabled in your project.\nSee DOWNLOAD_HANDLERS_BASE for example format.\n\n\nDOWNLOAD_HANDLERS_BASE\u00b6\nDefault:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\nA dict containing the request download handlers enabled by default in Scrapy.\nYou should never modify this setting in your project, modify\nDOWNLOAD_HANDLERS instead.\n\n\nDOWNLOAD_TIMEOUT\u00b6\nDefault: 180\nThe amount of time (in secs) that the downloader will wait before timing out.\n\n\nDUPEFILTER_CLASS\u00b6\nDefault: 'scrapy.dupefilter.RFPDupeFilter'\nThe class used to detect and filter duplicate requests.\nThe default (RFPDupeFilter) filters based on request fingerprint using\nthe scrapy.utils.request.request_fingerprint function.\n\n\nEDITOR\u00b6\nDefault: depends on the environment\nThe editor to use for editing spiders with the edit command. It\ndefaults to the EDITOR environment variable, if set. Otherwise, it defaults\nto vi (on Unix systems) or the IDLE editor (on Windows).\n\n\nEXTENSIONS\u00b6\nDefault:: {}\nA dict containing the extensions enabled in your project, and their orders.\n\n\nEXTENSIONS_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.webservice.WebService': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\nThe list of available extensions. Keep in mind that some of them need to\nbe enabled through a setting. By default, this setting contains all stable\nbuilt-in extensions.\nFor more information See the extensions user guide\nand the list of available extensions.\n\n\nITEM_PIPELINES\u00b6\nDefault: []\nThe item pipelines to use (a list of classes).\nExample:\nITEM_PIPELINES = [\n    'mybot.pipeline.validate.ValidateMyItem',\n    'mybot.pipeline.validate.StoreMyItem'\n]\n\n\n\n\nLOG_ENABLED\u00b6\nDefault: True\nWhether to enable logging.\n\n\nLOG_ENCODING\u00b6\nDefault: 'utf-8'\nThe encoding to use for logging.\n\n\nLOG_FILE\u00b6\nDefault: None\nFile name to use for logging output. If None, standard error will be used.\n\n\nLOG_LEVEL\u00b6\nDefault: 'DEBUG'\nMinimum level to log. Available levels are: CRITICAL, ERROR, WARNING,\nINFO, DEBUG. For more info see Logging.\n\n\nLOG_STDOUT\u00b6\nDefault: False\nIf True, all standard output (and error) of your process will be redirected\nto the log. For example if you print 'hello' it will appear in the Scrapy\nlog.\n\n\nMEMDEBUG_ENABLED\u00b6\nDefault: False\nWhether to enable memory debugging.\n\n\nMEMDEBUG_NOTIFY\u00b6\nDefault: []\nWhen memory debugging is enabled a memory report will be sent to the specified\naddresses if this setting is not empty, otherwise the report will be written to\nthe log.\nExample:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED\u00b6\nDefault: False\nScope: scrapy.contrib.memusage\nWhether to enable the memory usage extension that will shutdown the Scrapy\nprocess when it exceeds a memory limit, and also notify by email when that\nhappened.\nSee Memory usage extension.\n\n\nMEMUSAGE_LIMIT_MB\u00b6\nDefault: 0\nScope: scrapy.contrib.memusage\nThe maximum amount of memory to allow (in megabytes) before shutting down\nScrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.\nSee Memory usage extension.\n\n\nMEMUSAGE_NOTIFY_MAIL\u00b6\nDefault: False\nScope: scrapy.contrib.memusage\nA list of emails to notify if the memory limit has been reached.\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee Memory usage extension.\n\n\nMEMUSAGE_REPORT\u00b6\nDefault: False\nScope: scrapy.contrib.memusage\nWhether to send a memory usage report after each spider has been closed.\nSee Memory usage extension.\n\n\nMEMUSAGE_WARNING_MB\u00b6\nDefault: 0\nScope: scrapy.contrib.memusage\nThe maximum amount of memory to allow (in megabytes) before sending a warning\nemail notifying about it. If zero, no warning will be produced.\n\n\nNEWSPIDER_MODULE\u00b6\nDefault: ''\nModule where to create new spiders using the genspider command.\nExample:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY\u00b6\nDefault: True\nIf enabled, Scrapy will wait a random amount of time (between 0.5 and 1.5\n* DOWNLOAD_DELAY) while fetching requests from the same\nspider.\nThis randomization decreases the chance of the crawler being detected (and\nsubsequently blocked) by sites which analyze requests looking for statistically\nsignificant similarities in the time between their requests.\nThe randomization policy is the same used by wget --random-wait option.\nIf DOWNLOAD_DELAY is zero (default) this option has no effect.\n\n\nREDIRECT_MAX_TIMES\u00b6\nDefault: 20\nDefines the maximum times a request can be redirected. After this maximum the\nrequest\u2019s response is returned as is. We used Firefox default value for the\nsame task.\n\n\nREDIRECT_MAX_METAREFRESH_DELAY\u00b6\nDefault: 100\nSome sites use meta-refresh for redirecting to a session expired page, so we\nrestrict automatic redirection to a maximum delay (in seconds)\n\n\nREDIRECT_PRIORITY_ADJUST\u00b6\nDefault: +2\nAdjust redirect request priority relative to original request.\nA negative priority adjust means more priority.\n\n\nROBOTSTXT_OBEY\u00b6\nDefault: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\nIf enabled, Scrapy will respect robots.txt policies. For more information see\nRobotsTxtMiddleware\n\n\nSCHEDULER\u00b6\nDefault: 'scrapy.core.scheduler.Scheduler'\nThe scheduler to use for crawling.\n\n\nSPIDER_CONTRACTS\u00b6\nDefault:: {}\nA dict containing the scrapy contracts enabled in your project, used for\ntesting spiders. For more info see Spiders Contracts.\n\n\nSPIDER_CONTRACTS_BASE\u00b6\nDefault:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\nA dict containing the scrapy contracts enabled by default in Scrapy. You should\nnever modify this setting in your project, modify SPIDER_CONTRACTS\ninstead. For more info see Spiders Contracts.\n\n\nSPIDER_MIDDLEWARES\u00b6\nDefault:: {}\nA dict containing the spider middlewares enabled in your project, and their\norders. For more info see Activating a spider middleware.\n\n\nSPIDER_MIDDLEWARES_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\nA dict containing the spider middlewares enabled by default in Scrapy. You\nshould never modify this setting in your project, modify\nSPIDER_MIDDLEWARES instead. For more info see\nActivating a spider middleware.\n\n\nSPIDER_MODULES\u00b6\nDefault: []\nA list of modules where Scrapy will look for spiders.\nExample:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS\u00b6\nDefault: 'scrapy.statscol.MemoryStatsCollector'\nThe class to use for collecting stats, who must implement the\nStats Collector API.\n\n\nSTATS_DUMP\u00b6\nDefault: True\nDump the Scrapy stats (to the Scrapy log) once the spider\nfinishes.\nFor more info see: Stats Collection.\n\n\nSTATSMAILER_RCPTS\u00b6\nDefault: [] (empty list)\nSend Scrapy stats after spiders finish scraping. See\nStatsMailer for more info.\n\n\nTELNETCONSOLE_ENABLED\u00b6\nDefault: True\nA boolean which specifies if the telnet console\nwill be enabled (provided its extension is also enabled).\n\n\nTELNETCONSOLE_PORT\u00b6\nDefault: [6023, 6073]\nThe port range to use for the telnet console. If set to None or 0, a\ndynamically assigned port is used. For more info see\nTelnet Console.\n\n\nTEMPLATES_DIR\u00b6\nDefault: templates dir inside scrapy module\nThe directory where to look for templates when creating new projects with\nstartproject command.\n\n\nURLLENGTH_LIMIT\u00b6\nDefault: 2083\nScope: contrib.spidermiddleware.urllength\nThe maximum URL length to allow for crawled URLs. For more information about\nthe default value for this setting see: http://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT\u00b6\nDefault: \"Scrapy/VERSION (+http://scrapy.org)\"\nThe default User-Agent to use when crawling, unless overridden.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/settings.html", "title": ["Settings \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSignals\u00b6\nScrapy uses signals extensively to notify when certain events occur. You can\ncatch some of those signals in your Scrapy project (using an extension, for example) to perform additional tasks or extend Scrapy\nto add functionality not provided out of the box.\nEven though signals provide several arguments, the handlers that catch them\ndon\u2019t need to accept all of them - the signal dispatching mechanism will only\ndeliver the arguments that the handler receives.\nYou can connect to signals (or send your own) through the\nSignals API.\n\nDeferred signal handlers\u00b6\nSome signals support returning Twisted deferreds from their handlers, see\nthe Built-in signals reference below to know which ones.\n\n\nBuilt-in signals reference\u00b6\nHere\u2019s the list of Scrapy built-in signals and their meaning.\n\nengine_started\u00b6\n\nscrapy.signals.engine_started()\u00b6\nSent when the Scrapy engine has started crawling.\nThis signal supports returning deferreds from their handlers.\n\nNote\nThis signal may be fired after the spider_opened signal,\ndepending on how the spider was started. So don\u2019t rely on this signal\ngetting fired before spider_opened.\n\n\n\nengine_stopped\u00b6\n\nscrapy.signals.engine_stopped()\u00b6\nSent when the Scrapy engine is stopped (for example, when a crawling\nprocess has finished).\nThis signal supports returning deferreds from their handlers.\n\n\nitem_scraped\u00b6\n\nscrapy.signals.item_scraped(item, response, spider)\u00b6\nSent when an item has been scraped, after it has passed all the\nItem Pipeline stages (without being dropped).\nThis signal supports returning deferreds from their handlers.\nParameters:item (Item object) \u2013 the item scraped\nresponse (Response object) \u2013 the response from where the item was scraped\nspider (BaseSpider object) \u2013 the spider which scraped the item\n\n\n\nitem_dropped\u00b6\n\nscrapy.signals.item_dropped(item, spider, exception)\u00b6\nSent after an item has been dropped from the Item Pipeline\nwhen some stage raised a DropItem exception.\nThis signal supports returning deferreds from their handlers.\nParameters:item (Item object) \u2013 the item dropped from the Item Pipeline\nspider (BaseSpider object) \u2013 the spider which scraped the item\nexception (DropItem exception) \u2013 the exception (which must be a\nDropItem subclass) which caused the item\nto be dropped\n\n\n\nspider_closed\u00b6\n\nscrapy.signals.spider_closed(spider, reason)\u00b6\nSent after a spider has been closed. This can be used to release per-spider\nresources reserved on spider_opened.\nThis signal supports returning deferreds from their handlers.\nParameters:spider (BaseSpider object) \u2013 the spider which has been closed\nreason (str) \u2013 a string which describes the reason why the spider was closed. If\nit was closed because the spider has completed scraping, the reason\nis 'finished'. Otherwise, if the spider was manually closed by\ncalling the close_spider engine method, then the reason is the one\npassed in the reason argument of that method (which defaults to\n'cancelled'). If the engine was shutdown (for example, by hitting\nCtrl-C to stop it) the reason will be 'shutdown'.\n\n\n\nspider_opened\u00b6\n\nscrapy.signals.spider_opened(spider)\u00b6\nSent after a spider has been opened for crawling. This is typically used to\nreserve per-spider resources, but can be used for any task that needs to be\nperformed when a spider is opened.\nThis signal supports returning deferreds from their handlers.\nParameters:spider (BaseSpider object) \u2013 the spider which has been opened\n\n\nspider_idle\u00b6\n\nscrapy.signals.spider_idle(spider)\u00b6\nSent when a spider has gone idle, which means the spider has no further:\n\nrequests waiting to be downloaded\nrequests scheduled\nitems being processed in the item pipeline\n\nIf the idle state persists after all handlers of this signal have finished,\nthe engine starts closing the spider. After the spider has finished\nclosing, the spider_closed signal is sent.\nYou can, for example, schedule some requests in your spider_idle\nhandler to prevent the spider from being closed.\nThis signal does not support returning deferreds from their handlers.\nParameters:spider (BaseSpider object) \u2013 the spider which has gone idle\n\n\nspider_error\u00b6\n\nscrapy.signals.spider_error(failure, response, spider)\u00b6\nSent when a spider callback generates an error (ie. raises an exception).\nParameters:failure (Failure object) \u2013 the exception raised as a Twisted Failure object\nresponse (Response object) \u2013 the response being processed when the exception was raised\nspider (BaseSpider object) \u2013 the spider which raised the exception\n\n\n\nrequest_received\u00b6\n\nscrapy.signals.request_received(request, spider)\u00b6\nSent when the engine receives a Request from a spider.\nThis signal does not support returning deferreds from their handlers.\nParameters:request (Request object) \u2013 the request received\nspider (BaseSpider object) \u2013 the spider which generated the request\n\n\n\nresponse_received\u00b6\n\nscrapy.signals.response_received(response, request, spider)\u00b6\nSent when the engine receives a new Response from the\ndownloader.\nThis signal does not support returning deferreds from their handlers.\nParameters:response (Response object) \u2013 the response received\nrequest (Request object) \u2013 the request that generated the response\nspider (BaseSpider object) \u2013 the spider for which the response is intended\n\n\n\nresponse_downloaded\u00b6\n\nscrapy.signals.response_downloaded(response, request, spider)\u00b6\nSent by the downloader right after a HTTPResponse is downloaded.\nThis signal does not support returning deferreds from their handlers.\nParameters:response (Response object) \u2013 the response downloaded\nrequest (Request object) \u2013 the request that generated the response\nspider (BaseSpider object) \u2013 the spider for which the response is intended\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/signals.html", "title": ["Signals \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExceptions\u00b6\n\nBuilt-in Exceptions reference\u00b6\nHere\u2019s a list of all exceptions included in Scrapy and their usage.\n\nDropItem\u00b6\n\nexception scrapy.exceptions.DropItem\u00b6\nThe exception that must be raised by item pipeline stages to stop processing an\nItem. For more information see Item Pipeline.\n\n\nCloseSpider\u00b6\n\nexception scrapy.exceptions.CloseSpider(reason='cancelled')\u00b6\nThis exception can be raised from a spider callback to request the spider to be\nclosed/stopped. Supported arguments:\nParameters:reason (str) \u2013 the reason for closing\nFor example:\ndef parse_page(self, response):\n    if 'Bandwidth exceeded' in response.body:\n        raise CloseSpider('bandwidth_exceeded')\n\n\n\n\nIgnoreRequest\u00b6\n\nexception scrapy.exceptions.IgnoreRequest\u00b6\nThis exception can be raised by the Scheduler or any downloader middleware to\nindicate that the request should be ignored.\n\n\nNotConfigured\u00b6\n\nexception scrapy.exceptions.NotConfigured\u00b6\nThis exception can be raised by some components to indicate that they will\nremain disabled. Those components include:\n\nExtensions\nItem pipelines\nDownloader middlwares\nSpider middlewares\n\nThe exception must be raised in the component constructor.\n\n\nNotSupported\u00b6\n\nexception scrapy.exceptions.NotSupported\u00b6\nThis exception is raised to indicate an unsupported feature.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/exceptions.html", "title": ["Exceptions \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nItem Exporters\u00b6\nOnce you have scraped your Items, you often want to persist or export those\nitems, to use the data in some other application. That is, after all, the whole\npurpose of the scraping process.\nFor this purpose Scrapy provides a collection of Item Exporters for different\noutput formats, such as XML, CSV or JSON.\n\nUsing Item Exporters\u00b6\nIf you are in a hurry, and just want to use an Item Exporter to output scraped\ndata see the Feed exports. Otherwise, if you want to know how\nItem Exporters work or need more custom functionality (not covered by the\ndefault exports), continue reading below.\nIn order to use an Item Exporter, you  must instantiate it with its required\nargs. Each Item Exporter requires different arguments, so check each exporter\ndocumentation to be sure, in Built-in Item Exporters reference. After you have\ninstantiated you exporter, you have to:\n1. call the method start_exporting() in order to\nsignal the beginning of the exporting process\n2. call the export_item() method for each item you want\nto export\n3. and finally call the finish_exporting() to signal\nthe end of the exporting process\nHere you can see an Item Pipeline which uses an Item\nExporter to export scraped items to different files, one per spider:\nfrom scrapy import signals\nfrom scrapy.contrib.exporter import XmlItemExporter\n\nclass XmlExportPipeline(object):\n\n    def __init__(self):\n        self.files = {}\n\n     @classmethod\n     def from_crawler(cls, crawler):\n         pipeline = cls()\n         crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)\n         crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)\n         return pipeline\n\n    def spider_opened(self, spider):\n        file = open('%s_products.xml' % spider.name, 'w+b')\n        self.files[spider] = file\n        self.exporter = XmlItemExporter(file)\n        self.exporter.start_exporting()\n\n    def spider_closed(self, spider):\n        self.exporter.finish_exporting()\n        file = self.files.pop(spider)\n        file.close()\n\n    def process_item(self, item, spider):\n        self.exporter.export_item(item)\n        return item\n\n\n\nSerialization of item fields\u00b6\nBy default, the field values are passed unmodified to the underlying\nserialization library, and the decision of how to serialize them is delegated\nto each particular serialization library.\nHowever, you can customize how each field value is serialized before it is\npassed to the serialization library.\nThere are two ways to customize how a field will be serialized, which are\ndescribed next.\n\n1. Declaring a serializer in the field\u00b6\nYou can declare a serializer in the field metadata. The serializer must be a callable which receives a\nvalue and returns its serialized form.\nExample:\nfrom scrapy.item import Item, Field\n\ndef serialize_price(value):\n   return '$ %s' % str(value)\n\nclass Product(Item):\n    name = Field()\n    price = Field(serializer=serialize_price)\n\n\n\n\n2. Overriding the serialize_field() method\u00b6\nYou can also override the serialize() method to\ncustomize how your field value will be exported.\nMake sure you call the base class serialize() method\nafter your custom code.\nExample:\nfrom scrapy.contrib.exporter import XmlItemExporter\n\nclass ProductXmlExporter(XmlItemExporter):\n\n    def serialize_field(self, field, name, value):\n        if field == 'price':\n            return '$ %s' % str(value)\n        return super(Product, self).serialize_field(field, name, value)\n\n\n\n\n\nBuilt-in Item Exporters reference\u00b6\nHere is a list of the Item Exporters bundled with Scrapy. Some of them contain\noutput examples, which assume you\u2019re exporting these two items:\nItem(name='Color TV', price='1200')\nItem(name='DVD player', price='200')\n\n\n\nBaseItemExporter\u00b6\n\nclass scrapy.contrib.exporter.BaseItemExporter(fields_to_export=None, export_empty_fields=False, encoding='utf-8')\u00b6\nThis is the (abstract) base class for all Item Exporters. It provides\nsupport for common features used by all (concrete) Item Exporters, such as\ndefining what fields to export, whether to export empty fields, or which\nencoding to use.\nThese features can be configured through the constructor arguments which\npopulate their respective instance attributes: fields_to_export,\nexport_empty_fields, encoding.\n\nexport_item(item)\u00b6\nExports the given item. This method must be implemented in subclasses.\n\nserialize_field(field, name, value)\u00b6\nReturn the serialized value for the given field. You can override this\nmethod (in your custom Item Exporters) if you want to control how a\nparticular field or value will be serialized/exported.\nBy default, this method looks for a serializer declared in the item\nfield and returns the result of applying\nthat serializer to the value. If no serializer is found, it returns the\nvalue unchanged except for unicode values which are encoded to\nstr using the encoding declared in the encoding attribute.\nParameters:field (Field object) \u2013 the field being serialized\nname (str) \u2013 the name of the field being serialized\nvalue \u2013 the value being serialized\n\n\nstart_exporting()\u00b6\nSignal the beginning of the exporting process. Some exporters may use\nthis to generate some required header (for example, the\nXmlItemExporter). You must call this method before exporting any\nitems.\n\nfinish_exporting()\u00b6\nSignal the end of the exporting process. Some exporters may use this to\ngenerate some required footer (for example, the\nXmlItemExporter). You must always call this method after you\nhave no more items to export.\n\nfields_to_export\u00b6\nA list with the name of the fields that will be exported, or None if you\nwant to export all fields. Defaults to None.\nSome exporters (like CsvItemExporter) respect the order of the\nfields defined in this attribute.\n\nexport_empty_fields\u00b6\nWhether to include empty/unpopulated item fields in the exported data.\nDefaults to False. Some exporters (like CsvItemExporter)\nignore this attribute and always export all empty fields.\n\nencoding\u00b6\nThe encoding that will be used to encode unicode values. This only\naffects unicode values (which are always serialized to str using this\nencoding). Other value types are passed unchanged to the specific\nserialization library.\n\n\nXmlItemExporter\u00b6\n\nclass scrapy.contrib.exporter.XmlItemExporter(file, item_element='item', root_element='items', **kwargs)\u00b6\nExports Items in XML format to the specified file object.\nParameters:file \u2013 the file-like object to use for exporting the data.\nroot_element (str) \u2013 The name of root element in the exported XML.\nitem_element (str) \u2013 The name of each item element in the exported XML.\n\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor.\nA typical output of this exporter would be:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>Color TV</name>\n    <price>1200</price>\n </item>\n  <item>\n    <name>DVD player</name>\n    <price>200</price>\n </item>\n</items>\n\n\nUnless overridden in the serialize_field() method, multi-valued fields are\nexported by serializing each value inside a <value> element. This is for\nconvenience, as multi-valued fields are very common.\nFor example, the item:\nItem(name=['John', 'Doe'], age='23')\n\n\nWould be serialized as:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>\n      <value>John</value>\n      <value>Doe</value>\n    </name>\n    <age>23</age>\n  </item>\n</items>\n\n\n\n\nCsvItemExporter\u00b6\n\nclass scrapy.contrib.exporter.CsvItemExporter(file, include_headers_line=True, join_multivalued=', ', **kwargs)\u00b6\nExports Items in CSV format to the given file-like object. If the\nfields_to_export attribute is set, it will be used to define the\nCSV columns and their order. The export_empty_fields attribute has\nno effect on this exporter.\nParameters:file \u2013 the file-like object to use for exporting the data.\ninclude_headers_line (str) \u2013 If enabled, makes the exporter output a header\nline with the field names taken from\nBaseItemExporter.fields_to_export or the first exported item fields.\njoin_multivalued \u2013 The char (or chars) that will be used for joining\nmulti-valued fields, if found.\n\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor, and the leftover arguments to the\ncsv.writer constructor, so you can use any csv.writer constructor\nargument to customize this exporter.\nA typical output of this exporter would be:\nproduct,price\nColor TV,1200\nDVD player,200\n\n\n\n\nPickleItemExporter\u00b6\n\nclass scrapy.contrib.exporter.PickleItemExporter(file, protocol=0, **kwargs)\u00b6\nExports Items in pickle format to the given file-like object.\nParameters:file \u2013 the file-like object to use for exporting the data.\nprotocol (int) \u2013 The pickle protocol to use.\n\nFor more information, refer to the pickle module documentation.\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor.\nPickle isn\u2019t a human readable format, so no output examples are provided.\n\n\nPprintItemExporter\u00b6\n\nclass scrapy.contrib.exporter.PprintItemExporter(file, **kwargs)\u00b6\nExports Items in pretty print format to the specified file object.\nParameters:file \u2013 the file-like object to use for exporting the data.\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor.\nA typical output of this exporter would be:\n{'name': 'Color TV', 'price': '1200'}\n{'name': 'DVD player', 'price': '200'}\n\n\nLonger lines (when present) are pretty-formatted.\n\n\nJsonItemExporter\u00b6\n\nclass scrapy.contrib.exporter.JsonItemExporter(file, **kwargs)\u00b6\nExports Items in JSON format to the specified file-like object, writing all\nobjects as a list of objects. The additional constructor arguments are\npassed to the BaseItemExporter constructor, and the leftover\narguments to the JSONEncoder constructor, so you can use any\nJSONEncoder constructor argument to customize this exporter.\nParameters:file \u2013 the file-like object to use for exporting the data.\nA typical output of this exporter would be:\n[{\"name\": \"Color TV\", \"price\": \"1200\"},\n{\"name\": \"DVD player\", \"price\": \"200\"}]\n\n\n\nWarning\nJSON is very simple and flexible serialization format, but it\ndoesn\u2019t scale well for large amounts of data since incremental (aka.\nstream-mode) parsing is not well supported (if at all) among JSON parsers\n(on any language), and most of them just parse the entire object in\nmemory. If you want the power and simplicity of JSON with a more\nstream-friendly format, consider using JsonLinesItemExporter\ninstead, or splitting the output in multiple chunks.\n\n\n\nJsonLinesItemExporter\u00b6\n\nclass scrapy.contrib.exporter.JsonLinesItemExporter(file, **kwargs)\u00b6\nExports Items in JSON format to the specified file-like object, writing one\nJSON-encoded item per line. The additional constructor arguments are passed\nto the BaseItemExporter constructor, and the leftover arguments to\nthe JSONEncoder constructor, so you can use any JSONEncoder\nconstructor argument to customize this exporter.\nParameters:file \u2013 the file-like object to use for exporting the data.\nA typical output of this exporter would be:\n{\"name\": \"Color TV\", \"price\": \"1200\"}\n{\"name\": \"DVD player\", \"price\": \"200\"}\n\n\nUnlike the one produced by JsonItemExporter, the format produced by\nthis exporter is well suited for serializing large amounts of data.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/exporters.html", "title": ["Item Exporters \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nRelease notes\u00b6\n\n0.16.4 (released 2013-01-23)\u00b6\nfixes spelling errors in documentation (commit 6d2b3aa)\nadd doc about disabling an extension. refs #132 (commit c90de33)\nFixed error message formatting. log.err() doesn\u2019t support cool formatting and when error occured, the message was:    \u201cERROR: Error processing %(item)s\u201d (commit c16150c)\nlint and improve images pipeline error logging (commit 56b45fc)\nfixed doc typos (commit 243be84)\nadd documentation topics: Broad Crawls & Common Practies (commit 1fbb715)\nfix bug in scrapy parse command when spider is not specified explicitly. closes #209 (commit c72e682)\nUpdate docs/topics/commands.rst (commit 28eac7a)\n\n\n0.16.3 (released 2012-12-07)\u00b6\nRemove concurrency limitation when using download delays and still ensure inter-request delays are enforced (commit 487b9b5)\nadd error details when image pipeline fails (commit 8232569)\nimprove mac os compatibility (commit 8dcf8aa)\nsetup.py: use README.rst to populate long_description (commit 7b5310d)\ndoc: removed obsolete references to ClientForm (commit 80f9bb6)\ncorrect docs for default storage backend (commit 2aa491b)\ndoc: removed broken proxyhub link from FAQ (commit bdf61c4)\nMerge branch \u20180.16\u2019 of github.com:scrapy/scrapy into 0.16 (commit d5087b0)\nFixed docs typo in SpiderOpenCloseLogging example (commit 7184094)\n\n\n0.16.2 (released 2012-11-09)\u00b6\nscrapy contracts: python2.6 compat (commit a4a9199)\nscrapy contracts verbose option (commit ec41673)\nproper unittest-like output for scrapy contracts (commit 86635e4)\nadded open_in_browser to debugging doc (commit c9b690d)\nremoved reference to global scrapy stats from settings doc (commit dd55067)\nFix SpiderState bug in Windows platforms (commit 58998f4)\n\n\n0.16.1 (released 2012-10-26)\u00b6\nfixed LogStats extension, which got broken after a wrong merge before the 0.16 release (commit 8c780fd)\nbetter backwards compatibility for scrapy.conf.settings (commit 3403089)\nextended documentation on how to access crawler stats from extensions (commit c4da0b5)\nremoved .hgtags (no longer needed now that scrapy uses git) (commit d52c188)\nfix dashes under rst headers (commit fa4f7f9)\nset release date for 0.16.0 in news (commit e292246)\n\n\n0.16.0 (released 2012-10-18)\u00b6\nScrapy changes:\nadded Spiders Contracts, a mechanism for testing spiders in a formal/reproducible way\nadded options -o and -t to the runspider command\ndocumented AutoThrottle extension and added to extensions installed by default. You still need to enable it with AUTOTHROTTLE_ENABLED\nmajor Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (stats_spider_opened, etc). Stats are much simpler now, backwards compatibility is kept on the Stats Collector API and signals.\nadded process_start_requests() method to spider middlewares\ndropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.\ndropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.\ndropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.\ndocumented Core API\nlxml is now the default selectors backend instead of libxml2\nported FormRequest.from_response() to use lxml instead of ClientForm\nremoved modules: scrapy.xlib.BeautifulSoup and scrapy.xlib.ClientForm\nSitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (commit 10ed28b)\nStackTraceDump extension: also dump trackref live references (commit fe2ce93)\nnested items now fully supported in JSON and JSONLines exporters\nadded cookiejar Request meta key to support multiple cookie sessions per spider\ndecoupled encoding detection code to w3lib.encoding, and ported Scrapy code to use that mdule\ndropped support for Python 2.5. See http://blog.scrapy.org/scrapy-dropping-support-for-python-25\ndropped support for Twisted 2.5\nadded REFERER_ENABLED setting, to control referer middleware\nchanged default user agent to: Scrapy/VERSION (+http://scrapy.org)\nremoved (undocumented) HTMLImageLinkExtractor class from scrapy.contrib.linkextractors.image\nremoved per-spider settings (to be replaced by instantiating multiple crawler objects)\nUSER_AGENT spider attribute will no longer work, use user_agent attribute instead\nDOWNLOAD_TIMEOUT spider attribute will no longer work, use download_timeout attribute instead\nremoved ENCODING_ALIASES setting, as encoding auto-detection has been moved to the w3lib library\npromoted DjangoItem to main contrib\nLogFormatter method now return dicts(instead of strings) to support lazy formatting (issue 164, commit dcef7b0)\ndownloader handlers (DOWNLOAD_HANDLERS setting) now receive settings as the first argument of the constructor\nreplaced memory usage acounting with (more portable) resource module, removed scrapy.utils.memory module\nremoved signal: scrapy.mail.mail_sent\nremoved TRACK_REFS setting, now trackrefs is always enabled\nDBM is now the default storage backend for HTTP cache middleware\nnumber of log messages (per level) are now tracked through Scrapy stats (stat name: log_count/LEVEL)\nnumber received responses are now tracked through Scrapy stats (stat name: response_received_count)\nremoved scrapy.log.started attribute\nScrapyd changes:\nNew Scrapyd API methods: listjobs.json and cancel.json\nNew Scrapyd settings: items_dir and jobs_to_keep\nItems are now stored on disk using feed exports, and accessible through the Scrapyd web interface\nSupport making Scrapyd listen into a specific IP address (see bind_address option)\n\n\n0.14.4\u00b6\nadded precise to supported ubuntu distros (commit b7e46df)\nfixed bug in json-rpc webservice reported in https://groups.google.com/d/topic/scrapy-users/qgVBmFybNAQ/discussion. also removed no longer supported \u2018run\u2019 command from extras/scrapy-ws.py (commit 340fbdb)\nmeta tag attributes for content-type http equiv can be in any order. #123 (commit 0cb68af)\nreplace \u201cimport Image\u201d by more standard \u201cfrom PIL import Image\u201d. closes #88 (commit 4d17048)\nreturn trial status as bin/runtests.sh exit value. #118 (commit b7b2e7f)\n\n\n0.14.3\u00b6\nforgot to include pydispatch license. #118 (commit fd85f9c)\ninclude egg files used by testsuite in source distribution. #118 (commit c897793)\nupdate docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (commit 2548dcc)\nadded note to docs/topics/firebug.rst about google directory being shut down (commit 668e352)\nMerge branch \u20180.14\u2019 of github.com:scrapy/scrapy into 0.14 (commit 835d082)\ndont discard slot when empty, just save in another dict in order to recycle if needed again. (commit 8e9f607)\ndo not fail handling unicode xpaths in libxml2 backed selectors (commit b830e95)\nfixed minor mistake in Request objects documentation (commit bf3c9ee)\nfixed minor defect in link extractors documentation (commit ba14f38)\nremoved some obsolete remaining code related to sqlite support in scrapy (commit 0665175)\n\n\n0.14.2\u00b6\nmove buffer pointing to start of file before computing checksum. refs #92 (commit 6a5bef2)\nCompute image checksum before persisting images. closes #92 (commit 9817df1)\nremove leaking references in cached failures (commit 673a120)\nfixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (commit 11133e9)\nMerge branch \u20180.14\u2019 of github.com:scrapy/scrapy into 0.14 (commit 1627320)\nfixed struct.error on http compression middleware. closes #87 (commit 1423140)\najax crawling wasn\u2019t expanding for unicode urls (commit 0de3fb4)\nCatch start_requests iterator errors. refs #83 (commit 454a21d)\nSpeed-up libxml2 XPathSelector (commit 2fbd662)\nupdated versioning doc according to recent changes (commit 0a070f5)\nscrapyd: fixed documentation link (commit 2b4e4c3)\nextras/makedeb.py: no longer obtaining version from git (commit caffe0e)\n\n\n0.14.1\u00b6\nextras/makedeb.py: no longer obtaining version from git (commit caffe0e)\nbumped version to 0.14.1 (commit 6cb9e1c)\nfixed reference to tutorial directory (commit 4b86bd6)\ndoc: removed duplicated callback argument from Request.replace() (commit 1aeccdd)\nfixed formatting of scrapyd doc (commit 8bf19e6)\nDump stacks for all running threads and fix engine status dumped by StackTraceDump extension (commit 14a8e6e)\nadded comment about why we disable ssl on boto images upload (commit 5223575)\nSSL handshaking hangs when doing too many parallel connections to S3 (commit 63d583d)\nchange tutorial to follow changes on dmoz site (commit bcb3198)\nAvoid _disconnectedDeferred AttributeError exception in Twisted>=11.1.0 (commit 98f3f87)\nallow spider to set autothrottle max concurrency (commit 175a4b5)\n\n\n0.14\u00b6\n\nNew features and settings\u00b6\nSupport for AJAX crawleable urls\n\nNew persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (r2737)\n\nadded -o option to scrapy crawl, a shortcut for dumping scraped items into a file (or standard output using -)\n\nAdded support for passing custom settings to Scrapyd schedule.json api (r2779, r2783)\n\nNew ChunkedTransferMiddleware (enabled by default) to support chunked transfer encoding (r2769)\n\nAdd boto 2.0 support for S3 downloader handler (r2763)\n\nAdded marshal to formats supported by feed exports (r2744)\n\nIn request errbacks, offending requests are now received in failure.request attribute (r2738)\n\nBig downloader refactoring to support per domain/ip concurrency limits (r2732)\nCONCURRENT_REQUESTS_PER_SPIDER setting has been deprecated and replaced by:\nCONCURRENT_REQUESTS, CONCURRENT_REQUESTS_PER_DOMAIN, CONCURRENT_REQUESTS_PER_IP\n\n\ncheck the documentation for more details\n\n\n\nAdded builtin caching DNS resolver (r2728)\n\nMoved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](https://github.com/scrapinghub/scaws) (r2706, r2714)\n\nMoved spider queues to scrapyd: scrapy.spiderqueue -> scrapyd.spiderqueue (r2708)\n\nMoved sqlite utils to scrapyd: scrapy.utils.sqlite -> scrapyd.sqlite (r2781)\n\nReal support for returning iterators on start_requests() method. The iterator is now consumed during the crawl when the spider is getting idle (r2704)\n\nAdded REDIRECT_ENABLED setting to quickly enable/disable the redirect middleware (r2697)\n\nAdded RETRY_ENABLED setting to quickly enable/disable the retry middleware (r2694)\n\nAdded CloseSpider exception to manually close spiders (r2691)\n\nImproved encoding detection by adding support for HTML5 meta charset declaration (r2690)\n\nRefactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (r2688)\n\nAdded SitemapSpider (see documentation in Spiders page) (r2658)\n\nAdded LogStats extension for periodically logging basic stats (like crawled pages and scraped items) (r2657)\n\nMake handling of gzipped responses more robust (#319, r2643). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an IOError.\n\nSimplified !MemoryDebugger extension to use stats for dumping memory debugging info (r2639)\n\nAdded new command to edit spiders: scrapy edit (r2636) and -e flag to genspider command that uses it (r2653)\n\nChanged default representation of items to pretty-printed dicts. (r2631). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.\n\nAdded spider_error signal (r2628)\n\nAdded COOKIES_ENABLED setting (r2625)\n\nStats are now dumped to Scrapy log (default value of STATS_DUMP setting has been changed to True). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.\n\nAdded support for dynamically adjusting download delay and maximum concurrent requests (r2599)\n\nAdded new DBM HTTP cache storage backend (r2576)\n\nAdded listjobs.json API to Scrapyd (r2571)\n\nCsvItemExporter: added join_multivalued parameter (r2578)\n\nAdded namespace support to xmliter_lxml (r2552)\n\nImproved cookies middleware by making COOKIES_DEBUG nicer and documenting it (r2579)\n\nSeveral improvements to Scrapyd and Link extractors\n\n\n\nCode rearranged and removed\u00b6\nMerged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (r2630)\noriginal item_scraped signal was removed\noriginal item_passed signal was renamed to item_scraped\nold log lines Scraped Item... were removed\nold log lines Passed Item... were renamed to Scraped Item... lines and downgraded to DEBUG level\n\n\nReduced Scrapy codebase by striping part of Scrapy code into two new libraries:\nw3lib (several functions from scrapy.utils.{http,markup,multipart,response,url}, done in r2584)\nscrapely (was scrapy.contrib.ibl, done in r2586)\n\n\nRemoved unused function: scrapy.utils.request.request_info() (r2577)\n\nRemoved googledir project from examples/googledir. There\u2019s now a new example project called dirbot available on github: https://github.com/scrapy/dirbot\n\nRemoved support for default field values in Scrapy items (r2616)\n\nRemoved experimental crawlspider v2 (r2632)\n\nRemoved scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (DUPEFILTER_CLASS setting) (r2640)\n\nRemoved support for passing urls to scrapy crawl command (use scrapy parse instead) (r2704)\n\nRemoved deprecated Execution Queue (r2704)\n\nRemoved (undocumented) spider context extension (from scrapy.contrib.spidercontext) (r2780)\n\nremoved CONCURRENT_SPIDERS setting (use scrapyd maxproc instead) (r2789)\n\nRenamed attributes of core components: downloader.sites -> downloader.slots, scraper.sites -> scraper.slots (r2717, r2718)\n\nRenamed setting CLOSESPIDER_ITEMPASSED to CLOSESPIDER_ITEMCOUNT (r2655). Backwards compatibility kept.\n\n\n\n\n0.12\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features and improvements\u00b6\nPassed item is now sent in the item argument of the item_passed (#273)\nAdded verbose option to scrapy version command, useful for bug reports (#298)\nHTTP cache now stored by default in the project data dir (#279)\nAdded project data storage directory (#276, #277)\nDocumented file structure of Scrapy projects (see command-line tool doc)\nNew lxml backend for XPath selectors (#147)\nPer-spider settings (#245)\nSupport exit codes to signal errors in Scrapy commands (#248)\nAdded -c argument to scrapy shell command\nMade libxml2 optional (#260)\nNew deploy command (#261)\nAdded CLOSESPIDER_PAGECOUNT setting (#253)\nAdded CLOSESPIDER_ERRORCOUNT setting (#254)\n\n\nScrapyd changes\u00b6\nScrapyd now uses one process per spider\nIt stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)\nA minimal web ui was added, available at http://localhost:6800 by default\nThere is now a scrapy server command to start a Scrapyd server of the current project\n\n\nChanges to settings\u00b6\nadded HTTPCACHE_ENABLED setting (False by default) to enable HTTP cache middleware\nchanged HTTPCACHE_EXPIRATION_SECS semantics: now zero means \u201cnever expire\u201d.\n\n\nDeprecated/obsoleted functionality\u00b6\nDeprecated runserver command in favor of server command which starts a Scrapyd server. See also: Scrapyd changes\nDeprecated queue command in favor of using Scrapyd schedule.json API. See also: Scrapyd changes\nRemoved the !LxmlItemLoader (experimental contrib which never graduated to main contrib)\n\n\n\n0.10\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features and improvements\u00b6\nNew Scrapy service called scrapyd for deploying Scrapy crawlers in production (#218) (documentation available)\nSimplified Images pipeline usage which doesn\u2019t require subclassing your own images pipeline now (#217)\nScrapy shell now shows the Scrapy log by default (#206)\nRefactored execution queue in a common base code and pluggable backends called \u201cspider queues\u201d (#220)\nNew persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.\nAdded documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)\nFeed exporters with pluggable backends (#197) (documentation available)\nDeferred signals (#193)\nAdded two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)\nSupport for overriding default request headers per spider (#181)\nReplaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)\nSplitted Debian package into two packages - the library and the service (#187)\nScrapy log refactoring (#188)\nNew extension for keeping persistent spider contexts among different runs (#203)\nAdded dont_redirect request.meta key for avoiding redirects (#233)\nAdded dont_retry request.meta key for avoiding retries (#234)\n\n\nCommand-line tool changes\u00b6\nNew scrapy command which replaces the old scrapy-ctl.py (#199)\n- there is only one global scrapy command now, instead of one scrapy-ctl.py per project\n- Added scrapy.bat script for running more conveniently from Windows\nAdded bash completion to command-line tool (#210)\nRenamed command start to runserver (#209)\n\n\nAPI changes\u00b6\nurl and body attributes of Request objects are now read-only (#230)\n\nRequest.copy() and Request.replace() now also copies their callback and errback attributes (#231)\n\nRemoved UrlFilterMiddleware from scrapy.contrib (already disabled by default)\n\nOffsite middelware doesn\u2019t filter out any request coming from a spider that doesn\u2019t have a allowed_domains attribute (#225)\n\nRemoved Spider Manager load() method. Now spiders are loaded in the constructor itself.\n\nChanges to Scrapy Manager (now called \u201cCrawler\u201d):\nscrapy.core.manager.ScrapyManager class renamed to scrapy.crawler.Crawler\nscrapy.core.manager.scrapymanager singleton moved to scrapy.project.crawler\n\n\nMoved module: scrapy.contrib.spidermanager to scrapy.spidermanager\n\nSpider Manager singleton moved from scrapy.spider.spiders to the spiders` attribute of ``scrapy.project.crawler singleton.\n\nmoved Stats Collector classes: (#204)\nscrapy.stats.collector.StatsCollector to scrapy.statscol.StatsCollector\nscrapy.stats.collector.SimpledbStatsCollector to scrapy.contrib.statscol.SimpledbStatsCollector\n\n\ndefault per-command settings are now specified in the default_settings attribute of command object class (#201)\n\nchanged arguments of Item pipeline process_item() method from (spider, item) to (item, spider)\nbackwards compatibility kept (with deprecation warning)\n\n\nmoved scrapy.core.signals module to scrapy.signals\nbackwards compatibility kept (with deprecation warning)\n\n\nmoved scrapy.core.exceptions module to scrapy.exceptions\nbackwards compatibility kept (with deprecation warning)\n\n\nadded handles_request() class method to BaseSpider\n\ndropped scrapy.log.exc() function (use scrapy.log.err() instead)\n\ndropped component argument of scrapy.log.msg() function\n\ndropped scrapy.log.log_level attribute\n\nAdded from_settings() class methods to Spider Manager, and Item Pipeline Manager\n\n\n\nChanges to settings\u00b6\nAdded HTTPCACHE_IGNORE_SCHEMES setting to ignore certain schemes on !HttpCacheMiddleware (#225)\nAdded SPIDER_QUEUE_CLASS setting which defines the spider queue to use (#220)\nAdded KEEP_ALIVE setting (#220)\nRemoved SERVICE_QUEUE setting (#220)\nRemoved COMMANDS_SETTINGS_MODULE setting (#201)\nRenamed REQUEST_HANDLERS to DOWNLOAD_HANDLERS and make download handlers classes (instead of functions)\n\n\n\n0.9\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features and improvements\u00b6\nAdded SMTP-AUTH support to scrapy.mail\nNew settings added: MAIL_USER, MAIL_PASS (r2065 | #149)\nAdded new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (r2039)\nAdded web service for controlling Scrapy process (this also deprecates the web console. (r2053 | #167)\nSupport for running Scrapy as a service, for production systems (r1988, r2054, r2055, r2056, r2057 | #168)\nAdded wrapper induction library (documentation only available in source code for now). (r2011)\nSimplified and improved response encoding support (r1961, r1969)\nAdded LOG_ENCODING setting (r1956, documentation available)\nAdded RANDOMIZE_DOWNLOAD_DELAY setting (enabled by default) (r1923, doc available)\nMailSender is no longer IO-blocking (r1955 | #146)\nLinkextractors and new Crawlspider now handle relative base tag urls (r1960 | #148)\nSeveral improvements to Item Loaders and processors (r2022, r2023, r2024, r2025, r2026, r2027, r2028, r2029, r2030)\nAdded support for adding variables to telnet console (r2047 | #165)\nSupport for requests without callbacks (r2050 | #166)\n\n\nAPI changes\u00b6\nChange Spider.domain_name to Spider.name (SEP-012, r1975)\nResponse.encoding is now the detected encoding (r1961)\nHttpErrorMiddleware now returns None or raises an exception (r2006 | #157)\nscrapy.command modules relocation (r2035, r2036, r2037)\nAdded ExecutionQueue for feeding spiders to scrape (r2034)\nRemoved ExecutionEngine singleton (r2039)\nPorted S3ImagesStore (images pipeline) to use boto and threads (r2033)\nMoved module: scrapy.management.telnet to scrapy.telnet (r2047)\n\n\nChanges to default settings\u00b6\nChanged default SCHEDULER_ORDER to DFO (r1939)\n\n\n\n0.8\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features\u00b6\nAdded DEFAULT_RESPONSE_ENCODING setting (r1809)\nAdded dont_click argument to FormRequest.from_response() method (r1813, r1816)\nAdded clickdata argument to FormRequest.from_response() method (r1802, r1803)\nAdded support for HTTP proxies (HttpProxyMiddleware) (r1781, r1785)\nOffiste spider middleware now logs messages when filtering out requests (r1841)\n\n\nBackwards-incompatible changes\u00b6\nChanged scrapy.utils.response.get_meta_refresh() signature (r1804)\n\nRemoved deprecated scrapy.item.ScrapedItem class - use scrapy.item.Item instead (r1838)\n\nRemoved deprecated scrapy.xpath module - use scrapy.selector instead. (r1836)\n\nRemoved deprecated core.signals.domain_open signal - use core.signals.domain_opened instead (r1822)\n\nlog.msg() now receives a spider argument (r1822)\nOld domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the spider argument and pass spider references. If you really want to pass a string, use the component argument instead.\n\n\nChanged core signals domain_opened, domain_closed, domain_idle\n\nChanged Item pipeline to use spiders instead of domains\nThe domain argument of  process_item() item pipeline method was changed to  spider, the new signature is: process_item(spider, item) (r1827 | #105)\nTo quickly port your code (to work with Scrapy 0.8) just use spider.domain_name where you previously used domain.\n\n\nChanged Stats API to use spiders instead of domains (r1849 | #113)\nStatsCollector was changed to receive spider references (instead of domains) in its methods (set_value, inc_value, etc).\nadded StatsCollector.iter_spider_stats() method\nremoved StatsCollector.list_domains() method\nAlso, Stats signals were renamed and now pass around spider references (instead of domains). Here\u2019s a summary of the changes:\nTo quickly port your code (to work with Scrapy 0.8) just use spider.domain_name where you previously used domain. spider_stats contains exactly the same data as domain_stats.\n\n\nCloseDomain extension moved to scrapy.contrib.closespider.CloseSpider (r1833)\nIts settings were also renamed:\nCLOSEDOMAIN_TIMEOUT to CLOSESPIDER_TIMEOUT\nCLOSEDOMAIN_ITEMCOUNT to CLOSESPIDER_ITEMCOUNT\n\n\n\n\nRemoved deprecated SCRAPYSETTINGS_MODULE environment variable - use SCRAPY_SETTINGS_MODULE instead (r1840)\n\nRenamed setting: REQUESTS_PER_DOMAIN to CONCURRENT_REQUESTS_PER_SPIDER (r1830, r1844)\n\nRenamed setting: CONCURRENT_DOMAINS to CONCURRENT_SPIDERS (r1830)\n\nRefactored HTTP Cache middleware\n\nHTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (r1843 )\n\nRenamed exception: DontCloseDomain to DontCloseSpider (r1859 | #120)\n\nRenamed extension: DelayedCloseDomain to SpiderCloseDelay (r1861 | #121)\n\nRemoved obsolete scrapy.utils.markup.remove_escape_chars function - use scrapy.utils.markup.replace_escape_chars instead (r1865)\n\n\n\n\n0.7\u00b6\nFirst release of Scrapy.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/news.html", "title": ["Release notes \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nContributing to Scrapy\u00b6\nThere are many ways to contribute to Scrapy. Here are some of them:\nBlog about Scrapy. Tell the world how you\u2019re using Scrapy. This will help\nnewcomers with more examples and the Scrapy project to increase its\nvisibility.\nReport bugs and request features in the issue tracker, trying to follow\nthe guidelines detailed in Reporting bugs below.\nSubmit patches for new functionality and/or bug fixes. Please read\nWriting patches and Submitting patches below for details on how to\nwrite and submit a patch.\nJoin the scrapy-developers mailing list and share your ideas on how to\nimprove Scrapy. We\u2019re always open to suggestions.\n\nReporting bugs\u00b6\nWell-written bug reports are very helpful, so keep in mind the following\nguidelines when reporting a new bug.\ncheck the FAQ first to see if your issue is addressed in a\nwell-known question\ncheck the open issues to see if it has already been reported. If it has,\ndon\u2019t dismiss the report but check the ticket history and comments, you may\nfind additional useful information to contribute.\nsearch the scrapy-users list to see if it has been discussed there, or\nif you\u2019re not sure if what you\u2019re seeing is a bug. You can also ask in the\n#scrapy IRC channel.\nwrite complete, reproducible, specific bug reports. The smaller the test\ncase, the better. Remember that other developers won\u2019t have your project to\nreproduce the bug, so please include all relevant files required to reproduce\nit.\ninclude the output of scrapy version -v so developers working on your bug\nknow exactly which version and platform it occurred on, which is often very\nhelpful for reproducing it, or knowing if it was already fixed.\n\n\nWriting patches\u00b6\nThe better written a patch is, the higher chance that it\u2019ll get accepted and\nthe sooner that will be merged.\nWell-written patches should:\ncontain the minimum amount of code required for the specific change. Small\npatches are easier to review and merge. So, if you\u2019re doing more than one\nchange (or bug fix), please consider submitting one patch per change. Do not\ncollapse multiple changes into a single patch. For big changes consider using\na patch queue.\npass all unit-tests. See Running tests below.\ninclude one (or more) test cases that check the bug fixed or the new\nfunctionality added. See Writing tests below.\nif you\u2019re adding or changing a public (documented) API, please include\nthe documentation changes in the same patch.  See Documentation policies\nbelow.\n\n\nSubmitting patches\u00b6\nThe best way to submit a patch is to issue a pull request on Github,\noptionally creating a new issue first.\nAlternatively, we also accept the patches in the traditional way of sending\nthem to the scrapy-developers list.\nRegardless of which mechanism you use, remember to explain what was fixed or\nthe new functionality (what it is, why it\u2019s needed, etc). The more info you\ninclude, the easier will be for core developers to understand and accept your\npatch.\nYou can also discuss the new functionality (or bug fix) in scrapy-developers\nfirst, before creating the patch, but it\u2019s always good to have a patch ready to\nillustrate your arguments and show that you have put some additional thought\ninto the subject.\n\n\nCoding style\u00b6\nPlease follow these coding conventions when writing code for inclusion in\nScrapy:\nUnless otherwise specified, follow PEP 8.\nIt\u2019s OK to use lines longer than 80 chars if it improves the code\nreadability.\nDon\u2019t put your name in the code you contribute. Our policy is to keep\nthe contributor\u2019s name in the AUTHORS file distributed with Scrapy.\n\n\nDocumentation policies\u00b6\nDon\u2019t use docstrings for documenting classes, or methods which are\nalready documented in the official (sphinx) documentation. For example, the\nItemLoader.add_value() method should be documented in the sphinx\ndocumentation, not its docstring.\nDo use docstrings for documenting functions not present in the official\n(sphinx) documentation, such as functions from scrapy.utils package and\nits sub-modules.\n\n\nTests\u00b6\nTests are implemented using the Twisted unit-testing framework called\ntrial.\n\nRunning tests\u00b6\nTo run all tests go to the root directory of Scrapy source code and run:\n\nbin/runtests.sh (on unix)\nbin\\runtests.bat (on windows)\n\nTo run a specific test (say scrapy.tests.test_contrib_loader) use:\n\nbin/runtests.sh scrapy.tests.test_contrib_loader (on unix)\nbin\\runtests.bat scrapy.tests.test_contrib_loader (on windows)\n\n\n\nWriting tests\u00b6\nAll functionality (including new features and bug fixes) must include a test\ncase to check that it works as expected, so please include tests for your\npatches if you want them to get accepted sooner.\nScrapy uses unit-tests, which are located in the scrapy.tests package\n(scrapy/tests directory). Their module name typically resembles the full\npath of the module they\u2019re testing. For example, the item loaders code is in:\nscrapy.contrib.loader\n\n\nAnd their unit-tests are in:\nscrapy.tests.test_contrib_loader\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/contributing.html", "title": ["Contributing to Scrapy \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nVersioning and API Stability\u00b6\n\nVersioning\u00b6\nScrapy uses the odd-numbered versions for development releases.\nThere are 3 numbers in a Scrapy version: A.B.C\nA is the major version. This will rarely change and will signify very\nlarge changes. So far, only zero is available for A as Scrapy hasn\u2019t yet\nreached 1.0.\nB is the release number. This will include many changes including features\nand things that possibly break backwards compatibility. Even Bs will be\nstable branches, and odd Bs will be development.\nC is the bugfix release number.\nFor example:\n0.14.1 is the first bugfix release of the 0.14 series (safe to use in\nproduction)\n\n\nAPI Stability\u00b6\nAPI stability is one of Scrapy major goals for the 1.0 release, which doesn\u2019t\nhave a due date scheduled yet.\nMethods or functions that start with a single dash (_) are private and\nshould never be relied as stable. Besides those, the plan is to stabilize and\ndocument the entire API, as we approach the 1.0 release.\nAlso, keep in mind that stable doesn\u2019t mean complete: stable APIs could grow\nnew methods or functionality but the existing methods should keep working the\nsame way.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/versioning.html", "title": ["Versioning and API Stability \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExperimental features\u00b6\nThis section documents experimental Scrapy features that may become stable in\nfuture releases, but whose API is not yet stable. Use them with caution, and\nsubscribe to the mailing lists to get\nnotified of any changes.\nSince it\u2019s not revised so frequently, this section may contain documentation\nwhich is outdated, incomplete or overlapping with stable documentation (until\nit\u2019s properly merged) . Use at your own risk.\n\nWarning\nThis documentation is a work in progress. Use at your own risk.\n\nNo experimental features at this time\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/experimental/index.html", "title": ["Experimental features \u2014 Scrapy 0.16.4 documentation"]}][{"content": "\n          \n            \n  \nScrapy 0.16 documentation\u00b6\nThis documentation contains everything you need to know about Scrapy.\n\nGetting help\u00b6\nHaving trouble? We\u2019d like to help!\nTry the FAQ \u2013 it\u2019s got answers to some common questions.\nLooking for specific information? Try the Index or Module Index.\nSearch for information in the archives of the scrapy-users mailing list, or\npost a question.\nAsk a question in the #scrapy IRC channel.\nReport bugs with Scrapy in our issue tracker.\n\n\nFirst steps\u00b6\n\n\nScrapy at a glance\nUnderstand what Scrapy is and how it can help you.\nInstallation guide\nGet Scrapy installed on your computer.\nScrapy Tutorial\nWrite your first Scrapy project.\nExamples\nLearn more by playing with a pre-made Scrapy project.\n\n\nBasic concepts\u00b6\n\n\nCommand line tool\nLearn about the command-line tool used to manage your Scrapy project.\nItems\nDefine the data you want to scrape.\nSpiders\nWrite the rules to crawl your websites.\nSelectors\nExtract the data from web pages using XPath.\nScrapy shell\nTest your extraction code in an interactive environment.\nItem Loaders\nPopulate your items with the extracted data.\nItem Pipeline\nPost-process and store your scraped data.\nFeed exports\nOutput your scraped data using different formats and storages.\nLink Extractors\nConvenient classes to extract links to follow from pages.\n\n\nBuilt-in services\u00b6\n\n\nLogging\nUnderstand the simple logging facility provided by Scrapy.\nStats Collection\nCollect statistics about your scraping crawler.\nSending e-mail\nSend email notifications when certain events occur.\nTelnet Console\nInspect a running crawler using a built-in Python console.\nWeb Service\nMonitor and control a crawler using a web service.\n\n\nSolving specific problems\u00b6\n\n\nFrequently Asked Questions\nGet answers to most frequently asked questions.\nDebugging Spiders\nLearn how to debug common problems of your scrapy spider.\nSpiders Contracts\nLearn how to use contracts for testing your spiders.\nCommon Practices\nGet familiar with some Scrapy common practices.\nBroad Crawls\nTune Scrapy for crawling a lot domains in parallel.\nUsing Firefox for scraping\nLearn how to scrape with Firefox and some useful add-ons.\nUsing Firebug for scraping\nLearn how to scrape efficiently using Firebug.\nDebugging memory leaks\nLearn how to find and get rid of memory leaks in your crawler.\nDownloading Item Images\nDownload static images associated with your scraped items.\nUbuntu packages\nInstall latest Scrapy packages easily on Ubuntu\nScrapy Service (scrapyd)\nDeploying your Scrapy project in production.\nAutoThrottle extension\nAdjust crawl rate dynamically based on load.\nJobs: pausing and resuming crawls\nLearn how to pause and resume crawls for large spiders.\nDjangoItem\nWrite scraped items using Django models.\n\n\nExtending Scrapy\u00b6\n\n\nArchitecture overview\nUnderstand the Scrapy architecture.\nDownloader Middleware\nCustomize how pages get requested and downloaded.\nSpider Middleware\nCustomize the input and output of your spiders.\nExtensions\nExtend Scrapy with your custom functionality\nCore API\nUse it on extensions and middlewares to extend Scrapy functionality\n\n\nReference\u00b6\n\n\nCommand line tool\nLearn about the command-line tool and see all available commands.\nRequests and Responses\nUnderstand the classes used to represent HTTP requests and responses.\nSettings\nLearn how to configure Scrapy and see all available settings.\nSignals\nSee all available signals and how to work with them.\nExceptions\nSee all available exceptions and their meaning.\nItem Exporters\nQuickly export your scraped items to a file (XML, CSV, etc).\n\n\nAll the rest\u00b6\n\n\nRelease notes\nSee what has changed in recent Scrapy versions.\nContributing to Scrapy\nLearn how to contribute to the Scrapy project.\nVersioning and API Stability\nUnderstand Scrapy versioning and API stability.\nExperimental features\nLearn about bleeding-edge features.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/", "title": ["Scrapy 0.16 documentation \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nScrapy at a glance\u00b6\nScrapy is an application framework for crawling web sites and extracting\nstructured data which can be used for a wide range of useful applications, like\ndata mining, information processing or historical archival.\nEven though Scrapy was originally designed for screen scraping (more\nprecisely, web scraping), it can also be used to extract data using APIs\n(such as Amazon Associates Web Services) or as a general purpose web\ncrawler.\nThe purpose of this document is to introduce you to the concepts behind Scrapy\nso you can get an idea of how it works and decide if Scrapy is what you need.\nWhen you\u2019re ready to start a project, you can start with the tutorial.\n\nPick a website\u00b6\nSo you need to extract some information from a website, but the website doesn\u2019t\nprovide any API or mechanism to access that info programmatically.  Scrapy can\nhelp you extract that information.\nLet\u2019s say we want to extract the URL, name, description and size of all torrent\nfiles added today in the Mininova site.\nThe list of all torrents added today can be found on this page:\n\nhttp://www.mininova.org/today\n\n\nDefine the data you want to scrape\u00b6\nThe first thing is to define the data we want to scrape. In Scrapy, this is\ndone through Scrapy Items (Torrent files, in this case).\nThis would be our Item:\nfrom scrapy.item import Item, Field\n\nclass Torrent(Item):\n    url = Field()\n    name = Field()\n    description = Field()\n    size = Field()\n\n\n\n\nWrite a Spider to extract the data\u00b6\nThe next thing is to write a Spider which defines the start URL\n(http://www.mininova.org/today), the rules for following links and the rules\nfor extracting the data from pages.\nIf we take a look at that page content we\u2019ll see that all torrent URLs are like\nhttp://www.mininova.org/tor/NUMBER where NUMBER is an integer. We\u2019ll use\nthat to construct the regular expression for the links to follow: /tor/\\d+.\nWe\u2019ll use XPath for selecting the data to extract from the web page HTML\nsource. Let\u2019s take one of those torrent pages:\n\nhttp://www.mininova.org/tor/2657665\nAnd look at the page HTML source to construct the XPath to select the data we\nwant which is: torrent name, description and size.\nBy looking at the page HTML source we can see that the file name is contained\ninside a <h1> tag:\n<h1>Home[2009][Eng]XviD-ovd</h1>\n\n\nAn XPath expression to extract the name could be:\n//h1/text()\n\n\nAnd the description is contained inside a <div> tag with id=\"description\":\n<h2>Description:</h2>\n\n<div id=\"description\">\n\"HOME\" - a documentary film by Yann Arthus-Bertrand\n<br/>\n<br/>\n***\n<br/>\n<br/>\n\"We are living in exceptional times. Scientists tell us that we have 10 years to change the way we live, avert the depletion of natural resources and the catastrophic evolution of the Earth's climate.\n\n...\n\n\nAn XPath expression to select the description could be:\n//div[@id='description']\n\n\nFinally, the file size is contained in the second <p> tag inside the <div>\ntag with id=specifications:\n<div id=\"specifications\">\n\n<p>\n<strong>Category:</strong>\n<a href=\"/cat/4\">Movies</a> &gt; <a href=\"/sub/35\">Documentary</a>\n</p>\n\n<p>\n<strong>Total size:</strong>\n699.79&nbsp;megabyte</p>\n\n\nAn XPath expression to select the description could be:\n//div[@id='specifications']/p[2]/text()[2]\n\n\nFor more information about XPath see the XPath reference.\nFinally, here\u2019s the spider code:\nclass MininovaSpider(CrawlSpider):\n\n    name = 'mininova.org'\n    allowed_domains = ['mininova.org']\n    start_urls = ['http://www.mininova.org/today']\n    rules = [Rule(SgmlLinkExtractor(allow=['/tor/\\d+']), 'parse_torrent')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n\n        torrent = TorrentItem()\n        torrent['url'] = response.url\n        torrent['name'] = x.select(\"//h1/text()\").extract()\n        torrent['description'] = x.select(\"//div[@id='description']\").extract()\n        torrent['size'] = x.select(\"//div[@id='info-left']/p[2]/text()[2]\").extract()\n        return torrent\n\n\nFor brevity\u2019s sake, we intentionally left out the import statements. The\nTorrent item is defined above.\n\n\nRun the spider to extract the data\u00b6\nFinally, we\u2019ll run the spider to crawl the site an output file\nscraped_data.json with the scraped data in JSON format:\nscrapy crawl mininova.org -o scraped_data.json -t json\n\nThis uses feed exports to generate the JSON file.\nYou can easily change the export format (XML or CSV, for example) or the\nstorage backend (FTP or Amazon S3, for example).\nYou can also write an item pipeline to store the\nitems in a database very easily.\n\n\nReview scraped data\u00b6\nIf you check the scraped_data.json file after the process finishes, you\u2019ll\nsee the scraped items there:\n[{\"url\": \"http://www.mininova.org/tor/2657665\", \"name\": [\"Home[2009][Eng]XviD-ovd\"], \"description\": [\"HOME - a documentary film by ...\"], \"size\": [\"699.69 megabyte\"]},\n# ... other items ...\n]\n\n\nYou\u2019ll notice that all field values (except for the url which was assigned\ndirectly) are actually lists. This is because the selectors return lists. You may want to store single values, or\nperform some additional parsing/cleansing to the values. That\u2019s what\nItem Loaders are for.\n\n\nWhat else?\u00b6\nYou\u2019ve seen how to extract and store items from a website using Scrapy, but\nthis is just the surface. Scrapy provides a lot of powerful features for making\nscraping easy and efficient, such as:\nBuilt-in support for selecting and extracting data\nfrom HTML and XML sources\nBuilt-in support for cleaning and sanitizing the scraped data using a\ncollection of reusable filters (called Item Loaders)\nshared between all the spiders.\nBuilt-in support for generating feed exports in\nmultiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,\nS3, local filesystem)\nA media pipeline for automatically downloading images\n(or any other media) associated with the scraped items\nSupport for extending Scrapy by plugging\nyour own functionality using signals and a\nwell-defined API (middlewares, extensions, and\npipelines).\nWide range of built-in middlewares and extensions for:cookies and session handling\nHTTP compression\nHTTP authentication\nHTTP cache\nuser-agent spoofing\nrobots.txt\ncrawl depth restriction\nand more\n\nRobust encoding support and auto-detection, for dealing with foreign,\nnon-standard and broken encoding declarations.\nSupport for creating spiders based on pre-defined templates, to speed up\nspider creation and make their code more consistent on large projects. See\ngenspider command for more details.\nExtensible stats collection for multiple spider\nmetrics, useful for monitoring the performance of your spiders and detecting\nwhen they get broken\nAn Interactive shell console for trying XPaths, very\nuseful for writing and debugging your spiders\nA System service designed to ease the deployment and\nrun of your spiders in production.\nA built-in Web service for monitoring and\ncontrolling your bot\nA Telnet console for hooking into a Python\nconsole running inside your Scrapy process, to introspect and debug your\ncrawler\nLogging facility that you can hook on to for catching\nerrors during the scraping process.\nSupport for crawling based on URLs discovered through Sitemaps\nA caching DNS resolver\n\n\nWhat\u2019s next?\u00b6\nThe next obvious steps are for you to download Scrapy, read the\ntutorial and join the community. Thanks for your\ninterest!\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/overview.html", "title": ["Scrapy at a glance \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nInstallation guide\u00b6\n\nPre-requisites\u00b6\nThe installation steps assume that you have the following things installed:\nPython 2.6 or 2.7\nOpenSSL. This comes preinstalled in all operating systems except Windows (see Platform specific installation notes)\npip or easy_install Python package managers\n\n\nInstalling Scrapy\u00b6\nYou can install Scrapy using easy_install or pip (which is the canonical way to\ndistribute and install Python packages).\n\nNote\nCheck Platform specific installation notes first.\n\nTo install using pip:\npip install Scrapy\n\nTo install using easy_install:\neasy_install Scrapy\n\n\n\nPlatform specific installation notes\u00b6\n\nWindows\u00b6\nAfter installing Python, follow these steps before installing Scrapy:\nadd the C:\\python27\\Scripts and C:\\python27 folders to the system\npath by adding those directories to the PATH environment variable from\nthe Control Panel.\ninstall OpenSSL by following these steps:go to Win32 OpenSSL page\ndownload Visual C++ 2008 redistributables for your Windows and architecture\ndownload OpenSSL for your Windows and architecture (the regular version, not the light one)\nadd the c:\\openssl-win32\\bin (or similar) directory to your PATH, the same way you added python27 in the first step`` in the first step\n\nsome binary packages that Scrapy depends on (like Twisted, lxml and pyOpenSSL) require a compiler available to install, and fail if you don\u2019t have Visual Studio installed. You can find Windows installers for those in the following links. Make sure you respect your Python version and Windows architecture.pywin32: http://sourceforge.net/projects/pywin32/files/\nTwisted: http://twistedmatrix.com/trac/wiki/Downloads\nzope.interface: download the egg from zope.interface pypi page and install it by running easy_install file.egg\nlxml: http://pypi.python.org/pypi/lxml/\npyOpenSSL: https://launchpad.net/pyopenssl\n\n\nUbuntu 9.10 or above\u00b6\nDon\u2019t use the python-scrapy package provided by Ubuntu, they are\ntypically too old and slow to catch up with latest Scrapy.\nInstead, use the official Ubuntu Packages, which already\nsolve all dependencies for you and are continuously updated with the latest bug\nfixes.\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/install.html", "title": ["Installation guide \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nScrapy Tutorial\u00b6\nIn this tutorial, we\u2019ll assume that Scrapy is already installed on your system.\nIf that\u2019s not the case, see Installation guide.\nWe are going to use Open directory project (dmoz) as\nour example domain to scrape.\nThis tutorial will walk you through these tasks:\nCreating a new Scrapy project\nDefining the Items you will extract\nWriting a spider to crawl a site and extract\nItems\nWriting an Item Pipeline to store the\nextracted Items\nScrapy is written in Python. If you\u2019re new to the language you might want to\nstart by getting an idea of what the language is like, to get the most out of\nScrapy.  If you\u2019re already familiar with other languages, and want to learn\nPython quickly, we recommend Learn Python The Hard Way.  If you\u2019re new to programming\nand want to start with Python, take a look at this list of Python resources\nfor non-programmers.\n\nCreating a project\u00b6\nBefore you start scraping, you will have set up a new Scrapy project. Enter a\ndirectory where you\u2019d like to store your code and then run:\nscrapy startproject tutorial\n\nThis will create a tutorial directory with the following contents:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\nThese are basically:\nscrapy.cfg: the project configuration file\ntutorial/: the project\u2019s python module, you\u2019ll later import your code from\nhere.\ntutorial/items.py: the project\u2019s items file.\ntutorial/pipelines.py: the project\u2019s pipelines file.\ntutorial/settings.py: the project\u2019s settings file.\ntutorial/spiders/: a directory where you\u2019ll later put your spiders.\n\n\nDefining our Item\u00b6\nItems are containers that will be loaded with the scraped data; they work\nlike simple python dicts but provide additional protecting against populating\nundeclared fields, to prevent typos.\nThey are declared by creating an scrapy.item.Item class an defining\nits attributes as scrapy.item.Field objects, like you will in an ORM\n(don\u2019t worry if you\u2019re not familiar with ORMs, you will see that this is an\neasy task).\nWe begin by modeling the item that we will use to hold the sites data obtained\nfrom dmoz.org, as we want to capture the name, url and description of the\nsites, we define fields for each of these three attributes. To do that, we edit\nitems.py, found in the tutorial directory. Our Item class looks like this:\nfrom scrapy.item import Item, Field\n\nclass DmozItem(Item):\n    title = Field()\n    link = Field()\n    desc = Field()\n\n\nThis may seem complicated at first, but defining the item allows you to use other handy\ncomponents of Scrapy that need to know how your item looks like.\n\n\nOur first Spider\u00b6\nSpiders are user-written classes used to scrape information from a domain (or group\nof domains).\nThey define an initial list of URLs to download, how to follow links, and how\nto parse the contents of those pages to extract items.\nTo create a Spider, you must subclass scrapy.spider.BaseSpider, and\ndefine the three main, mandatory, attributes:\nname: identifies the Spider. It must be\nunique, that is, you can\u2019t set the same name for different Spiders.\n\nstart_urls: is a list of URLs where the\nSpider will begin to crawl from.  So, the first pages downloaded will be those\nlisted here. The subsequent URLs will be generated successively from data\ncontained in the start URLs.\n\nparse() is a method of the spider, which will\nbe called with the downloaded Response object of each\nstart URL. The response is passed to the method as the first and only\nargument.\nThis method is responsible for parsing the response data and extracting\nscraped data (as scraped items) and more URLs to follow.\nThe parse() method is in charge of processing\nthe response and returning scraped data (as Item\nobjects) and more URLs to follow (as Request objects).\n\nThis is the code for our first Spider; save it in a file named\ndmoz_spider.py under the dmoz/spiders directory:\nfrom scrapy.spider import BaseSpider\n\nclass DmozSpider(BaseSpider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        open(filename, 'wb').write(response.body)\n\n\n\nCrawling\u00b6\nTo put our spider to work, go to the project\u2019s top level directory and run:\nscrapy crawl dmoz\n\nThe crawl dmoz command runs the spider for the dmoz.org domain. You\nwill get an output similar to this:\n2008-08-20 03:51:13-0300 [scrapy] INFO: Started project: dmoz\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled extensions: ...\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled downloader middlewares: ...\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled spider middlewares: ...\n2008-08-20 03:51:13-0300 [tutorial] INFO: Enabled item pipelines: ...\n2008-08-20 03:51:14-0300 [dmoz] INFO: Spider opened\n2008-08-20 03:51:14-0300 [dmoz] DEBUG: Crawled <http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: <None>)\n2008-08-20 03:51:14-0300 [dmoz] DEBUG: Crawled <http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: <None>)\n2008-08-20 03:51:14-0300 [dmoz] INFO: Spider closed (finished)\n\nPay attention to the lines containing [dmoz], which corresponds to our\nspider. You can see a log line for each URL defined in start_urls. Because\nthese URLs are the starting ones, they have no referrers, which is shown at the\nend of the log line, where it says (referer: <None>).\nBut more interesting, as our parse method instructs, two files have been\ncreated: Books and Resources, with the content of both URLs.\n\nWhat just happened under the hood?\u00b6\nScrapy creates scrapy.http.Request objects for each URL in the\nstart_urls attribute of the Spider, and assigns them the parse method of\nthe spider as their callback function.\nThese Requests are scheduled, then executed, and\nscrapy.http.Response objects are returned and then fed back to the\nspider, through the parse() method.\n\n\n\nExtracting Items\u00b6\n\nIntroduction to Selectors\u00b6\nThere are several ways to extract data from web pages. Scrapy uses a mechanism\nbased on XPath expressions called XPath selectors.\nFor more information about selectors and other extraction mechanisms see the\nXPath selectors documentation.\nHere are some examples of XPath expressions and their meanings:\n/html/head/title: selects the <title> element, inside the <head>\nelement of a HTML document\n/html/head/title/text(): selects the text inside the aforementioned\n<title> element.\n//td: selects all the <td> elements\n//div[@class=\"mine\"]: selects all div elements which contain an\nattribute class=\"mine\"\nThese are just a couple of simple examples of what you can do with XPath, but\nXPath expressions are indeed much more powerful. To learn more about XPath we\nrecommend this XPath tutorial.\nFor working with XPaths, Scrapy provides a XPathSelector\nclass, which comes in two flavours, HtmlXPathSelector\n(for HTML data) and XmlXPathSelector (for XML data). In\norder to use them you must instantiate the desired class with a\nResponse object.\nYou can see selectors as objects that represent nodes in the document\nstructure. So, the first instantiated selectors are associated to the root\nnode, or the entire document.\nSelectors have three methods (click on the method to see the complete API\ndocumentation).\nselect(): returns a list of selectors, each of\nthem representing the nodes selected by the xpath expression given as\nargument.\n\nextract(): returns a unicode string with\nthe data selected by the XPath selector.\n\n\nre(): returns a list of unicode strings\nextracted by applying the regular expression given as argument.\n\n\n\nTrying Selectors in the Shell\u00b6\nTo illustrate the use of Selectors we\u2019re going to use the built-in Scrapy\nshell, which also requires IPython (an extended Python console)\ninstalled on your system.\nTo start a shell, you must go to the project\u2019s top level directory and run:\nscrapy shell http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\n\nThis is what the shell looks like:\n[ ... Scrapy log here ... ]\n\n[s] Available Scrapy objects:\n[s] 2010-08-19 21:45:59-0300 [default] INFO: Spider closed (finished)\n[s]   hxs        <HtmlXPathSelector (http://www.dmoz.org/Computers/Programming/Languages/Python/Books/) xpath=None>\n[s]   item       Item()\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   spider     <BaseSpider 'default' at 0x1b6c2d0>\n[s]   xxs        <XmlXPathSelector (http://www.dmoz.org/Computers/Programming/Languages/Python/Books/) xpath=None>\n[s] Useful shortcuts:\n[s]   shelp()           Print this help\n[s]   fetch(req_or_url) Fetch a new request or URL and update shell objects\n[s]   view(response)    View response in a browser\n\nIn [1]:\n\nAfter the shell loads, you will have the response fetched in a local\nresponse variable, so if you type response.body you will see the body\nof the response, or you can type response.headers to see its headers.\nThe shell also instantiates two selectors, one for HTML (in the hxs\nvariable) and one for XML (in the xxs variable) with this response. So let\u2019s\ntry them:\nIn [1]: hxs.select('//title')\nOut[1]: [<HtmlXPathSelector (title) xpath=//title>]\n\nIn [2]: hxs.select('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: hxs.select('//title/text()')\nOut[3]: [<HtmlXPathSelector (text) xpath=//title/text()>]\n\nIn [4]: hxs.select('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: hxs.select('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\nExtracting the data\u00b6\nNow, let\u2019s try to extract some real information from those pages.\nYou could type response.body in the console, and inspect the source code to\nfigure out the XPaths you need to use. However, inspecting the raw HTML code\nthere could become a very tedious task. To make this an easier task, you can\nuse some Firefox extensions like Firebug. For more information see\nUsing Firebug for scraping and Using Firefox for scraping.\nAfter inspecting the page source, you\u2019ll find that the web sites information\nis inside a <ul> element, in fact the second <ul> element.\nSo we can select each <li> element belonging to the sites list with this\ncode:\nhxs.select('//ul/li')\n\n\nAnd from them, the sites descriptions:\nhxs.select('//ul/li/text()').extract()\n\n\nThe sites titles:\nhxs.select('//ul/li/a/text()').extract()\n\n\nAnd the sites links:\nhxs.select('//ul/li/a/@href').extract()\n\n\nAs we said before, each select() call returns a list of selectors, so we can\nconcatenate further select() calls to dig deeper into a node. We are going to use\nthat property here, so:\nsites = hxs.select('//ul/li')\nfor site in sites:\n    title = site.select('a/text()').extract()\n    link = site.select('a/@href').extract()\n    desc = site.select('text()').extract()\n    print title, link, desc\n\n\n\nNote\nFor a more detailed description of using nested selectors, see\nNesting selectors and\nWorking with relative XPaths in the Selectors\ndocumentation\n\nLet\u2019s add this code to our spider:\nfrom scrapy.spider import BaseSpider\nfrom scrapy.selector import HtmlXPathSelector\n\nclass DmozSpider(BaseSpider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        sites = hxs.select('//ul/li')\n        for site in sites:\n            title = site.select('a/text()').extract()\n            link = site.select('a/@href').extract()\n            desc = site.select('text()').extract()\n            print title, link, desc\n\n\nNow try crawling the dmoz.org domain again and you\u2019ll see sites being printed\nin your output, run:\nscrapy crawl dmoz\n\n\n\n\nUsing our item\u00b6\nItem objects are custom python dicts; you can access the\nvalues of their fields (attributes of the class we defined earlier) using the\nstandard dict syntax like:\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\nSpiders are expected to return their scraped data inside\nItem objects. So, in order to return the data we\u2019ve\nscraped so far, the final code for our Spider would be like this:\nfrom scrapy.spider import BaseSpider\nfrom scrapy.selector import HtmlXPathSelector\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(BaseSpider):\n   name = \"dmoz\"\n   allowed_domains = [\"dmoz.org\"]\n   start_urls = [\n       \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n       \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n   ]\n\n   def parse(self, response):\n       hxs = HtmlXPathSelector(response)\n       sites = hxs.select('//ul/li')\n       items = []\n       for site in sites:\n           item = DmozItem()\n           item['title'] = site.select('a/text()').extract()\n           item['link'] = site.select('a/@href').extract()\n           item['desc'] = site.select('text()').extract()\n           items.append(item)\n       return items\n\n\n\nNote\nYou can find a fully-functional variant of this spider in the dirbot\nproject available at https://github.com/scrapy/dirbot\n\nNow doing a crawl on the dmoz.org domain yields DmozItem\u2018s:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\nStoring the scraped data\u00b6\nThe simplest way to store the scraped data is by using the Feed exports, with the following command:\nscrapy crawl dmoz -o items.json -t json\n\nThat will generate a items.json file containing all scraped items,\nserialized in JSON.\nIn small projects (like the one in this tutorial), that should be enough.\nHowever, if you want to perform more complex things with the scraped items, you\ncan write an Item Pipeline. As with Items, a\nplaceholder file for Item Pipelines has been set up for you when the project is\ncreated, in tutorial/pipelines.py. Though you don\u2019t need to implement any item\npipeline if you just want to store the scraped items.\n\n\nNext steps\u00b6\nThis tutorial covers only the basics of Scrapy, but there\u2019s a lot of other\nfeatures not mentioned here. Check the What else? section in\nScrapy at a glance chapter for a quick overview of the most important ones.\nThen, we recommend you continue by playing with an example project (see\nExamples), and then continue with the section\nBasic concepts.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/tutorial.html", "title": ["Scrapy Tutorial \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExamples\u00b6\nThe best way to learn is with examples, and Scrapy is no exception. For this\nreason, there is an example Scrapy project named dirbot, that you can use to\nplay and learn more about Scrapy. It contains the dmoz spider described in the\ntutorial.\nThis dirbot project is available at: https://github.com/scrapy/dirbot\nIt contains a README file with a detailed description of the project contents.\nIf you\u2019re familiar with git, you can checkout the code. Otherwise you can\ndownload a tarball or zip file of the project by clicking on Downloads.\nThe scrapy tag on Snipplr is used for sharing code snippets such as spiders,\nmiddlewares, extensions, or scripts. Feel free (and encouraged!) to share any\ncode there.\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/intro/examples.html", "title": ["Examples \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nCommand line tool\u00b6\n\nNew in version 0.10.\nScrapy is controlled through the scrapy command-line tool, to be referred\nhere as the \u201cScrapy tool\u201d to differentiate it from their sub-commands which we\njust call \u201ccommands\u201d, or \u201cScrapy commands\u201d.\nThe Scrapy tool provides several commands, for multiple purposes, and each one\naccepts a different set of arguments and options.\n\nDefault structure of Scrapy projects\u00b6\nBefore delving into the command-line tool and its sub-commands, let\u2019s first\nunderstand the directory structure of a Scrapy project.\nEven thought it can be modified, all Scrapy projects have the same file\nstructure by default, similar to this:\nscrapy.cfg\nmyproject/\n    __init__.py\n    items.py\n    pipelines.py\n    settings.py\n    spiders/\n        __init__.py\n        spider1.py\n        spider2.py\n        ...\n\nThe directory where the scrapy.cfg file resides is known as the project\nroot directory. That file contains the name of the python module that defines\nthe project settings. Here is an example:\n[settings]\ndefault = myproject.settings\n\n\n\n\nUsing the scrapy tool\u00b6\nYou can start by running the Scrapy tool with no arguments and it will print\nsome usage help and the available commands:\nScrapy X.Y - no active project\n\nUsage:\n  scrapy <command> [options] [args]\n\nAvailable commands:\n  crawl         Start crawling a spider or URL\n  fetch         Fetch a URL using the Scrapy downloader\n[...]\n\nThe first line will print the currently active project, if you\u2019re inside a\nScrapy project. In this, it was run from outside a project. If run from inside\na project it would have printed something like this:\nScrapy X.Y - project: myproject\n\nUsage:\n  scrapy <command> [options] [args]\n\n[...]\n\n\nCreating projects\u00b6\nThe first thing you typically do with the scrapy tool is create your Scrapy\nproject:\nscrapy startproject myproject\n\nThat will create a Scrapy project under the myproject directory.\nNext, you go inside the new project directory:\ncd myproject\n\nAnd you\u2019re ready to use use the scrapy command to manage and control your\nproject from there.\n\n\nControlling projects\u00b6\nYou use the scrapy tool from inside your projects to control and manage\nthem.\nFor example, to create a new spider:\nscrapy genspider mydomain mydomain.com\n\nSome Scrapy commands (like crawl) must be run from inside a Scrapy\nproject. See the commands reference below for more\ninformation on which commands must be run from inside projects, and which not.\nAlso keep in mind that some commands may have slightly different behaviours\nwhen running them from inside projects. For example, the fetch command will use\nspider-overridden behaviours (such as the user_agent attribute to override\nthe user-agent) if the url being fetched is associated with some specific\nspider. This is intentional, as the fetch command is meant to be used to\ncheck how spiders are downloading pages.\n\n\n\nAvailable tool commands\u00b6\nThis section contains a list of the available built-in commands with a\ndescription and some usage examples. Remember you can always get more info\nabout each command by running:\nscrapy <command> -h\n\n\nAnd you can see all available commands with:\nscrapy -h\n\n\nThere are two kinds of commands, those that only work from inside a Scrapy\nproject (Project-specific commands) and those that also work without an active\nScrapy project (Global commands), though they may behave slightly different\nwhen running from inside a project (as they would use the project overridden\nsettings).\nGlobal commands:\nstartproject\nsettings\nrunspider\nshell\nfetch\nview\nversion\nProject-only commands:\ncrawl\ncheck\nlist\nedit\nparse\ngenspider\nserver\ndeploy\n\nstartproject\u00b6\nSyntax: scrapy startproject <project_name>\nRequires project: no\nCreates a new Scrapy project named project_name, under the project_name\ndirectory.\nUsage example:\n$ scrapy startproject myproject\n\n\n\ngenspider\u00b6\nSyntax: scrapy genspider [-t template] <name> <domain>\nRequires project: yes\nCreate a new spider in the current project.\nThis is just a convenient shortcut command for creating spiders based on\npre-defined templates, but certainly not the only way to create spiders. You\ncan just create the spider source code files yourself, instead of using this\ncommand.\nUsage example:\n$ scrapy genspider -l\nAvailable templates:\n  basic\n  crawl\n  csvfeed\n  xmlfeed\n\n$ scrapy genspider -d basic\nfrom scrapy.spider import BaseSpider\n\nclass $classname(BaseSpider):\n    name = \"$name\"\n    allowed_domains = [\"$domain\"]\n    start_urls = (\n        'http://www.$domain/',\n        )\n\n    def parse(self, response):\n        pass\n\n$ scrapy genspider -t basic example example.com\nCreated spider 'example' using template 'basic' in module:\n  mybot.spiders.example\n\n\n\ncrawl\u00b6\nSyntax: scrapy crawl <spider>\nRequires project: yes\nStart crawling a spider.\nUsage examples:\n$ scrapy crawl myspider\n[ ... myspider starts crawling ... ]\n\n\n\ncheck\u00b6\nSyntax: scrapy check [-l] <spider>\nRequires project: yes\nRun contract checks.\nUsage examples:\n$ scrapy check -l\nfirst_spider\n  * parse\n  * parse_item\nsecond_spider\n  * parse\n  * parse_item\n\n$ scrapy check\n[FAILED] first_spider:parse_item\n>>> 'RetailPricex' field is missing\n\n[FAILED] first_spider:parse\n>>> Returned 92 requests, expected 0..4\n\n\n\nserver\u00b6\nSyntax: scrapy server\nRequires project: yes\nStart Scrapyd server for this project, which can be referred from the JSON API\nwith the project name default. For more info see: Scrapy Service (scrapyd).\nUsage example:\n$ scrapy server\n[ ... scrapyd starts and stays idle waiting for spiders to get scheduled ... ]\n\nTo schedule spiders, use the Scrapyd JSON API.\n\n\nlist\u00b6\nSyntax: scrapy list\nRequires project: yes\nList all available spiders in the current project. The output is one spider per\nline.\nUsage example:\n$ scrapy list\nspider1\nspider2\n\n\n\nedit\u00b6\nSyntax: scrapy edit <spider>\nRequires project: yes\nEdit the given spider using the editor defined in the EDITOR\nsetting.\nThis command is provided only as a convenient shortcut for the most common\ncase, the developer is of course free to choose any tool or IDE to write and\ndebug his spiders.\nUsage example:\n$ scrapy edit spider1\n\n\n\nfetch\u00b6\nSyntax: scrapy fetch <url>\nRequires project: no\nDownloads the given URL using the Scrapy downloader and writes the contents to\nstandard output.\nThe interesting thing about this command is that it fetches the page how the\nthe spider would download it. For example, if the spider has an USER_AGENT\nattribute which overrides the User Agent, it will use that one.\nSo this command can be used to \u201csee\u201d how your spider would fetch certain page.\nIf used outside a project, no particular per-spider behaviour would be applied\nand it will just use the default Scrapy downloder settings.\nUsage examples:\n$ scrapy fetch --nolog http://www.example.com/some/page.html\n[ ... html content here ... ]\n\n$ scrapy fetch --nolog --headers http://www.example.com/\n{'Accept-Ranges': ['bytes'],\n 'Age': ['1263   '],\n 'Connection': ['close     '],\n 'Content-Length': ['596'],\n 'Content-Type': ['text/html; charset=UTF-8'],\n 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],\n 'Etag': ['\"573c1-254-48c9c87349680\"'],\n 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],\n 'Server': ['Apache/2.2.3 (CentOS)']}\n\n\n\nview\u00b6\nSyntax: scrapy view <url>\nRequires project: no\nOpens the given URL in a browser, as your Scrapy spider would \u201csee\u201d it.\nSometimes spiders see pages differently from regular users, so this can be used\nto check what the spider \u201csees\u201d and confirm it\u2019s what you expect.\nUsage example:\n$ scrapy view http://www.example.com/some/page.html\n[ ... browser starts ... ]\n\n\n\nshell\u00b6\nSyntax: scrapy shell [url]\nRequires project: no\nStarts the Scrapy shell for the given URL (if given) or empty if not URL is\ngiven. See Scrapy shell for more info.\nUsage example:\n$ scrapy shell http://www.example.com/some/page.html\n[ ... scrapy shell starts ... ]\n\n\n\nparse\u00b6\nSyntax: scrapy parse <url> [options]\nRequires project: yes\nFetches the given URL and parses with the spider that handles it, using the\nmethod passed with the --callback option, or parse if not given.\nSupported options:\n--callback or -c: spider method to use as callback for parsing the\nresponse\n--rules or -r: use CrawlSpider\nrules to discover the callback (ie. spider method) to use for parsing the\nresponse\n--noitems: don\u2019t show scraped items\n--nolinks: don\u2019t show extracted links\n--depth or -d: depth level for which the requests should be followed\nrecursively (default: 1)\n--verbose or -v: display information for each depth level\nUsage example:\n$ scrapy parse http://www.example.com/ -c parse_item\n[ ... scrapy log lines crawling example.com spider ... ]\n\n>>> STATUS DEPTH LEVEL 1 <<<\n# Scraped Items  ------------------------------------------------------------\n[{'name': u'Example item',\n 'category': u'Furniture',\n 'length': u'12 cm'}]\n\n# Requests  -----------------------------------------------------------------\n[]\n\n\n\nsettings\u00b6\nSyntax: scrapy settings [options]\nRequires project: no\nGet the value of a Scrapy setting.\nIf used inside a project it\u2019ll show the project setting value, otherwise it\u2019ll\nshow the default Scrapy value for that setting.\nExample usage:\n$ scrapy settings --get BOT_NAME\nscrapybot\n$ scrapy settings --get DOWNLOAD_DELAY\n0\n\n\n\nrunspider\u00b6\nSyntax: scrapy runspider <spider_file.py>\nRequires project: no\nRun a spider self-contained in a Python file, without having to create a\nproject.\nExample usage:\n$ scrapy runspider myspider.py\n[ ... spider starts crawling ... ]\n\n\n\nversion\u00b6\nSyntax: scrapy version [-v]\nRequires project: no\nPrints the Scrapy version. If used with -v it also prints Python, Twisted\nand Platform info, which is useful for bug reports.\n\n\ndeploy\u00b6\n\nNew in version 0.11.\nSyntax: scrapy deploy [ <target:project> | -l <target> | -L ]\nRequires project: yes\nDeploy the project into a Scrapyd server. See Deploying your project.\n\n\n\nCustom project commands\u00b6\nYou can also add your custom project commands by using the\nCOMMANDS_MODULE setting. See the Scrapy commands in\nscrapy/commands for examples on how to implement your commands.\n\nCOMMANDS_MODULE\u00b6\nDefault: '' (empty string)\nA module to use for looking custom Scrapy commands. This is used to add custom\ncommands for your Scrapy project.\nExample:\nCOMMANDS_MODULE = 'mybot.commands'\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/commands.html", "title": ["Command line tool \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nItems\u00b6\nThe main goal in scraping is to extract structured data from unstructured\nsources, typically, web pages. Scrapy provides the Item class for this\npurpose.\nItem objects are simple containers used to collect the scraped data.\nThey provide a dictionary-like API with a convenient syntax for declaring\ntheir available fields.\n\nDeclaring Items\u00b6\nItems are declared using a simple class definition syntax and Field\nobjects. Here is an example:\nfrom scrapy.item import Item, Field\n\nclass Product(Item):\n    name = Field()\n    price = Field()\n    stock = Field()\n    last_updated = Field(serializer=str)\n\n\n\nNote\nThose familiar with Django will notice that Scrapy Items are\ndeclared similar to Django Models, except that Scrapy Items are much\nsimpler as there is no concept of different field types.\n\n\n\nItem Fields\u00b6\nField objects are used to specify metadata for each field. For\nexample, the serializer function for the last_updated field illustrated in\nthe example above.\nYou can specify any kind of metadata for each field. There is no restriction on\nthe values accepted by Field objects. For this same\nreason, there isn\u2019t a reference list of all available metadata keys. Each key\ndefined in Field objects could be used by a different components, and\nonly those components know about it. You can also define and use any other\nField key in your project too, for your own needs. The main goal of\nField objects is to provide a way to define all field metadata in one\nplace. Typically, those components whose behaviour depends on each field use\ncertain field keys to configure that behaviour. You must refer to their\ndocumentation to see which metadata keys are used by each component.\nIt\u2019s important to note that the Field objects used to declare the item\ndo not stay assigned as class attributes. Instead, they can be accessed through\nthe Item.fields attribute.\nAnd that\u2019s all you need to know about declaring items.\n\n\nWorking with Items\u00b6\nHere are some examples of common tasks performed with items, using the\nProduct item declared above. You will\nnotice the API is very similar to the dict API.\n\nCreating items\u00b6\n>>> product = Product(name='Desktop PC', price=1000)\n>>> print product\nProduct(name='Desktop PC', price=1000)\n\n\n\n\nGetting field values\u00b6\n>>> product['name']\nDesktop PC\n>>> product.get('name')\nDesktop PC\n\n>>> product['price']\n1000\n\n>>> product['last_updated']\nTraceback (most recent call last):\n    ...\nKeyError: 'last_updated'\n\n>>> product.get('last_updated', 'not set')\nnot set\n\n>>> product['lala'] # getting unknown field\nTraceback (most recent call last):\n    ...\nKeyError: 'lala'\n\n>>> product.get('lala', 'unknown field')\n'unknown field'\n\n>>> 'name' in product  # is name field populated?\nTrue\n\n>>> 'last_updated' in product  # is last_updated populated?\nFalse\n\n>>> 'last_updated' in product.fields  # is last_updated a declared field?\nTrue\n\n>>> 'lala' in product.fields  # is lala a declared field?\nFalse\n\n\n\n\nSetting field values\u00b6\n>>> product['last_updated'] = 'today'\n>>> product['last_updated']\ntoday\n\n>>> product['lala'] = 'test' # setting unknown field\nTraceback (most recent call last):\n    ...\nKeyError: 'Product does not support field: lala'\n\n\n\n\nAccessing all populated values\u00b6\nTo access all populated values, just use the typical dict API:\n>>> product.keys()\n['price', 'name']\n\n>>> product.items()\n[('price', 1000), ('name', 'Desktop PC')]\n\n\n\n\nOther common tasks\u00b6\nCopying items:\n>>> product2 = Product(product)\n>>> print product2\nProduct(name='Desktop PC', price=1000)\n\n\nCreating dicts from items:\n>>> dict(product) # create a dict from all populated values\n{'price': 1000, 'name': 'Desktop PC'}\n\n\nCreating items from dicts:\n>>> Product({'name': 'Laptop PC', 'price': 1500})\nProduct(price=1500, name='Laptop PC')\n\n>>> Product({'name': 'Laptop PC', 'lala': 1500}) # warning: unknown field in dict\nTraceback (most recent call last):\n    ...\nKeyError: 'Product does not support field: lala'\n\n\n\n\n\nExtending Items\u00b6\nYou can extend Items (to add more fields or to change some metadata for some\nfields) by declaring a subclass of your original Item.\nFor example:\nclass DiscountedProduct(Product):\n    discount_percent = Field(serializer=str)\n    discount_expiration_date = Field()\n\n\nYou can also extend field metadata by using the previous field metadata and\nappending more values, or changing existing values, like this:\nclass SpecificProduct(Product):\n    name = Field(Product.fields['name'], serializer=my_serializer)\n\n\nThat adds (or replaces) the serializer metadata key for the name field,\nkeeping all the previously existing metadata values.\n\n\nItem objects\u00b6\n\nclass scrapy.item.Item([arg])\u00b6\nReturn a new Item optionally initialized from the given argument.\nItems replicate the standard dict API, including its constructor. The\nonly additional attribute provided by Items is:\n\nfields\u00b6\nA dictionary containing all declared fields for this Item, not only\nthose populated. The keys are the field names and the values are the\nField objects used in the Item declaration.\n\n\nField objects\u00b6\n\nclass scrapy.item.Field([arg])\u00b6\nThe Field class is just an alias to the built-in dict class and\ndoesn\u2019t provide any extra functionality or attributes. In other words,\nField objects are plain-old Python dicts. A separate class is used\nto support the item declaration syntax\nbased on class attributes.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/items.html", "title": ["Items \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSpiders\u00b6\nSpiders are classes which define how a certain site (or group of sites) will be\nscraped, including how to perform the crawl (ie. follow links) and how to\nextract structured data from their pages (ie. scraping items). In other words,\nSpiders are the place where you define the custom behaviour for crawling and\nparsing pages for a particular site (or, in some cases, group of sites).\nFor spiders, the scraping cycle goes through something like this:\nYou start by generating the initial Requests to crawl the first URLs, and\nspecify a callback function to be called with the response downloaded from\nthose requests.\nThe first requests to perform are obtained by calling the\nstart_requests() method which (by default)\ngenerates Request for the URLs specified in the\nstart_urls and the\nparse method as callback function for the\nRequests.\n\nIn the callback function, you parse the response (web page) and return either\nItem objects, Request objects,\nor an iterable of both. Those Requests will also contain a callback (maybe\nthe same) and will then be downloaded by Scrapy and then their\nresponse handled by the specified callback.\n\nIn callback functions, you parse the page contents, typically using\nSelectors (but you can also use BeautifulSoup, lxml or whatever\nmechanism you prefer) and generate items with the parsed data.\n\nFinally, the items returned from the spider will be typically persisted to a\ndatabase (in some Item Pipeline) or written to\na file using Feed exports.\n\nEven though this cycle applies (more or less) to any kind of spider, there are\ndifferent kinds of default spiders bundled into Scrapy for different purposes.\nWe will talk about those types here.\n\nSpider arguments\u00b6\nSpiders can receive arguments that modify their behaviour. Some common uses for\nspider arguments are to define the start URLs or to restrict the crawl to\ncertain sections of the site, but they can be used to configure any\nfunctionality of the spider.\nSpider arguments are passed through the crawl command using the\n-a option. For example:\nscrapy crawl myspider -a category=electronics\n\nSpiders receive arguments in their constructors:\nclass MySpider(BaseSpider):\n    name = 'myspider'\n\n    def __init__(self, category=None):\n        self.start_urls = ['http://www.example.com/categories/%s' % category]\n        # ...\n\n\nSpider arguments can also be passed through the schedule.json API.\n\n\nBuilt-in spiders reference\u00b6\nScrapy comes with some useful generic spiders that you can use, to subclass\nyour spiders from. Their aim is to provide convenient functionality for a few\ncommon scraping cases, like following all links on a site based on certain\nrules, crawling from Sitemaps, or parsing a XML/CSV feed.\nFor the examples used in the following spiders, we\u2019ll assume you have a project\nwith a TestItem declared in a myproject.items module:\nfrom scrapy.item import Item\n\nclass TestItem(Item):\n    id = Field()\n    name = Field()\n    description = Field()\n\n\n\nBaseSpider\u00b6\n\nclass scrapy.spider.BaseSpider\u00b6\nThis is the simplest spider, and the one from which every other spider\nmust inherit from (either the ones that come bundled with Scrapy, or the ones\nthat you write yourself). It doesn\u2019t provide any special functionality. It just\nrequests the given start_urls/start_requests, and calls the spider\u2019s\nmethod parse for each of the resulting responses.\n\nname\u00b6\nA string which defines the name for this spider. The spider name is how\nthe spider is located (and instantiated) by Scrapy, so it must be\nunique. However, nothing prevents you from instantiating more than one\ninstance of the same spider. This is the most important spider attribute\nand it\u2019s required.\nIf the spider scrapes a single domain, a common practice is to name the\nspider after the domain, or without the TLD. So, for example, a\nspider that crawls mywebsite.com would often be called\nmywebsite.\n\nallowed_domains\u00b6\nAn optional list of strings containing domains that this spider is\nallowed to crawl. Requests for URLs not belonging to the domain names\nspecified in this list won\u2019t be followed if\nOffsiteMiddleware is enabled.\n\nstart_urls\u00b6\nA list of URLs where the spider will begin to crawl from, when no\nparticular URLs are specified. So, the first pages downloaded will be those\nlisted here. The subsequent URLs will be generated successively from data\ncontained in the start URLs.\n\nstart_requests()\u00b6\nThis method must return an iterable with the first Requests to crawl for\nthis spider.\nThis is the method called by Scrapy when the spider is opened for\nscraping when no particular URLs are specified. If particular URLs are\nspecified, the make_requests_from_url() is used instead to create\nthe Requests. This method is also called only once from Scrapy, so it\u2019s\nsafe to implement it as a generator.\nThe default implementation uses make_requests_from_url() to\ngenerate Requests for each url in start_urls.\nIf you want to change the Requests used to start scraping a domain, this is\nthe method to override. For example, if you need to start by logging in using\na POST request, you could do:\ndef start_requests(self):\n    return [FormRequest(\"http://www.example.com/login\",\n                        formdata={'user': 'john', 'pass': 'secret'},\n                        callback=self.logged_in)]\n\ndef logged_in(self, response):\n    # here you would extract links to follow and return Requests for\n    # each of them, with another callback\n    pass\n\n\n\nmake_requests_from_url(url)\u00b6\nA method that receives a URL and returns a Request\nobject (or a list of Request objects) to scrape. This\nmethod is used to construct the initial requests in the\nstart_requests() method, and is typically used to convert urls to\nrequests.\nUnless overridden, this method returns Requests with the parse()\nmethod as their callback function, and with dont_filter parameter enabled\n(see Request class for more info).\n\nparse(response)\u00b6\nThis is the default callback used by Scrapy to process downloaded\nresponses, when their requests don\u2019t specify a callback.\nThe parse method is in charge of processing the response and returning\nscraped data and/or more URLs to follow. Other Requests callbacks have\nthe same requirements as the BaseSpider class.\nThis method, as well as any other Request callback, must return an\niterable of Request and/or\nItem objects.\nParameters:response (:class:~scrapy.http.Response`) \u2013 the response to parse\n\nlog(message[, level, component])\u00b6\nLog a message using the scrapy.log.msg() function, automatically\npopulating the spider argument with the name of this\nspider. For more information see Logging.\n\nBaseSpider example\u00b6\nLet\u2019s see an example:\nfrom scrapy import log # This module is useful for printing out debug information\nfrom scrapy.spider import BaseSpider\n\nclass MySpider(BaseSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = [\n        'http://www.example.com/1.html',\n        'http://www.example.com/2.html',\n        'http://www.example.com/3.html',\n    ]\n\n    def parse(self, response):\n        self.log('A response from %s just arrived!' % response.url)\n\n\nAnother example returning multiples Requests and Items from a single callback:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\nfrom myproject.items import MyItem\n\nclass MySpider(BaseSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = [\n        'http://www.example.com/1.html',\n        'http://www.example.com/2.html',\n        'http://www.example.com/3.html',\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        for h3 in hxs.select('//h3').extract():\n            yield MyItem(title=h3)\n\n        for url in hxs.select('//a/@href').extract():\n            yield Request(url, callback=self.parse)\n\n\n\n\n\nCrawlSpider\u00b6\n\nclass scrapy.contrib.spiders.CrawlSpider\u00b6\nThis is the most commonly used spider for crawling regular websites, as it\nprovides a convenient mechanism for following links by defining a set of rules.\nIt may not be the best suited for your particular web sites or project, but\nit\u2019s generic enough for several cases, so you can start from it and override it\nas needed for more custom functionality, or just implement your own spider.\nApart from the attributes inherited from BaseSpider (that you must\nspecify), this class supports a new attribute:\n\nrules\u00b6\nWhich is a list of one (or more) Rule objects.  Each Rule\ndefines a certain behaviour for crawling the site. Rules objects are\ndescribed below. If multiple rules match the same link, the first one\nwill be used, according to the order they\u2019re defined in this attribute.\n\nCrawling rules\u00b6\n\nclass scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)\u00b6\nlink_extractor is a Link Extractor object which\ndefines how links will be extracted from each crawled page.\ncallback is a callable or a string (in which case a method from the spider\nobject with that name will be used) to be called for each link extracted with\nthe specified link_extractor. This callback receives a response as its first\nargument and must return a list containing Item and/or\nRequest objects (or any subclass of them).\n\nWarning\nWhen writing crawl spider rules, avoid using parse as\ncallback, since the CrawlSpider uses the parse method\nitself to implement its logic. So if you override the parse method,\nthe crawl spider will no longer work.\n\ncb_kwargs is a dict containing the keyword arguments to be passed to the\ncallback function\nfollow is a boolean which specifies if links should be followed from each\nresponse extracted with this rule. If callback is None follow defaults\nto True, otherwise it default to False.\nprocess_links is a callable, or a string (in which case a method from the\nspider object with that name will be used) which will be called for each list\nof links extracted from each response using the specified link_extractor.\nThis is mainly used for filtering purposes.\nprocess_request is a callable, or a string (in which case a method from\nthe spider object with that name will be used) which will be called with\nevery request extracted by this rule, and must return a request or None (to\nfilter out the request).\n\n\nCrawlSpider example\u00b6\nLet\u2019s now take a look at an example CrawlSpider with rules:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.item import Item\n\nclass MySpider(CrawlSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com']\n\n    rules = (\n        # Extract links matching 'category.php' (but not matching 'subsection.php')\n        # and follow links from them (since no callback means follow=True by default).\n        Rule(SgmlLinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))),\n\n        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n        Rule(SgmlLinkExtractor(allow=('item\\.php', )), callback='parse_item'),\n    )\n\n    def parse_item(self, response):\n        self.log('Hi, this is an item page! %s' % response.url)\n\n        hxs = HtmlXPathSelector(response)\n        item = Item()\n        item['id'] = hxs.select('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)')\n        item['name'] = hxs.select('//td[@id=\"item_name\"]/text()').extract()\n        item['description'] = hxs.select('//td[@id=\"item_description\"]/text()').extract()\n        return item\n\n\nThis spider would start crawling example.com\u2019s home page, collecting category\nlinks, and item links, parsing the latter with the parse_item method. For\neach item response, some data will be extracted from the HTML using XPath, and\na Item will be filled with it.\n\n\n\nXMLFeedSpider\u00b6\n\nclass scrapy.contrib.spiders.XMLFeedSpider\u00b6\nXMLFeedSpider is designed for parsing XML feeds by iterating through them by a\ncertain node name.  The iterator can be chosen from: iternodes, xml,\nand html.  It\u2019s recommended to use the iternodes iterator for\nperformance reasons, since the xml and html iterators generate the\nwhole DOM at once in order to parse it.  However, using html as the\niterator may be useful when parsing XML with bad markup.\nTo set the iterator and the tag name, you must define the following class\nattributes:\n\niterator\u00b6\nA string which defines the iterator to use. It can be either:\n\n'iternodes' - a fast iterator based on regular expressions\n'html' - an iterator which uses HtmlXPathSelector. Keep in mind\nthis uses DOM parsing and must load all DOM in memory which could be a\nproblem for big feeds\n'xml' - an iterator which uses XmlXPathSelector. Keep in mind\nthis uses DOM parsing and must load all DOM in memory which could be a\nproblem for big feeds\n\nIt defaults to: 'iternodes'.\n\nitertag\u00b6\nA string with the name of the node (or element) to iterate in. Example:\nitertag = 'product'\n\n\n\nnamespaces\u00b6\nA list of (prefix, uri) tuples which define the namespaces\navailable in that document that will be processed with this spider. The\nprefix and uri will be used to automatically register\nnamespaces using the\nregister_namespace() method.\nYou can then specify nodes with namespaces in the itertag\nattribute.\nExample:\nclass YourSpider(XMLFeedSpider):\n\n    namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]\n    itertag = 'n:url'\n    # ...\n\n\nApart from these new attributes, this spider has the following overrideable\nmethods too:\n\nadapt_response(response)\u00b6\nA method that receives the response as soon as it arrives from the spider\nmiddleware, before the spider starts parsing it. It can be used to modify\nthe response body before parsing it. This method receives a response and\nalso returns a response (it could be the same or another one).\n\nparse_node(response, selector)\u00b6\nThis method is called for the nodes matching the provided tag name\n(itertag).  Receives the response and an XPathSelector for each node.\nOverriding this method is mandatory. Otherwise, you spider won\u2019t work.\nThis method must return either a Item object, a\nRequest object, or an iterable containing any of\nthem.\n\nprocess_results(response, results)\u00b6\nThis method is called for each result (item or request) returned by the\nspider, and it\u2019s intended to perform any last time processing required\nbefore returning the results to the framework core, for example setting the\nitem IDs. It receives a list of results and the response which originated\nthose results. It must return a list of results (Items or Requests).\n\nXMLFeedSpider example\u00b6\nThese spiders are pretty easy to use, let\u2019s have a look at one example:\nfrom scrapy import log\nfrom scrapy.contrib.spiders import XMLFeedSpider\nfrom myproject.items import TestItem\n\nclass MySpider(XMLFeedSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com/feed.xml']\n    iterator = 'iternodes' # This is actually unnecessary, since it's the default value\n    itertag = 'item'\n\n    def parse_node(self, response, node):\n        log.msg('Hi, this is a <%s> node!: %s' % (self.itertag, ''.join(node.extract())))\n\n        item = Item()\n        item['id'] = node.select('@id').extract()\n        item['name'] = node.select('name').extract()\n        item['description'] = node.select('description').extract()\n        return item\n\n\nBasically what we did up there was to create a spider that downloads a feed from\nthe given start_urls, and then iterates through each of its item tags,\nprints them out, and stores some random data in an Item.\n\n\n\nCSVFeedSpider\u00b6\n\nclass scrapy.contrib.spiders.CSVFeedSpider\u00b6\nThis spider is very similar to the XMLFeedSpider, except that it iterates\nover rows, instead of nodes. The method that gets called in each iteration\nis parse_row().\n\ndelimiter\u00b6\nA string with the separator character for each field in the CSV file\nDefaults to ',' (comma).\n\nheaders\u00b6\nA list of the rows contained in the file CSV feed which will be used to\nextract fields from it.\n\nparse_row(response, row)\u00b6\nReceives a response and a dict (representing each row) with a key for each\nprovided (or detected) header of the CSV file.  This spider also gives the\nopportunity to override adapt_response and process_results methods\nfor pre- and post-processing purposes.\n\nCSVFeedSpider example\u00b6\nLet\u2019s see an example similar to the previous one, but using a\nCSVFeedSpider:\nfrom scrapy import log\nfrom scrapy.contrib.spiders import CSVFeedSpider\nfrom myproject.items import TestItem\n\nclass MySpider(CSVFeedSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com/feed.csv']\n    delimiter = ';'\n    headers = ['id', 'name', 'description']\n\n    def parse_row(self, response, row):\n        log.msg('Hi, this is a row!: %r' % row)\n\n        item = TestItem()\n        item['id'] = row['id']\n        item['name'] = row['name']\n        item['description'] = row['description']\n        return item\n\n\n\n\n\nSitemapSpider\u00b6\n\nclass scrapy.contrib.spiders.SitemapSpider\u00b6\nSitemapSpider allows you to crawl a site by discovering the URLs using\nSitemaps.\nIt supports nested sitemaps and discovering sitemap urls from\nrobots.txt.\n\nsitemap_urls\u00b6\nA list of urls pointing to the sitemaps whose urls you want to crawl.\nYou can also point to a robots.txt and it will be parsed to extract\nsitemap urls from it.\n\nsitemap_rules\u00b6\nA list of tuples (regex, callback) where:\nregex is a regular expression to match urls extracted from sitemaps.\nregex can be either a str or a compiled regex object.\ncallback is the callback to use for processing the urls that match\nthe regular expression. callback can be a string (indicating the\nname of a spider method) or a callable.\nFor example:\nsitemap_rules = [('/product/', 'parse_product')]\n\n\nRules are applied in order, and only the first one that matches will be\nused.\nIf you omit this attribute, all urls found in sitemaps will be\nprocessed with the parse callback.\n\nsitemap_follow\u00b6\nA list of regexes of sitemap that should be followed. This is is only\nfor sites that use Sitemap index files that point to other sitemap\nfiles.\nBy default, all sitemaps are followed.\n\nSitemapSpider examples\u00b6\nSimplest example: process all urls discovered through sitemaps using the\nparse callback:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/sitemap.xml']\n\n    def parse(self, response):\n        pass # ... scrape item here ...\n\n\nProcess some urls with certain callback and other urls with a different\ncallback:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/sitemap.xml']\n    sitemap_rules = [\n        ('/product/', 'parse_product'),\n        ('/category/', 'parse_category'),\n    ]\n\n    def parse_product(self, response):\n        pass # ... scrape product ...\n\n    def parse_category(self, response):\n        pass # ... scrape category ...\n\n\nFollow sitemaps defined in the robots.txt file and only follow sitemaps\nwhose url contains /sitemap_shop:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/robots.txt']\n    sitemap_rules = [\n        ('/shop/', 'parse_shop'),\n    ]\n    sitemap_follow = ['/sitemap_shops']\n\n    def parse_shop(self, response):\n        pass # ... scrape shop here ...\n\n\nCombine SitemapSpider with other sources of urls:\nfrom scrapy.contrib.spiders import SitemapSpider\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = ['http://www.example.com/robots.txt']\n    sitemap_rules = [\n        ('/shop/', 'parse_shop'),\n    ]\n\n    other_urls = ['http://www.example.com/about']\n\n    def start_requests(self):\n        requests = list(super(MySpider, self).start_requests())\n        requests += [Request(x, callback=self.parse_other) for x in self.other_urls]\n        return requests\n\n    def parse_shop(self, response):\n        pass # ... scrape shop here ...\n\n    def parse_other(self, response):\n        pass # ... scrape other here ...\n\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/spiders.html", "title": ["Spiders \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nLink Extractors\u00b6\nLinkExtractors are objects whose only purpose is to extract links from web\npages (scrapy.http.Response objects) which will be eventually\nfollowed.\nThere are two Link Extractors available in Scrapy by default, but you create\nyour own custom Link Extractors to suit your needs by implementing a simple\ninterface.\nThe only public method that every LinkExtractor has is extract_links,\nwhich receives a Response object and returns a list\nof links. Link Extractors are meant to be instantiated once and their\nextract_links method called several times with different responses, to\nextract links to follow.\nLink extractors are used in the CrawlSpider\nclass (available in Scrapy), through a set of rules, but you can also use it in\nyour spiders, even if you don\u2019t subclass from\nCrawlSpider, as its purpose is very simple: to\nextract links.\n\nBuilt-in link extractors reference\u00b6\nAll available link extractors classes bundled with Scrapy are provided in the\nscrapy.contrib.linkextractors module.\n\nSgmlLinkExtractor\u00b6\n\nclass scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None)\u00b6\nThe SgmlLinkExtractor extends the base BaseSgmlLinkExtractor by\nproviding additional filters that you can specify to extract links,\nincluding regular expressions patterns that the links must match to be\nextracted. All those filters are configured through these constructor\nparameters:\nParameters:allow (a regular expression (or list of)) \u2013 a single regular expression (or list of regular expressions)\nthat the (absolute) urls must match in order to be extracted. If not\ngiven (or empty), it will match all links.\ndeny (a regular expression (or list of)) \u2013 a single regular expression (or list of regular expressions)\nthat the (absolute) urls must match in order to be excluded (ie. not\nextracted). It has precedence over the allow parameter. If not\ngiven (or empty) it won\u2019t exclude any links.\nallow_domains (str or list) \u2013 a single value or a list of string containing\ndomains which will be considered for extracting the links\ndeny_domains (str or list) \u2013 a single value or a list of strings containing\ndomains which won\u2019t be considered for extracting the links\ndeny_extensions (list) \u2013 a list of extensions that should be ignored when\nextracting links. If not given, it will default to the\nIGNORED_EXTENSIONS list defined in the scrapy.linkextractor\nmodule.\nrestrict_xpaths (str or list) \u2013 is a XPath (or list of XPath\u2019s) which defines\nregions inside the response where links should be extracted from.\nIf given, only the text selected by those XPath will be scanned for\nlinks. See examples below.\ntags (str or list) \u2013 a tag or a list of tags to consider when extracting links.\nDefaults to ('a', 'area').\nattrs (list) \u2013 list of attributes which should be considered when looking\nfor links to extract (only for those tags specified in the tags\nparameter). Defaults to ('href',)\ncanonicalize (boolean) \u2013 canonicalize each extracted url (using\nscrapy.utils.url.canonicalize_url). Defaults to True.\nunique (boolean) \u2013 whether duplicate filtering should be applied to extracted\nlinks.\nprocess_value (callable) \u2013 see process_value argument of\nBaseSgmlLinkExtractor class constructor\n\n\n\nBaseSgmlLinkExtractor\u00b6\n\nclass scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor(tag=\"a\", attr=\"href\", unique=False, process_value=None)\u00b6\nThe purpose of this Link Extractor is only to serve as a base class for the\nSgmlLinkExtractor. You should use that one instead.\nThe constructor arguments are:\nParameters:tag (str or callable) \u2013 either a string (with the name of a tag) or a function that\nreceives a tag name and returns True if links should be extracted from\nthat tag, or False if they shouldn\u2019t. Defaults to 'a'.  request\n(once it\u2019s downloaded) as its first parameter. For more information, see\nPassing additional data to callback functions.\nattr (str or callable) \u2013 either string (with the name of a tag attribute), or a\nfunction that receives an attribute name and returns True if\nlinks should be extracted from it, or False if they shouldn\u2019t.\nDefaults to href.\nunique (boolean) \u2013 is a boolean that specifies if a duplicate filtering should\nbe applied to links extracted.\nprocess_value (callable) \u2013 a function which receives each value extracted from\nthe tag and attributes scanned and can modify the value and return a\nnew one, or return None to ignore the link altogether. If not\ngiven, process_value defaults to lambda x: x.\nFor example, to extract links from this code:\n<a href=\"javascript:goToPage('../other/page.html'); return false\">Link text</a>\n\n\nYou can use the following function in process_value:\ndef process_value(value):\n    m = re.search(\"javascript:goToPage\\('(.*?)'\", value)\n    if m:\n        return m.group(1)\n\n\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/link-extractors.html", "title": ["Link Extractors \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nLogging\u00b6\nScrapy provides a logging facility which can be used through the\nscrapy.log module. The current underlying implementation uses Twisted\nlogging but this may change in the future.\nThe logging service must be explicitly started through the scrapy.log.start() function.\n\nLog levels\u00b6\nScrapy provides 5 logging levels:\nCRITICAL - for critical errors\nERROR - for regular errors\nWARNING - for warning messages\nINFO - for informational messages\nDEBUG - for debugging messages\n\n\nHow to set the log level\u00b6\nYou can set the log level using the \u2013loglevel/-L command line option, or\nusing the LOG_LEVEL setting.\n\n\nHow to log messages\u00b6\nHere\u2019s a quick example of how to log a message using the WARNING level:\nfrom scrapy import log\nlog.msg(\"This is a warning\", level=log.WARNING)\n\n\n\n\nLogging from Spiders\u00b6\nThe recommended way to log from spiders is by using the Spider\nlog() method, which already populates the\nspider argument of the scrapy.log.msg() function. The other arguments\nare passed directly to the msg() function.\n\n\nscrapy.log module\u00b6\n\nscrapy.log.start(logfile=None, loglevel=None, logstdout=None)\u00b6\nStart the logging facility. This must be called before actually logging any\nmessages. Otherwise, messages logged before this call will get lost.\nParameters:logfile (str) \u2013 the file path to use for logging output. If omitted, the\nLOG_FILE setting will be used. If both are None, the log\nwill be sent to standard error.\nloglevel \u2013 the minimum logging level to log. Available values are:\nCRITICAL, ERROR, WARNING, INFO and\nDEBUG.\nlogstdout (boolean) \u2013 if True, all standard output (and error) of your\napplication will be logged instead. For example if you \u201cprint \u2018hello\u2019\u201d\nit will appear in the Scrapy log. If omitted, the LOG_STDOUT\nsetting will be used.\n\n\nscrapy.log.msg(message, level=INFO, spider=None)\u00b6\nLog a message\nParameters:message (str) \u2013 the message to log\nlevel \u2013 the log level for this message. See\nLog levels.\nspider (BaseSpider object) \u2013 the spider to use for logging this message. This parameter\nshould always be used when logging things related to a particular\nspider.\n\n\nscrapy.log.CRITICAL\u00b6\nLog level for critical errors\n\nscrapy.log.ERROR\u00b6\nLog level for errors\n\nscrapy.log.WARNING\u00b6\nLog level for warnings\n\nscrapy.log.INFO\u00b6\nLog level for informational messages (recommended level for production\ndeployments)\n\nscrapy.log.DEBUG\u00b6\nLog level for debugging messages (recommended level for development)\n\n\nLogging settings\u00b6\nThese settings can be used to configure the logging:\nLOG_ENABLED\nLOG_ENCODING\nLOG_FILE\nLOG_LEVEL\nLOG_STDOUT\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/logging.html", "title": ["Logging \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nStats Collection\u00b6\nScrapy provides a convenient facility for collecting stats in the form of\nkey/values, where values are often counters. The facility is called the Stats\nCollector, and can be accessed through the stats\nattribute of the Crawler API, as illustrated by the examples in\nthe Common Stats Collector uses section below.\nHowever, the Stats Collector is always available, so you can always import it\nin your module and use its API (to increment or set new stat keys), regardless\nof whether the stats collection is enabled or not. If it\u2019s disabled, the API\nwill still work but it won\u2019t collect anything. This is aimed at simplifying the\nstats collector usage: you should spend no more than one line of code for\ncollecting stats in your spider, Scrapy extension, or whatever code you\u2019re\nusing the Stats Collector from.\nAnother feature of the Stats Collector is that it\u2019s very efficient (when\nenabled) and extremely efficient (almost unnoticeable) when disabled.\nThe Stats Collector keeps a stats table per open spider which is automatically\nopened when the spider is opened, and closed when the spider is closed.\n\nCommon Stats Collector uses\u00b6\nAccess the stats collector through the stats\nattribute. Here is an example of an extension that access stats:\nclass ExtensionThatAccessStats(object):\n\n    def __init__(self, stats):\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.stats)\n\n\nSet stat value:\nstats.set_value('hostname', socket.gethostname())\n\n\nIncrement stat value:\nstats.inc_value('pages_crawled')\n\n\nSet stat value only if greater than previous:\nstats.max_value('max_items_scraped', value)\n\n\nSet stat value only if lower than previous:\nstats.min_value('min_free_memory_percent', value)\n\n\nGet stat value:\n>>> stats.get_value('pages_crawled')\n8\n\n\nGet all stats:\n>>> stats.get_stats()\n{'pages_crawled': 1238, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}\n\n\n\n\nAvailable Stats Collectors\u00b6\nBesides the basic StatsCollector there are other Stats Collectors\navailable in Scrapy which extend the basic Stats Collector. You can select\nwhich Stats Collector to use through the STATS_CLASS setting. The\ndefault Stats Collector used is the MemoryStatsCollector.\n\nMemoryStatsCollector\u00b6\n\nclass scrapy.statscol.MemoryStatsCollector\u00b6\nA simple stats collector that keeps the stats of the last scraping run (for\neach spider) in memory, after they\u2019re closed. The stats can be accessed\nthrough the spider_stats attribute, which is a dict keyed by spider\ndomain name.\nThis is the default Stats Collector used in Scrapy.\n\nspider_stats\u00b6\nA dict of dicts (keyed by spider name) containing the stats of the last\nscraping run for each spider.\n\n\nDummyStatsCollector\u00b6\n\nclass scrapy.statscol.DummyStatsCollector\u00b6\nA Stats collector which does nothing but is very efficient (because it does\nnothing). This stats collector can be set via the STATS_CLASS\nsetting, to disable stats collect in order to improve performance. However,\nthe performance penalty of stats collection is usually marginal compared to\nother Scrapy workload like parsing pages.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/stats.html", "title": ["Stats Collection \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSending e-mail\u00b6\nAlthough Python makes sending e-mails relatively easy via the smtplib\nlibrary, Scrapy provides its own facility for sending e-mails which is very\neasy to use and it\u2019s implemented using Twisted non-blocking IO, to avoid\ninterfering with the non-blocking IO of the crawler. It also provides a\nsimple API for sending attachments and it\u2019s very easy to configure, with a few\nsettings.\n\nQuick example\u00b6\nThere are two ways to instantiate the mail sender. You can instantiate it using\nthe standard constructor:\nfrom scrapy.mail import MailSender\nmailer = MailSender()\n\n\nOr you can instantiate it passing a Scrapy settings object, which will respect\nthe settings:\nmailer = MailSender.from_settings(settings)\n\n\nAnd here is how to use it to send an e-mail (without attachments):\nmailer.send(to=[\"someone@example.com\"], subject=\"Some subject\", body=\"Some body\", cc=[\"another@example.com\"])\n\n\n\n\nMailSender class reference\u00b6\nMailSender is the preferred class to use for sending emails from Scrapy, as it\nuses Twisted non-blocking IO, like the rest of the framework.\n\nclass scrapy.mail.MailSender(smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, smtpport=None)\u00b6\nParameters:smtphost (str) \u2013 the SMTP host to use for sending the emails. If omitted, the\nMAIL_HOST setting will be used.\nmailfrom (str) \u2013 the address used to send emails (in the From: header).\nIf omitted, the MAIL_FROM setting will be used.\nsmtpuser \u2013 the SMTP user. If omitted, the MAIL_USER\nsetting will be used. If not given, no SMTP authentication will be\nperformed.\nsmtppass (str) \u2013 the SMTP pass for authentication.\nsmtpport (int) \u2013 the SMTP port to connect to\n\n\nclassmethod from_settings(settings)\u00b6\nInstantiate using a Scrapy settings object, which will respect\nthese Scrapy settings.\nParameters:settings (scrapy.settings.Settings object) \u2013 the e-mail recipients\n\nsend(to, subject, body, cc=None, attachs=())\u00b6\nSend email to the given recipients.\nParameters:to (list) \u2013 the e-mail recipients\nsubject (str) \u2013 the subject of the e-mail\ncc (list) \u2013 the e-mails to CC\nbody (str) \u2013 the e-mail body\nattachs (iterable) \u2013 an iterable of tuples (attach_name, mimetype,\nfile_object) where  attach_name is a string with the name that will\nappear on the e-mail\u2019s attachment, mimetype is the mimetype of the\nattachment and file_object is a readable file object with the\ncontents of the attachment\n\n\n\nMail settings\u00b6\nThese settings define the default constructor values of the MailSender\nclass, and can be used to configure e-mail notifications in your project without\nwriting any code (for those extensions and code that uses MailSender).\n\nMAIL_FROM\u00b6\nDefault: 'scrapy@localhost'\nSender email to use (From: header) for sending emails.\n\n\nMAIL_HOST\u00b6\nDefault: 'localhost'\nSMTP host to use for sending emails.\n\n\nMAIL_PORT\u00b6\nDefault: 25\nSMTP port to use for sending emails.\n\n\nMAIL_USER\u00b6\nDefault: None\nUser to use for SMTP authentication. If disabled no SMTP authentication will be\nperformed.\n\n\nMAIL_PASS\u00b6\nDefault: None\nPassword to use for SMTP authentication, along with MAIL_USER.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/email.html", "title": ["Sending e-mail \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nTelnet Console\u00b6\nScrapy comes with a built-in telnet console for inspecting and controlling a\nScrapy running process. The telnet console is just a regular python shell\nrunning inside the Scrapy process, so you can do literally anything from it.\nThe telnet console is a built-in Scrapy extension which comes enabled by default, but you can also\ndisable it if you want. For more information about the extension itself see\nTelnet console extension.\n\nHow to access the telnet console\u00b6\nThe telnet console listens in the TCP port defined in the\nTELNETCONSOLE_PORT setting, which defaults to 6023. To access\nthe console you need to type:\ntelnet localhost 6023\n>>>\n\n\nYou need the telnet program which comes installed by default in Windows, and\nmost Linux distros.\n\n\nAvailable variables in the telnet console\u00b6\nThe telnet console is like a regular Python shell running inside the Scrapy\nprocess, so you can do anything from it including importing new modules, etc.\nHowever, the telnet console comes with some default variables defined for\nconvenience:\nShortcut\nDescription\ncrawler\nthe Scrapy Crawler (scrapy.crawler.Crawler object)\nengine\nCrawler.engine attribute\nspider\nthe active spider\nslot\nthe engine slot\nextensions\nthe Extension Manager (Crawler.extensions attribute)\nstats\nthe Stats Collector (Crawler.stats attribute)\nsettings\nthe Scrapy settings object (Crawler.settings attribute)\nest\nprint a report of the engine status\nprefs\nfor memory debugging (see Debugging memory leaks)\np\na shortcut to the pprint.pprint function\nhpy\nfor memory debugging (see Debugging memory leaks)\n\n\nTelnet console usage examples\u00b6\nHere are some example tasks you can do with the telnet console:\n\nView engine status\u00b6\nYou can use the est() method of the Scrapy engine to quickly show its state\nusing the telnet console:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 21.3188259602\nengine.is_idle()                                : False\nengine.has_capacity()                           : True\nengine.scheduler.is_idle()                      : False\nlen(engine.scheduler.pending_requests)          : 1\nengine.downloader.is_idle()                     : False\nlen(engine.downloader.slots)                    : 1\nengine.scraper.is_idle()                        : False\nlen(engine.scraper.slots)                       : 1\n\nSpider: <GayotSpider 'gayotcom' at 0x2dc2b10>\n  engine.spider_is_idle(spider)                      : False\n  engine.slots[spider].closing                       : False\n  len(engine.scheduler.pending_requests[spider])     : 11504\n  len(engine.downloader.slots[spider].queue)         : 9\n  len(engine.downloader.slots[spider].active)        : 17\n  len(engine.downloader.slots[spider].transferring)  : 8\n  engine.downloader.slots[spider].lastseen           : 1311311093.61\n  len(engine.scraper.slots[spider].queue)            : 0\n  len(engine.scraper.slots[spider].active)           : 0\n  engine.scraper.slots[spider].active_size           : 0\n  engine.scraper.slots[spider].itemproc_size         : 0\n  engine.scraper.slots[spider].needs_backout()       : False\n\n\n\n\nPause, resume and stop the Scrapy engine\u00b6\nTo pause:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\nTo resume:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\nTo stop:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet Console signals\u00b6\n\nscrapy.telnet.update_telnet_vars(telnet_vars)\u00b6\nSent just before the telnet console is opened. You can hook up to this\nsignal to add, remove or update the variables that will be available in the\ntelnet local namespace. In order to do that, you need to update the\ntelnet_vars dict in your handler.\nParameters:telnet_vars (dict) \u2013 the dict of telnet variables\n\n\nTelnet settings\u00b6\nThese are the settings that control the telnet console\u2019s behaviour:\n\nTELNETCONSOLE_PORT\u00b6\nDefault: [6023, 6073]\nThe port range to use for the telnet console. If set to None or 0, a\ndynamically assigned port is used.\n\n\nTELNETCONSOLE_HOST\u00b6\nDefault: '0.0.0.0'\nThe interface the telnet console should listen on\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/telnetconsole.html", "title": ["Telnet Console \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nWeb Service\u00b6\nScrapy comes with a built-in web service for monitoring and controlling a\nrunning crawler. The service exposes most resources using the JSON-RPC 2.0\nprotocol, but there are also other (read-only) resources which just output JSON\ndata.\nProvides an extensible web service for managing a Scrapy process. It\u2019s enabled\nby the WEBSERVICE_ENABLED setting. The web server will listen in the\nport specified in WEBSERVICE_PORT, and will log to the file\nspecified in WEBSERVICE_LOGFILE.\nThe web service is a built-in Scrapy extension\nwhich comes enabled by default, but you can also disable it if you\u2019re running\ntight on memory.\n\nWeb service resources\u00b6\nThe web service contains several resources, defined in the\nWEBSERVICE_RESOURCES setting. Each resource provides a different\nfunctionality. See Available JSON-RPC resources for a list of\nresources available by default.\nAlthough you can implement your own resources using any protocol, there are\ntwo kinds of resources bundled with Scrapy:\nSimple JSON resources - which are read-only and just output JSON data\nJSON-RPC resources - which provide direct access to certain Scrapy objects\nusing the JSON-RPC 2.0 protocol\n\nAvailable JSON-RPC resources\u00b6\nThese are the JSON-RPC resources available by default in Scrapy:\n\nCrawler JSON-RPC resource\u00b6\n\nclass scrapy.contrib.webservice.crawler.CrawlerResource\u00b6\nProvides access to the main Crawler object that controls the Scrapy\nprocess.\nAvailable by default at: http://localhost:6080/crawler\n\n\nStats Collector JSON-RPC resource\u00b6\n\nclass scrapy.contrib.webservice.stats.StatsResource\u00b6\nProvides access to the Stats Collector used by the crawler.\nAvailable by default at: http://localhost:6080/stats\n\n\nSpider Manager JSON-RPC resource\u00b6\nYou can access the spider manager JSON-RPC resource through the\nCrawler JSON-RPC resource at: http://localhost:6080/crawler/spiders\n\n\nExtension Manager JSON-RPC resource\u00b6\nYou can access the extension manager JSON-RPC resource through the\nCrawler JSON-RPC resource at: http://localhost:6080/crawler/spiders\n\n\n\nAvailable JSON resources\u00b6\nThese are the JSON resources available by default:\n\nEngine status JSON resource\u00b6\n\nclass scrapy.contrib.webservice.enginestatus.EngineStatusResource\u00b6\nProvides access to engine status metrics.\nAvailable by default at: http://localhost:6080/enginestatus\n\n\n\n\nWeb service settings\u00b6\nThese are the settings that control the web service behaviour:\n\nWEBSERVICE_ENABLED\u00b6\nDefault: True\nA boolean which specifies if the web service will be enabled (provided its\nextension is also enabled).\n\n\nWEBSERVICE_LOGFILE\u00b6\nDefault: None\nA file to use for logging HTTP requests made to the web service. If unset web\nthe log is sent to standard scrapy log.\n\n\nWEBSERVICE_PORT\u00b6\nDefault: [6080, 7030]\nThe port range to use for the web service. If set to None or 0, a\ndynamically assigned port is used.\n\n\nWEBSERVICE_HOST\u00b6\nDefault: '0.0.0.0'\nThe interface the web service should listen on\n\n\nWEBSERVICE_RESOURCES\u00b6\nDefault: {}\nThe list of web service resources enabled for your project. See\nWeb service resources. These are added to the ones available by\ndefault in Scrapy, defined in the WEBSERVICE_RESOURCES_BASE setting.\n\n\nWEBSERVICE_RESOURCES_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n    'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n    'scrapy.contrib.webservice.stats.StatsResource': 1,\n}\n\n\nThe list of web service resources available by default in Scrapy. You shouldn\u2019t\nchange this setting in your project, change WEBSERVICE_RESOURCES\ninstead. If you want to disable some resource set its value to None in\nWEBSERVICE_RESOURCES.\n\n\n\nWriting a web service resource\u00b6\nWeb service resources are implemented using the Twisted Web API. See this\nTwisted Web guide for more information on Twisted web and Twisted web\nresources.\nTo write a web service resource you should subclass the JsonResource or\nJsonRpcResource classes and implement the renderGET method.\n\nclass scrapy.webservice.JsonResource\u00b6\nA subclass of twisted.web.resource.Resource that implements a JSON web\nservice resource. See\n\nws_name\u00b6\nThe name by which the Scrapy web service will known this resource, and\nalso the path where this resource will listen. For example, assuming\nScrapy web service is listening on http://localhost:6080/ and the\nws_name is 'resource1' the URL for that resource will be:\n\nhttp://localhost:6080/resource1/\n\nclass scrapy.webservice.JsonRpcResource(crawler, target=None)\u00b6\nThis is a subclass of JsonResource for implementing JSON-RPC\nresources. JSON-RPC resources wrap Python (Scrapy) objects around a\nJSON-RPC API. The resource wrapped must be returned by the\nget_target() method, which returns the target passed in the\nconstructor by default\n\nget_target()\u00b6\nReturn the object wrapped by this JSON-RPC resource. By default, it\nreturns the object passed on the constructor.\n\n\nExamples of web service resources\u00b6\n\nStatsResource (JSON-RPC resource)\u00b6\nfrom scrapy.webservice import JsonRpcResource\n\nclass StatsResource(JsonRpcResource):\n\n    ws_name = 'stats'\n\n    def __init__(self, crawler):\n        JsonRpcResource.__init__(self, crawler, crawler.stats)\n\n\n\n\nEngineStatusResource (JSON resource)\u00b6\nfrom scrapy.webservice import JsonResource\nfrom scrapy.utils.engine import get_engine_status\n\nclass EngineStatusResource(JsonResource):\n\n    ws_name = 'enginestatus'\n\n    def __init__(self, crawler, spider_name=None):\n        JsonResource.__init__(self, crawler)\n        self._spider_name = spider_name\n        self.isLeaf = spider_name is not None\n\n    def render_GET(self, txrequest):\n        status = get_engine_status(self.crawler.engine)\n        if self._spider_name is None:\n            return status\n        for sp, st in status['spiders'].items():\n            if sp.name == self._spider_name:\n                return st\n\n    def getChild(self, name, txrequest):\n        return EngineStatusResource(name, self.crawler)\n\n\n\n\n\nExample of web service client\u00b6\n\nscrapy-ws.py script\u00b6\n#!/usr/bin/env python\n\"\"\"\nExample script to control a Scrapy server using its JSON-RPC web service.\n\nIt only provides a reduced functionality as its main purpose is to illustrate\nhow to write a web service client. Feel free to improve or write you own.\n\nAlso, keep in mind that the JSON-RPC API is not stable. The recommended way for\ncontrolling a Scrapy server is through the execution queue (see the \"queue\"\ncommand).\n\n\"\"\"\n\nimport sys, optparse, urllib, json\nfrom urlparse import urljoin\n\nfrom scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError\n\ndef get_commands():\n    return {\n        'help': cmd_help,\n        'stop': cmd_stop,\n        'list-available': cmd_list_available,\n        'list-running': cmd_list_running,\n        'list-resources': cmd_list_resources,\n        'get-global-stats': cmd_get_global_stats,\n        'get-spider-stats': cmd_get_spider_stats,\n    }\n\ndef cmd_help(args, opts):\n    \"\"\"help - list available commands\"\"\"\n    print \"Available commands:\"\n    for _, func in sorted(get_commands().items()):\n        print \"  \", func.__doc__\n\ndef cmd_stop(args, opts):\n    \"\"\"stop <spider> - stop a running spider\"\"\"\n    jsonrpc_call(opts, 'crawler/engine', 'close_spider', args[0])\n\ndef cmd_list_running(args, opts):\n    \"\"\"list-running - list running spiders\"\"\"\n    for x in json_get(opts, 'crawler/engine/open_spiders'):\n        print x\n\ndef cmd_list_available(args, opts):\n    \"\"\"list-available - list name of available spiders\"\"\"\n    for x in jsonrpc_call(opts, 'crawler/spiders', 'list'):\n        print x\n\ndef cmd_list_resources(args, opts):\n    \"\"\"list-resources - list available web service resources\"\"\"\n    for x in json_get(opts, '')['resources']:\n        print x\n\ndef cmd_get_spider_stats(args, opts):\n    \"\"\"get-spider-stats <spider> - get stats of a running spider\"\"\"\n    stats = jsonrpc_call(opts, 'stats', 'get_stats', args[0])\n    for name, value in stats.items():\n        print \"%-40s %s\" % (name, value)\n\ndef cmd_get_global_stats(args, opts):\n    \"\"\"get-global-stats - get global stats\"\"\"\n    stats = jsonrpc_call(opts, 'stats', 'get_stats')\n    for name, value in stats.items():\n        print \"%-40s %s\" % (name, value)\n\ndef get_wsurl(opts, path):\n    return urljoin(\"http://%s:%s/\"% (opts.host, opts.port), path)\n\ndef jsonrpc_call(opts, path, method, *args, **kwargs):\n    url = get_wsurl(opts, path)\n    return jsonrpc_client_call(url, method, *args, **kwargs)\n\ndef json_get(opts, path):\n    url = get_wsurl(opts, path)\n    return json.loads(urllib.urlopen(url).read())\n\ndef parse_opts():\n    usage = \"%prog [options] <command> [arg] ...\"\n    description = \"Scrapy web service control script. Use '%prog help' \" \\\n        \"to see the list of available commands.\"\n    op = optparse.OptionParser(usage=usage, description=description)\n    op.add_option(\"-H\", dest=\"host\", default=\"localhost\", \\\n        help=\"Scrapy host to connect to\")\n    op.add_option(\"-P\", dest=\"port\", type=\"int\", default=6080, \\\n        help=\"Scrapy port to connect to\")\n    opts, args = op.parse_args()\n    if not args:\n        op.print_help()\n        sys.exit(2)\n    cmdname, cmdargs, opts = args[0], args[1:], opts\n    commands = get_commands()\n    if cmdname not in commands:\n        sys.stderr.write(\"Unknown command: %s\\n\\n\" % cmdname)\n        cmd_help(None, None)\n        sys.exit(1)\n    return commands[cmdname], cmdargs, opts\n\ndef main():\n    cmd, args, opts = parse_opts()\n    try:\n        cmd(args, opts)\n    except IndexError:\n        print cmd.__doc__\n    except JsonRpcError, e:\n        print str(e)\n        if e.data:\n            print \"Server Traceback below:\"\n            print e.data\n\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/webservice.html", "title": ["Web Service \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nFrequently Asked Questions\u00b6\n\nHow does Scrapy compare to BeautifulSoup or lxml?\u00b6\nBeautifulSoup and lxml are libraries for parsing HTML and XML. Scrapy is\nan application framework for writing web spiders that crawl web sites and\nextract data from them.\nScrapy provides a built-in mechanism for extracting data (called\nselectors) but you can easily use BeautifulSoup\n(or lxml) instead, if you feel more comfortable working with them. After\nall, they\u2019re just parsing libraries which can be imported and used from any\nPython code.\nIn other words, comparing BeautifulSoup (or lxml) to Scrapy is like\ncomparing jinja2 to Django.\n\n\nWhat Python versions does Scrapy support?\u00b6\nScrapy runs in Python 2.6 and 2.7.\n\n\nDoes Scrapy work with Python 3.0?\u00b6\nNo, and there are no plans to port Scrapy to Python 3.0 yet. At the moment,\nScrapy works with Python 2.6 and 2.7.\n\nSee also\nWhat Python versions does Scrapy support?.\n\n\n\nDid Scrapy \u201csteal\u201d X from Django?\u00b6\nProbably, but we don\u2019t like that word. We think Django is a great open source\nproject and an example to follow, so we\u2019ve used it as an inspiration for\nScrapy.\nWe believe that, if something is already done well, there\u2019s no need to reinvent\nit. This concept, besides being one of the foundations for open source and free\nsoftware, not only applies to software but also to documentation, procedures,\npolicies, etc. So, instead of going through each problem ourselves, we choose\nto copy ideas from those projects that have already solved them properly, and\nfocus on the real problems we need to solve.\nWe\u2019d be proud if Scrapy serves as an inspiration for other projects. Feel free\nto steal from us!\n\n\nDoes Scrapy work with HTTP proxies?\u00b6\nYes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP\nProxy downloader middleware. See\nHttpProxyMiddleware.\n\n\nScrapy crashes with: ImportError: No module named win32api\u00b6\nYou need to install pywin32 because of this Twisted bug.\n\n\nHow can I simulate a user login in my spider?\u00b6\nSee Using FormRequest.from_response() to simulate a user login.\n\n\nDoes Scrapy crawl in breath-first or depth-first order?\u00b6\nBy default, Scrapy uses a LIFO queue for storing pending requests, which\nbasically means that it crawls in DFO order. This order is more convenient\nin most cases. If you do want to crawl in true BFO order, you can do it by\nsetting the following settings:\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'\n\n\n\n\nMy Scrapy crawler has memory leaks. What can I do?\u00b6\nSee Debugging memory leaks.\nAlso, Python has a builtin memory leak issue which is described in\nLeaks without leaks.\n\n\nHow can I make Scrapy consume less memory?\u00b6\nSee previous question.\n\n\nCan I use Basic HTTP Authentication in my spiders?\u00b6\nYes, see HttpAuthMiddleware.\n\n\nWhy does Scrapy download pages in English instead of my native language?\u00b6\nTry changing the default Accept-Language request header by overriding the\nDEFAULT_REQUEST_HEADERS setting.\n\n\nWhere can I find some example Scrapy projects?\u00b6\nSee Examples.\n\n\nCan I run a spider without creating a project?\u00b6\nYes. You can use the runspider command. For example, if you have a\nspider written in a my_spider.py file you can run it with:\nscrapy runspider my_spider.py\n\nSee runspider command for more info.\n\n\nI get \u201cFiltered offsite request\u201d messages. How can I fix them?\u00b6\nThose messages (logged with DEBUG level) don\u2019t necessarily mean there is a\nproblem, so you may not need to fix them.\nThose message are thrown by the Offsite Spider Middleware, which is a spider\nmiddleware (enabled by default) whose purpose is to filter out requests to\ndomains outside the ones covered by the spider.\nFor more info see:\nOffsiteMiddleware.\n\n\nWhat is the recommended way to deploy a Scrapy crawler in production?\u00b6\nSee Scrapy Service (scrapyd).\n\n\nCan I use JSON for large exports?\u00b6\nIt\u2019ll depend on how large your output is. See this warning in JsonItemExporter\ndocumentation.\n\n\nCan I return (Twisted) deferreds from signal handlers?\u00b6\nSome signals support returning deferreds from their handlers, others don\u2019t. See\nthe Built-in signals reference to know which ones.\n\n\nWhat does the response status code 999 means?\u00b6\n999 is a custom reponse status code used by Yahoo sites to throttle requests.\nTry slowing down the crawling speed by using a download delay of 2 (or\nhigher) in your spider:\nclass MySpider(CrawlSpider):\n\n    name = 'myspider'\n\n    DOWNLOAD_DELAY = 2\n\n    # [ ... rest of the spider code ... ]\n\n\nOr by setting a global download delay in your project with the\nDOWNLOAD_DELAY setting.\n\n\nCan I call pdb.set_trace() from my spiders to debug them?\u00b6\nYes, but you can also use the Scrapy shell which allows you too quickly analyze\n(and even modify) the response being processed by your spider, which is, quite\noften, more useful than plain old pdb.set_trace().\nFor more info see Invoking the shell from spiders to inspect responses.\n\n\nSimplest way to dump all my scraped items into a JSON/CSV/XML file?\u00b6\nTo dump into a JSON file:\nscrapy crawl myspider -o items.json -t json\n\nTo dump into a CSV file:\nscrapy crawl myspider -o items.csv -t csv\n\nTo dump into a XML file:\nscrapy crawl myspider -o items.xml -t xml\n\nFor more information see Feed exports\n\n\nWhat\u2019s this huge cryptic __VIEWSTATE parameter used in some forms?\u00b6\nThe __VIEWSTATE parameter is used in sites built with ASP.NET/VB.NET. For\nmore info on how it works see this page. Also, here\u2019s an example spider\nwhich scrapes one of these sites.\n\n\nWhat\u2019s the best way to parse big XML/CSV data feeds?\u00b6\nParsing big feeds with XPath selectors can be problematic since they need to\nbuild the DOM of the entire feed in memory, and this can be quite slow and\nconsume a lot of memory.\nIn order to avoid parsing all the entire feed at once in memory, you can use\nthe functions xmliter and csviter from scrapy.utils.iterators\nmodule. In fact, this is what the feed spiders (see Spiders) use\nunder the cover.\n\n\nDoes Scrapy manage cookies automatically?\u00b6\nYes, Scrapy receives and keeps track of cookies sent by servers, and sends them\nback on subsequent requests, like any regular web browser does.\nFor more info see Requests and Responses and CookiesMiddleware.\n\n\nHow can I see the cookies being sent and received from Scrapy?\u00b6\nEnable the COOKIES_DEBUG setting.\n\n\nHow can I instruct a spider to stop itself?\u00b6\nRaise the CloseSpider exception from a callback. For\nmore info see: CloseSpider.\n\n\nHow can I prevent my Scrapy bot from getting banned?\u00b6\nSee Avoiding getting banned.\n\n\nShould I use spider arguments or settings to configure my spider?\u00b6\nBoth spider arguments and settings\ncan be used to configure your spider. There is no strict rule that mandates to\nuse one or the other, but settings are more suited for parameters that, once\nset, don\u2019t change much, while spider arguments are meant to change more often,\neven on each spider run and sometimes are required for the spider to run at all\n(for example, to set the start url of a spider).\nTo illustrate with an example, assuming you have a spider that needs to log\ninto a site to scrape data, and you only want to scrape data from a certain\nsection of the site (which varies each time). In that case, the credentials to\nlog in would be settings, while the url of the section to scrape would be a\nspider argument.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/faq.html", "title": ["Frequently Asked Questions \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDebugging Spiders\u00b6\nThis document explains the most common techniques for debugging spiders.\nConsider the following scrapy spider below:\nclass MySpider(BaseSpider):\n    name = 'myspider'\n    start_urls = (\n        'http://example.com/page1',\n        'http://example.com/page2',\n        )\n\n    def parse(self, response):\n        # collect `item_urls`\n        for item_url in item_urls:\n            yield Request(url=item_url, callback=self.parse_item)\n\n    def parse_item(self, response):\n        item = MyItem()\n        # populate `item` fields\n        yield Request(url=item_details_url, meta={'item': item},\n            callback=self.parse_details)\n\n    def parse_details(self, response):\n        item = response.meta['item']\n        # populate more `item` fields\n        return item\n\n\nBasically this is a simple spider which parses two pages of items (the\nstart_urls). Items also have a details page with additional information, so we\nuse the meta functionality of Request to pass a\npartially populated item.\n\nParse Command\u00b6\nThe most basic way of checking the output of your spider is to use the\nparse command. It allows to check the behaviour of different parts\nof the spider at the method level. It has the advantage of being flexible and\nsimple to use, but does not allow debugging code inside a method.\nIn order to see the item scraped from a specific url:\n$ scrapy parse --spider=myspider -c parse_item -d 2 <item_url>\n[ ... scrapy log lines crawling example.com spider ... ]\n\n>>> STATUS DEPTH LEVEL 2 <<<\n# Scraped Items  ------------------------------------------------------------\n[{'url': <item_url>}]\n\n# Requests  -----------------------------------------------------------------\n[]\n\nUsing the --verbose or -v option we can see the status at each depth level:\n$ scrapy parse --spider=myspider -c parse_item -d 2 -v <item_url>\n[ ... scrapy log lines crawling example.com spider ... ]\n\n>>> DEPTH LEVEL: 1 <<<\n# Scraped Items  ------------------------------------------------------------\n[]\n\n# Requests  -----------------------------------------------------------------\n[<GET item_details_url>]\n\n\n>>> DEPTH LEVEL: 2 <<<\n# Scraped Items  ------------------------------------------------------------\n[{'url': <item_url>}]\n\n# Requests  -----------------------------------------------------------------\n[]\n\nChecking items scraped from a single start_url, can also be easily achieved\nusing:\n$ scrapy parse --spider=myspider -d 3 'http://example.com/page1'\n\n\n\nScrapy Shell\u00b6\nWhile the parse command is very useful for checking behaviour of a\nspider, it is of little help to check what happens inside a callback, besides\nshowing the response received and the output. How to debug the situation when\nparse_details sometimes receives no item?\nFortunately, the shell is your bread and butter in this case (see\nInvoking the shell from spiders to inspect responses):\nfrom scrapy.shell import inspect_response\n\ndef parse_details(self, response):\n    item = response.meta.get('item', None)\n    if item:\n        # populate more `item` fields\n        return item\n    else:\n        inspect_response(response, self)\n\n\nSee also: Invoking the shell from spiders to inspect responses.\n\n\nOpen in browser\u00b6\nSometimes you just want to see how a certain response looks in a browser, you\ncan use the open_in_browser function for that. Here is an example of how\nyou would use it:\nfrom scrapy.utils.response import open_in_browser\n\ndef parse_details(self, response):\n    if \"item name\" not in response.body:\n        open_in_browser(response)\n\n\nopen_in_browser will open a browser with the response received by Scrapy at\nthat point, adjusting the base tag so that images and styles are displayed\nproperly.\n\n\nLogging\u00b6\nLogging is another useful option for getting information about your spider run.\nAlthough not as convenient, it comes with the advantage that the logs will be\navailable in all future runs should they be necessary again:\nfrom scrapy import log\n\ndef parse_details(self, response):\n    item = response.meta.get('item', None)\n    if item:\n        # populate more `item` fields\n        return item\n    else:\n        self.log('No item received for %s' % response.url,\n            level=log.WARNING)\n\n\nFor more information, check the Logging section.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/debug.html", "title": ["Debugging Spiders \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSpiders Contracts\u00b6\n\nNew in version 0.15.\n\nNote\nThis is a new feature (introduced in Scrapy 0.15) and may be subject\nto minor functionality/API updates. Check the release notes to\nbe notified of updates.\n\nTesting spiders can get particularly annoying and while nothing prevents you\nfrom writing unit tests the task gets cumbersome quickly. Scrapy offers an\nintegrated way of testing your spiders by the means of contracts.\nThis allows you to test each callback of your spider by hardcoding a sample url\nand check various constraints for how the callback processes the response. Each\ncontract is prefixed with an @ and included in the docstring. See the\nfollowing example:\ndef parse(self, response):\n    \"\"\" This function parses a sample response. Some contracts are mingled\n    with this docstring.\n\n    @url http://www.amazon.com/s?field-keywords=selfish+gene\n    @returns items 1 16\n    @returns requests 0 0\n    @scrapes Title Author Year Price\n    \"\"\"\n\n\nThis callback is tested using three built-in contracts:\n\nclass scrapy.contracts.default.UrlContract\u00b6\nThis contract (@url) sets the sample url used when checking other\ncontract conditions for this spider. This contract is mandatory. All\ncallbacks lacking this contract are ignored when running the checks:\n@url url\n\n\nclass scrapy.contracts.default.ReturnsContract\u00b6\nThis contract (@returns) sets lower and upper bounds for the items and\nrequests returned by the spider. The upper bound is optional:\n@returns item(s)|request(s) [min [max]]\n\n\nclass scrapy.contracts.default.ScrapesContract\u00b6\nThis contract (@scrapes) checks that all the items returned by the\ncallback have the specified fields:\n@scrapes field_1 field_2 ...\n\nUse the check command to run the contract checks.\n\nCustom Contracts\u00b6\nIf you find you need more power than the built-in scrapy contracts you can\ncreate and load your own contracts in the project by using the\nSPIDER_CONTRACTS setting:\nSPIDER_CONTRACTS = {\n    'myproject.contracts.ResponseCheck': 10,\n    'myproject.contracts.ItemValidate': 10,\n}\n\n\nEach contract must inherit from scrapy.contracts.Contract and can\noverride three methods:\n\nclass scrapy.contracts.Contract(method, *args)\u00b6\nParameters:method (function) \u2013 callback function to which the contract is associated\nargs (list) \u2013 list of arguments passed into the docstring (whitespace\nseparated)\n\n\nadjust_request_args(args)\u00b6\nThis receives a dict as an argument containing default arguments\nfor Request object. Must return the same or a\nmodified version of it.\n\npre_process(response)\u00b6\nThis allows hooking in various checks on the response received from the\nsample request, before it\u2019s being passed to the callback.\n\npost_process(output)\u00b6\nThis allows processing the output of the callback. Iterators are\nconverted listified before being passed to this hook.\nHere is a demo contract which checks the presence of a custom header in the\nresponse received. Raise scrapy.exceptions.ContractFail in order to\nget the failures pretty printed:\nfrom scrapy.contracts import Contract\nfrom scrapy.exceptions import ContractFail\n\nclass HasHeaderContract(Contract):\n    \"\"\" Demo contract which checks the presence of a custom header\n        @has_header X-CustomHeader\n    \"\"\"\n\n    name = 'has_header'\n\n    def pre_process(self, response):\n        for header in self.args:\n            if header not in response.headers:\n                raise ContractFail('X-CustomHeader not present')\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/contracts.html", "title": ["Spiders Contracts \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nCommon Practices\u00b6\nThis section documents common practices when using Scrapy. These are things\nthat cover many topics and don\u2019t often fall into any other specific section.\n\nRun Scrapy from a script\u00b6\nYou can use the API to run Scrapy from a script, instead of\nthe typical way of running Scrapy via scrapy crawl.\nWhat follows is a working example of how to do that, using the testspiders\nproject as example. Remember that Scrapy is built on top of the Twisted\nasynchronous networking library, so you need run it inside the Twisted reactor.\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\n\n\nSee also\nTwisted Reactor Overview.\n\n\n\nRunning multiple spiders in the same process\u00b6\nBy default, Scrapy runs a single spider per process when you run scrapy\ncrawl. However, Scrapy supports running multiple spiders per process using\nthe internal API.\nHere is an example, using the testspiders project:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\ndef setup_crawler(domain):\n    spider = FollowAllSpider(domain=domain)\n    crawler = Crawler(Settings())\n    crawler.configure()\n    crawler.crawl(spider)\n    crawler.start()\n\nfor domain in ['scrapinghub.com', 'insophia.com']:\n    setup_crawler(domain)\nlog.start()\nreactor.run()\n\n\n\nSee also\nRun Scrapy from a script.\n\n\n\nDistributed crawls\u00b6\nScrapy doesn\u2019t provide any built-in facility for running crawls in a distribute\n(multi-server) manner. However, there are some ways to distribute crawls, which\nvary depending on how you plan to distribute them.\nIf you have many spiders, the obvious way to distribute the load is to setup\nmany Scrapyd instances and distribute spider runs among those.\nIf you instead want to run a single (big) spider through many machines, what\nyou usually do is partition the urls to crawl and send them to each separate\nspider. Here is a concrete example:\nFirst, you prepare the list of urls to crawl and put them into separate\nfiles/urls:\nhttp://somedomain.com/urls-to-crawl/spider1/part1.list\nhttp://somedomain.com/urls-to-crawl/spider1/part2.list\nhttp://somedomain.com/urls-to-crawl/spider1/part3.list\n\nThen you fire a spider run on 3 different Scrapyd servers. The spider would\nreceive a (spider) argument part with the number of the partition to\ncrawl:\ncurl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1\ncurl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2\ncurl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3\n\n\n\nAvoiding getting banned\u00b6\nSome websites implement certain measures to prevent bots from crawling them,\nwith varying degrees of sophistication. Getting around those measures can be\ndifficult and tricky, and may sometimes require special infrastructure. Please\nconsider contacting commercial support if in doubt.\nHere are some tips to keep in mind when dealing with these kind of sites:\nrotate your user agent from a pool of well-known ones from browsers (google\naround to get a list of them)\ndisable cookies (see COOKIES_ENABLED) as some sites may use\ncookies to spot bot behaviour\nuse download delays (2 or higher). See DOWNLOAD_DELAY setting.\nif possible, use Google cache to fetch pages, instead of hitting the sites\ndirectly\nuse a pool of rotating IPs. For example, the free Tor project or paid\nservices like ProxyMesh\nIf you are still unable to prevent your bot getting banned, consider contacting\ncommercial support.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/practices.html", "title": ["Common Practices \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nBroad Crawls\u00b6\nScrapy defaults are optimized for crawling specific sites. These sites are\noften handled by a single Scrapy spider, although this is not necessary or\nrequired (for example, there are generic spiders that handle any given site\nthrown at them).\nIn addition to this \u201cfocused crawl\u201d, there is another common type of crawling\nwhich covers a large (potentially unlimited) number of domains, and is only\nlimited by time or other arbitrary constraint, rather than stopping when the\ndomain was crawled to completion or when there are no more requests to perform.\nThese are called \u201cbroad crawls\u201d and is the typical crawlers employed by search\nengines.\nThese are some common properties often found in broad crawls:\nthey crawl many domains (often, unbounded) instead of a specific set of sites\nthey don\u2019t necessarily crawl domains to completion, because it would\nimpractical (or impossible) to do so, and instead limit the crawl by time or\nnumber of pages crawled\nthey are simpler in logic (as opposed to very complex spiders with many\nextraction rules) because data is often post-processed in a separate stage\nthey crawl many domains concurrently, which allows them to achieve faster\ncrawl speeds by not being limited by any particular site constraint (each site\nis crawled slowly to respect politeness, but many sites are crawled in\nparallel)\nAs said above, Scrapy default settings are optimized for focused crawls, not\nbroad crawls. However, due to its asynchronous architecture, Scrapy is very\nwell suited for performing fast broad crawls. This page summarize some things\nyou need to keep in mind when using Scrapy for doing broad crawls, along with\nconcrete suggestions of Scrapy settings to tune in order to achieve an\nefficient broad crawl.\n\nIncrease concurrency\u00b6\nConcurrency is the number of requests that are processed in parallel. There is\na global limit and a per-domain limit.\nThe default global concurrency limit in Scrapy is not suitable for crawling\nmany different  domains in parallel, so you will want to increase it. How much\nto increase it will depend on how much CPU you crawler will have available. A\ngood starting point is 100, but the best way to find out is by doing some\ntrials and identifying at what concurrency your Scrapy process gets CPU\nbounded. For optimum performance, You should pick a concurrency where CPU usage\nis at 80-90%.\nTo increase the global concurrency use:\nCONCURRENT_REQUESTS = 100\n\n\n\n\nReduce log level\u00b6\nWhen doing broad crawls you are often only interested in the crawl rates you\nget and any errors found. These stats are reported by Scrapy when using the\nINFO log level. In order to save CPU (and log storage requirements) you\nshould not use DEBUG log level when preforming large broad crawls in\nproduction. Using DEBUG level when developing your (broad) crawler may fine\nthough.\nTo set the log level use:\nLOG_LEVEL = 'INFO'\n\n\n\n\nDisable cookies\u00b6\nDisable cookies unless you really need. Cookies are often not needed when\ndoing broad crawls (search engine crawlers ignore them), and they improve\nperformance by saving some CPU cycles and reducing the memory footprint of your\nScrapy crawler.\nTo disable cookies use:\nCOOKIES_ENABLED = False\n\n\n\n\nDisable retries\u00b6\nRetrying failed HTTP requests can slow down the crawls substantially, specially\nwhen sites causes are very slow (or fail) to respond, thus causing a timeout\nerror which gets retried many times, unnecessarily, preventing crawler capacity\nto be reused for other domains.\nTo disable retries use:\nRETRY_ENABLED = False\n\n\n\n\nReduce download timeout\u00b6\nUnless you are crawling from a very slow connection (which shouldn\u2019t be the\ncase for broad crawls) reduce the download timeout so that stuck requests are\ndiscarded quickly and free up capacity to process the next ones.\nTo reduce the download timeout use:\nDOWNLOAD_TIMEOUT = 15\n\n\n\n\nDisable redirects\u00b6\nConsider disabling redirects, unless you are interested in following them. When\ndoing broad crawls it\u2019s common to save redirects and resolve them when\nrevisiting the site at a later crawl. This also help to keep the number of\nrequest constant per crawl batch, otherwise redirect loops may cause the\ncrawler to dedicate too many resources on any specific domain.\nTo disable redirects use:\nREDIRECT_ENABLED = False\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/broad-crawls.html", "title": ["Broad Crawls \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nUsing Firefox for scraping\u00b6\nHere is a list of tips and advice on using Firefox for scraping, along with a\nlist of useful Firefox add-ons to ease the scraping process.\n\nCaveats with inspecting the live browser DOM\u00b6\nSince Firefox add-ons operate on a live browser DOM, what you\u2019ll actually see\nwhen inspecting the page source is not the original HTML, but a modified one\nafter applying some browser clean up and executing Javascript code.  Firefox,\nin particular, is known for adding <tbody> elements to tables.  Scrapy, on\nthe other hand, does not modify the original page HTML, so you won\u2019t be able to\nextract any data if you use <tbody in your XPath expressions.\nTherefore, you should keep in mind the following things when working with\nFirefox and XPath:\nDisable Firefox Javascript while inspecting the DOM looking for XPaths to be\nused in Scrapy\nNever use full XPath paths, use relative and clever ones based on attributes\n(such as id, class, width, etc) or any identifying features like\ncontains(@href, 'image').\nNever include <tbody> elements in your XPath expressions unless you\nreally know what you\u2019re doing\n\n\nUseful Firefox add-ons for scraping\u00b6\n\nFirebug\u00b6\nFirebug is a widely known tool among web developers and it\u2019s also very\nuseful for scraping. In particular, its Inspect Element feature comes very\nhandy when you need to construct the XPaths for extracting data because it\nallows you to view the HTML code of each page element while moving your mouse\nover it.\nSee Using Firebug for scraping for a detailed guide on how to use Firebug with\nScrapy.\n\n\nXPather\u00b6\nXPather allows you to test XPath expressions directly on the pages.\n\n\nXPath Checker\u00b6\nXPath Checker is another Firefox add-on for testing XPaths on your pages.\n\n\nTamper Data\u00b6\nTamper Data is a Firefox add-on which allows you to view and modify the HTTP\nrequest headers sent by Firefox. Firebug also allows to view HTTP headers, but\nnot to modify them.\n\n\nFirecookie\u00b6\nFirecookie makes it easier to view and manage cookies. You can use this\nextension to create a new cookie, delete existing cookies, see a list of cookies\nfor the current site, manage cookies permissions and a lot more.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/firefox.html", "title": ["Using Firefox for scraping \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nUsing Firebug for scraping\u00b6\n\nNote\nGoogle Directory, the example website used in this guide is no longer\navailable as it has been shut down by Google. The concepts in this guide\nare still valid though. If you want to update this guide to use a new\n(working) site, your contribution will be more than welcome!. See Contributing to Scrapy\nfor information on how to do so.\n\n\nIntroduction\u00b6\nThis document explains how to use Firebug (a Firefox add-on) to make the\nscraping process easier and more fun. For other useful Firefox add-ons see\nUseful Firefox add-ons for scraping. There are some caveats with using Firefox add-ons\nto inspect pages, see Caveats with inspecting the live browser DOM.\nIn this example, we\u2019ll show how to use Firebug to scrape data from the\nGoogle Directory, which contains the same data as the Open Directory\nProject used in the tutorial but with a different\nface.\nFirebug comes with a very useful feature called Inspect Element which allows\nyou to inspect the HTML code of the different page elements just by hovering\nyour mouse over them. Otherwise you would have to search for the tags manually\nthrough the HTML body which can be a very tedious task.\nIn the following screenshot you can see the Inspect Element tool in action.\n\nAt first sight, we can see that the directory is divided in categories, which\nare also divided in subcategories.\nHowever, it seems that there are more subcategories than the ones being shown\nin this page, so we\u2019ll keep looking:\n\nAs expected, the subcategories contain links to other subcategories, and also\nlinks to actual websites, which is the purpose of the directory.\n\n\nGetting links to follow\u00b6\nBy looking at the category URLs we can see they share a pattern:\n\nhttp://directory.google.com/Category/Subcategory/Another_Subcategory\nOnce we know that, we are able to construct a regular expression to follow\nthose links. For example, the following one:\ndirectory\\.google\\.com/[A-Z][a-zA-Z_/]+$\n\nSo, based on that regular expression we can create the first crawling rule:\nRule(SgmlLinkExtractor(allow='directory.google.com/[A-Z][a-zA-Z_/]+$', ),\n    'parse_category',\n    follow=True,\n),\n\n\nThe Rule object instructs\nCrawlSpider based spiders how to follow the\ncategory links. parse_category will be a method of the spider which will\nprocess and extract data from those pages.\nThis is how the spider would look so far:\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\n\nclass GoogleDirectorySpider(CrawlSpider):\n    name = 'directory.google.com'\n    allowed_domains = ['directory.google.com']\n    start_urls = ['http://directory.google.com/']\n\n    rules = (\n        Rule(SgmlLinkExtractor(allow='directory\\.google\\.com/[A-Z][a-zA-Z_/]+$'),\n            'parse_category', follow=True,\n        ),\n    )\n\n    def parse_category(self, response):\n        # write the category page data extraction code here\n        pass\n\n\n\n\nExtracting the data\u00b6\nNow we\u2019re going to write the code to extract data from those pages.\nWith the help of Firebug, we\u2019ll take a look at some page containing links to\nwebsites (say http://directory.google.com/Top/Arts/Awards/) and find out how we can\nextract those links using XPath selectors. We\u2019ll also\nuse the Scrapy shell to test those XPath\u2019s and make sure\nthey work as we expect.\n\nAs you can see, the page markup is not very descriptive: the elements don\u2019t\ncontain id, class or any attribute that clearly identifies them, so\nwe\u2019\u2018ll use the ranking bars as a reference point to select the data to extract\nwhen we construct our XPaths.\nAfter using FireBug, we can see that each link is inside a td tag, which is\nitself inside a tr tag that also contains the link\u2019s ranking bar (in\nanother td).\nSo we can select the ranking bar, then find its parent (the tr), and then\nfinally, the link\u2019s td (which contains the data we want to scrape).\nThis results in the following XPath:\n//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td//a\n\nIt\u2019s important to use the Scrapy shell to test these\ncomplex XPath expressions and make sure they work as expected.\nBasically, that expression will look for the ranking bar\u2019s td element, and\nthen select any td element who has a descendant a element whose\nhref attribute contains the string #pagerank\u201c\nOf course, this is not the only XPath, and maybe not the simpler one to select\nthat data. Another approach could be, for example, to find any font tags\nthat have that grey colour of the links,\nFinally, we can write our parse_category() method:\ndef parse_category(self, response):\n    hxs = HtmlXPathSelector(response)\n\n    # The path to website links in directory page\n    links = hxs.select('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n\n    for link in links:\n        item = DirectoryItem()\n        item['name'] = link.select('a/text()').extract()\n        item['url'] = link.select('a/@href').extract()\n        item['description'] = link.select('font[2]/text()').extract()\n        yield item\n\n\nBe aware that you may find some elements which appear in Firebug but\nnot in the original HTML, such as the typical case of <tbody>\nelements.\nor tags which Therefer   in page HTML\nsources may on Firebug inspects the live DOM\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/firebug.html", "title": ["Using Firebug for scraping \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDebugging memory leaks\u00b6\nIn Scrapy, objects such as Requests, Responses and Items have a finite\nlifetime: they are created, used for a while, and finally destroyed.\nFrom all those objects, the Request is probably the one with the longest\nlifetime, as it stays waiting in the Scheduler queue until it\u2019s time to process\nit. For more info see Architecture overview.\nAs these Scrapy objects have a (rather long) lifetime, there is always the risk\nof accumulating them in memory without releasing them properly and thus causing\nwhat is known as a \u201cmemory leak\u201d.\nTo help debugging memory leaks, Scrapy provides a built-in mechanism for\ntracking objects references called trackref,\nand you can also use a third-party library called Guppy for more advanced memory debugging (see below for more\ninfo). Both mechanisms must be used from the Telnet Console.\n\nCommon causes of memory leaks\u00b6\nIt happens quite often (sometimes by accident, sometimes on purpose) that the\nScrapy developer passes objects referenced in Requests (for example, using the\nmeta attribute or the request callback function)\nand that effectively bounds the lifetime of those referenced objects to the\nlifetime of the Request. This is, by far, the most common cause of memory leaks\nin Scrapy projects, and a quite difficult one to debug for newcomers.\nIn big projects, the spiders are typically written by different people and some\nof those spiders could be \u201cleaking\u201d and thus affecting the rest of the other\n(well-written) spiders when they get to run concurrently, which, in turn,\naffects the whole crawling process.\nAt the same time, it\u2019s hard to avoid the reasons that cause these leaks\nwithout restricting the power of the framework, so we have decided not to\nrestrict the functionally but provide useful tools for debugging these leaks,\nwhich quite often consist in an answer to the question: which spider is leaking?.\nThe leak could also come from a custom middleware, pipeline or extension that\nyou have written, if you are not releasing the (previously allocated) resources\nproperly. For example, if you\u2019re allocating resources on\nspider_opened but not releasing them on spider_closed.\n\n\nDebugging memory leaks with trackref\u00b6\ntrackref is a module provided by Scrapy to debug the most common cases of\nmemory leaks. It basically tracks the references to all live Requests,\nResponses, Item and Selector objects.\nYou can enter the telnet console and inspect how many objects (of the classes\nmentioned above) are currently alive using the prefs() function which is an\nalias to the print_live_refs() function:\ntelnet localhost 6023\n\n>>> prefs()\nLive References\n\nExampleSpider                       1   oldest: 15s ago\nHtmlResponse                       10   oldest: 1s ago\nXPathSelector                       2   oldest: 0s ago\nFormRequest                       878   oldest: 7s ago\n\nAs you can see, that report also shows the \u201cage\u201d of the oldest object in each\nclass.\nIf you do have leaks, chances are you can figure out which spider is leaking by\nlooking at the oldest request or response. You can get the oldest object of\neach class using the get_oldest() function like\nthis (from the telnet console).\n\nWhich objects are tracked?\u00b6\nThe objects tracked by trackrefs are all from these classes (and all its\nsubclasses):\nscrapy.http.Request\nscrapy.http.Response\nscrapy.item.Item\nscrapy.selector.XPathSelector\nscrapy.spider.BaseSpider\nscrapy.selector.document.Libxml2Document\n\n\nA real example\u00b6\nLet\u2019s see a concrete example of an hypothetical case of memory leaks.\nSuppose we have some spider with a line similar to this one:\nreturn Request(\"http://www.somenastyspider.com/product.php?pid=%d\" % product_id,\n    callback=self.parse, meta={referer: response}\")\n\nThat line is passing a response reference inside a request which effectively\nties the response lifetime to the requests\u2019 one, and that would definitely\ncause memory leaks.\nLet\u2019s see how we can discover which one is the nasty spider (without knowing it\na-priori, of course) by using the trackref tool.\nAfter the crawler is running for a few minutes and we notice its memory usage\nhas grown a lot, we can enter its telnet console and check the live\nreferences:\n>>> prefs()\nLive References\n\nSomenastySpider                     1   oldest: 15s ago\nHtmlResponse                     3890   oldest: 265s ago\nXPathSelector                       2   oldest: 0s ago\nRequest                          3878   oldest: 250s ago\n\n\nThe fact that there are so many live responses (and that they\u2019re so old) is\ndefinitely suspicious, as responses should have a relatively short lifetime\ncompared to Requests. So let\u2019s check the oldest response:\n>>> from scrapy.utils.trackref import get_oldest\n>>> r = get_oldest('HtmlResponse')\n>>> r.url\n'http://www.somenastyspider.com/product.php?pid=123'\n\n\nThere it is. By looking at the URL of the oldest response we can see it belongs\nto the somenastyspider.com spider. We can now go and check the code of that\nspider to discover the nasty line that is generating the leaks (passing\nresponse references inside requests).\nIf you want to iterate over all objects, instead of getting the oldest one, you\ncan use the iter_all() function:\n>>> from scrapy.utils.trackref import iter_all\n>>> [r.url for r in iter_all('HtmlResponse')]\n['http://www.somenastyspider.com/product.php?pid=123',\n 'http://www.somenastyspider.com/product.php?pid=584',\n...\n\n\n\n\nToo many spiders?\u00b6\nIf your project has too many spiders, the output of prefs() can be\ndifficult to read. For this reason, that function has a ignore argument\nwhich can be used to ignore a particular class (and all its subclases). For\nexample, using:\n>>> from scrapy.spider import BaseSpider\n>>> prefs(ignore=BaseSpider)\n\n\nWon\u2019t show any live references to spiders.\n\n\nscrapy.utils.trackref module\u00b6\nHere are the functions available in the trackref module.\n\nclass scrapy.utils.trackref.object_ref\u00b6\nInherit from this class (instead of object) if you want to track live\ninstances with the trackref module.\n\nscrapy.utils.trackref.print_live_refs(class_name, ignore=NoneType)\u00b6\nPrint a report of live references, grouped by class name.\nParameters:ignore (class or classes tuple) \u2013 if given, all objects from the specified class (or tuple of\nclasses) will be ignored.\n\nscrapy.utils.trackref.get_oldest(class_name)\u00b6\nReturn the oldest object alive with the given class name, or None if\nnone is found. Use print_live_refs() first to get a list of all\ntracked live objects per class name.\n\nscrapy.utils.trackref.iter_all(class_name)\u00b6\nReturn an iterator over all objects alive with the given class name, or\nNone if none is found. Use print_live_refs() first to get a list\nof all tracked live objects per class name.\n\n\n\nDebugging memory leaks with Guppy\u00b6\ntrackref provides a very convenient mechanism for tracking down memory\nleaks, but it only keeps track of the objects that are more likely to cause\nmemory leaks (Requests, Responses, Items, and Selectors). However, there are\nother cases where the memory leaks could come from other (more or less obscure)\nobjects. If this is your case, and you can\u2019t find your leaks using trackref,\nyou still have another resource: the Guppy library.\nIf you use setuptools, you can install Guppy with the following command:\neasy_install guppy\n\nThe telnet console also comes with a built-in shortcut (hpy) for accessing\nGuppy heap objects. Here\u2019s an example to view all Python objects available in\nthe heap using Guppy:\n>>> x = hpy.heap()\n>>> x.bytype\nPartition of a set of 297033 objects. Total size = 52587824 bytes.\n Index  Count   %     Size   % Cumulative  % Type\n     0  22307   8 16423880  31  16423880  31 dict\n     1 122285  41 12441544  24  28865424  55 str\n     2  68346  23  5966696  11  34832120  66 tuple\n     3    227   0  5836528  11  40668648  77 unicode\n     4   2461   1  2222272   4  42890920  82 type\n     5  16870   6  2024400   4  44915320  85 function\n     6  13949   5  1673880   3  46589200  89 types.CodeType\n     7  13422   5  1653104   3  48242304  92 list\n     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern\n     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers\n<1676 more rows. Type e.g. '_.more' to view.>\n\n\nYou can see that most space is used by dicts. Then, if you want to see from\nwhich attribute those dicts are referenced, you could do:\n>>> x.bytype[0].byvia\nPartition of a set of 22307 objects. Total size = 16423880 bytes.\n Index  Count   %     Size   % Cumulative  % Referred Via:\n     0  10982  49  9416336  57   9416336  57 '.__dict__'\n     1   1820   8  2681504  16  12097840  74 '.__dict__', '.func_globals'\n     2   3097  14  1122904   7  13220744  80\n     3    990   4   277200   2  13497944  82 \"['cookies']\"\n     4    987   4   276360   2  13774304  84 \"['cache']\"\n     5    985   4   275800   2  14050104  86 \"['meta']\"\n     6    897   4   251160   2  14301264  87 '[2]'\n     7      1   0   196888   1  14498152  88 \"['moduleDict']\", \"['modules']\"\n     8    672   3   188160   1  14686312  89 \"['cb_kwargs']\"\n     9     27   0   155016   1  14841328  90 '[1]'\n<333 more rows. Type e.g. '_.more' to view.>\n\n\nAs you can see, the Guppy module is very powerful but also requires some deep\nknowledge about Python internals. For more info about Guppy, refer to the\nGuppy documentation.\n\n\nLeaks without leaks\u00b6\nSometimes, you may notice that the memory usage of your Scrapy process will\nonly increase, but never decrease. Unfortunately, this could happen even\nthough neither Scrapy nor your project are leaking memory. This is due to a\n(not so well) known problem of Python, which may not return released memory to\nthe operating system in some cases. For more information on this issue see:\nPython Memory Management\nPython Memory Management Part 2\nPython Memory Management Part 3\nThe improvements proposed by Evan Jones, which are detailed in this paper,\ngot merged in Python 2.5, but this only reduces the problem, it doesn\u2019t fix it\ncompletely. To quote the paper:\n\nUnfortunately, this patch can only free an arena if there are no more\nobjects allocated in it anymore. This means that fragmentation is a large\nissue. An application could have many megabytes of free memory, scattered\nthroughout all the arenas, but it will be unable to free any of it. This is\na problem experienced by all memory allocators. The only way to solve it is\nto move to a compacting garbage collector, which is able to move objects in\nmemory. This would require significant changes to the Python interpreter.\nThis problem will be fixed in future Scrapy releases, where we plan to adopt a\nnew process model and run spiders in a pool of recyclable sub-processes.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/leaks.html", "title": ["Debugging memory leaks \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDownloading Item Images\u00b6\nScrapy provides an item pipeline for downloading\nimages attached to a particular item, for example, when you scrape products and\nalso want to download their images locally.\nThis pipeline, called the Images Pipeline and implemented in the\nImagesPipeline class, provides a convenient way for\ndownloading and storing images locally with some additional features:\nConvert all downloaded images to a common format (JPG) and mode (RGB)\nAvoid re-downloading images which were downloaded recently\nThumbnail generation\nCheck images width/height to make sure they meet a minimum constraint\nThis pipeline also keeps an internal queue of those images which are currently\nbeing scheduled for download, and connects those items that arrive containing\nthe same image, to that queue. This avoids downloading the same image more than\nonce when it\u2019s shared by several items.\nThe Python Imaging Library is used for thumbnailing and normalizing images\nto JPEG/RGB format, so you need to install that library in order to use the\nimages pipeline.\n\nUsing the Images Pipeline\u00b6\nThe typical workflow, when using the ImagesPipeline goes like\nthis:\nIn a Spider, you scrape an item and put the URLs of its images into a\nimage_urls field.\nThe item is returned from the spider and goes to the item pipeline.\nWhen the item reaches the ImagesPipeline, the URLs in the\nimage_urls field are scheduled for download using the standard\nScrapy scheduler and downloader (which means the scheduler and downloader\nmiddlewares are reused), but with a higher priority, processing them before other\npages are scraped. The item remains \u201clocked\u201d at that particular pipeline stage\nuntil the images have finish downloading (or fail for some reason).\nWhen the images are downloaded another field (images) will be populated\nwith the results. This field will contain a list of dicts with information\nabout the images downloaded, such as the downloaded path, the original\nscraped url (taken from the image_urls field) , and the image checksum.\nThe images in the list of the images field will retain the same order of\nthe original image_urls field. If some image failed downloading, an\nerror will be logged and the image won\u2019t be present in the images field.\n\n\nUsage example\u00b6\nIn order to use the image pipeline you just need to enable it and define an item with the image_urls and\nimages fields:\nfrom scrapy.item import Item\n\nclass MyItem(Item):\n\n    # ... other item fields ...\n    image_urls = Field()\n    images = Field()\n\n\nIf you need something more complex and want to override the custom images\npipeline behaviour, see Implementing your custom Images Pipeline.\n\n\nEnabling your Images Pipeline\u00b6\nTo enable your images pipeline you must first add it to your project\nITEM_PIPELINES setting:\nITEM_PIPELINES = ['scrapy.contrib.pipeline.images.ImagesPipeline']\n\n\nAnd set the IMAGES_STORE setting to a valid directory that will be\nused for storing the downloaded images. Otherwise the pipeline will remain\ndisabled, even if you include it in the ITEM_PIPELINES setting.\nFor example:\nIMAGES_STORE = '/path/to/valid/dir'\n\n\n\n\nImages Storage\u00b6\nFile system is currently the only officially supported storage, but there is\nalso (undocumented) support for Amazon S3.\n\nFile system storage\u00b6\nThe images are stored in files (one per image), using a SHA1 hash of their\nURLs for the file names.\nFor example, the following image URL:\nhttp://www.example.com/image.jpg\n\nWhose SHA1 hash is:\n3afec3b4765f8f0a07b78f98c07b83f013567a0a\n\nWill be downloaded and stored in the following file:\n<IMAGES_STORE>/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg\n\nWhere:\n<IMAGES_STORE> is the directory defined in IMAGES_STORE setting\nfull is a sub-directory to separate full images from thumbnails (if\nused). For more info see Thumbnail generation.\n\n\n\nAdditional features\u00b6\n\nImage expiration\u00b6\nThe Image Pipeline avoids downloading images that were downloaded recently. To\nadjust this retention delay use the IMAGES_EXPIRES setting, which\nspecifies the delay in number of days:\n# 90 days of delay for image expiration\nIMAGES_EXPIRES = 90\n\n\n\n\nThumbnail generation\u00b6\nThe Images Pipeline can automatically create thumbnails of the downloaded\nimages.\nIn order use this feature, you must set IMAGES_THUMBS to a dictionary\nwhere the keys are the thumbnail names and the values are their dimensions.\nFor example:\nIMAGES_THUMBS = {\n    'small': (50, 50),\n    'big': (270, 270),\n}\n\n\nWhen you use this feature, the Images Pipeline will create thumbnails of the\neach specified size with this format:\n<IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg\n\nWhere:\n<size_name> is the one specified in the IMAGES_THUMBS\ndictionary keys (small, big, etc)\n<image_id> is the SHA1 hash of the image url\nExample of image files stored using small and big thumbnail names:\n<IMAGES_STORE>/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg\n<IMAGES_STORE>/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg\n<IMAGES_STORE>/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg\n\nThe first one is the full image, as downloaded from the site.\n\n\nFiltering out small images\u00b6\nYou can drop images which are too small, by specifying the minimum allowed size\nin the IMAGES_MIN_HEIGHT and IMAGES_MIN_WIDTH settings.\nFor example:\nIMAGES_MIN_HEIGHT = 110\nIMAGES_MIN_WIDTH = 110\n\n\nNote: these size constraints don\u2019t affect thumbnail generation at all.\nBy default, there are no size constraints, so all images are processed.\n\n\n\nImplementing your custom Images Pipeline\u00b6\nHere are the methods that you should override in your custom Images Pipeline:\n\nclass scrapy.contrib.pipeline.images.ImagesPipeline\u00b6\n\nget_media_requests(item, info)\u00b6\nAs seen on the workflow, the pipeline will get the URLs of the images to\ndownload from the item. In order to do this, you must override the\nget_media_requests() method and return a Request for each\nimage URL:\ndef get_media_requests(self, item, info):\n    for image_url in item['image_urls']:\n        yield Request(image_url)\n\n\nThose requests will be processed by the pipeline and, when they have finished\ndownloading, the results will be sent to the\nitem_completed() method, as a list of 2-element tuples.\nEach tuple will contain (success, image_info_or_failure) where:\nsuccess is a boolean which is True if the image was downloaded\nsuccessfully or False if it failed for some reason\nimage_info_or_error is a dict containing the following keys (if success\nis True) or a Twisted Failure if there was a problem.url - the url where the image was downloaded from. This is the url of\nthe request returned from the get_media_requests()\nmethod.\npath - the path (relative to IMAGES_STORE) where the image\nwas stored\nchecksum - a MD5 hash of the image contents\n\nThe list of tuples received by item_completed() is\nguaranteed to retain the same order of the requests returned from the\nget_media_requests() method.\nHere\u2019s a typical value of the results argument:\n[(True,\n  {'checksum': '2b00042f7481c7b056c4b410d28f33cf',\n   'path': 'full/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg',\n   'url': 'http://www.example.com/images/product1.jpg'}),\n (True,\n  {'checksum': 'b9628c4ab9b595f72f280b90c4fd093d',\n   'path': 'full/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg',\n   'url': 'http://www.example.com/images/product2.jpg'}),\n (False,\n  Failure(...))]\n\n\nBy default the get_media_requests() method returns None which\nmeans there are no images to download for the item.\n\nitem_completed(results, items, info)\u00b6\nThe ImagesPipeline.item_completed() method called when all image\nrequests for a single item have completed (either finished downloading, or\nfailed for some reason).\nThe item_completed() method must return the\noutput that will be sent to subsequent item pipeline stages, so you must\nreturn (or drop) the item, as you would in any pipeline.\nHere is an example of the item_completed() method where we\nstore the downloaded image paths (passed in results) in the image_paths\nitem field, and we drop the item if it doesn\u2019t contain any images:\nfrom scrapy.exceptions import DropItem\n\ndef item_completed(self, results, item, info):\n    image_paths = [x['path'] for ok, x in results if ok]\n    if not image_paths:\n        raise DropItem(\"Item contains no images\")\n    item['image_paths'] = image_paths\n    return item\n\n\nBy default, the item_completed() method returns the item.\n\n\nCustom Images pipeline example\u00b6\nHere is a full example of the Images Pipeline whose methods are examplified\nabove:\nfrom scrapy.contrib.pipeline.images import ImagesPipeline\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request\n\nclass MyImagesPipeline(ImagesPipeline):\n\n    def get_media_requests(self, item, info):\n        for image_url in item['image_urls']:\n            yield Request(image_url)\n\n    def item_completed(self, results, item, info):\n        image_paths = [x['path'] for ok, x in results if ok]\n        if not image_paths:\n            raise DropItem(\"Item contains no images\")\n        item['image_paths'] = image_paths\n        return item\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/images.html", "title": ["Downloading Item Images \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nUbuntu packages\u00b6\n\nNew in version 0.10.\nScrapinghub publishes apt-gettable packages which are generally fresher than\nthose in Ubuntu, and more stable too since they\u2019re continuously built from\nGithub repo (master & stable branches) and so they contain the latest bug\nfixes.\nTo use the packages, just add the following line to your\n/etc/apt/sources.list, and then run aptitude update and aptitude\ninstall scrapy-0.13:\ndeb http://archive.scrapy.org/ubuntu DISTRO main\n\nReplacing DISTRO with the name of your Ubuntu release, which you can get\nwith command:\nlsb_release -cs\n\n\nSupported Ubuntu releases are: karmic, lucid, maverick, natty,\noneiric, precise.\nFor Ubuntu Precise (12.04):\ndeb http://archive.scrapy.org/ubuntu precise main\n\nFor Ubuntu Oneiric (11.10):\ndeb http://archive.scrapy.org/ubuntu oneiric main\n\nFor Ubuntu Natty (11.04):\ndeb http://archive.scrapy.org/ubuntu natty main\n\nFor Ubuntu Maverick (10.10):\ndeb http://archive.scrapy.org/ubuntu maverick main\n\nFor Ubuntu Lucid (10.04):\ndeb http://archive.scrapy.org/ubuntu lucid main\n\nFor Ubuntu Karmic (9.10):\ndeb http://archive.scrapy.org/ubuntu karmic main\n\n\nWarning\nPlease note that these packages are updated frequently, and so if\nyou find you can\u2019t download the packages, try updating your apt package\nlists first, e.g., with apt-get update or aptitude update.\n\nThe public GPG key used to sign these packages can be imported into you APT\nkeyring as follows:\ncurl -s http://archive.scrapy.org/ubuntu/archive.key | sudo apt-key add -\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/ubuntu.html", "title": ["Ubuntu packages \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nScrapy Service (scrapyd)\u00b6\n\nNew in version 0.10.\nScrapy comes with a built-in service, called \u201cScrapyd\u201d, which allows you to\ndeploy (aka. upload) your projects and control their spiders using a JSON web\nservice.\n\nProjects and versions\u00b6\nScrapyd can manage multiple projects and each project can have multiple\nversions uploaded, but only the latest one will be used for launching new\nspiders.\nA common (and useful) convention to use for the version name is the revision\nnumber of the version control tool you\u2019re using to track your Scrapy project\ncode. For example: r23. The versions are not compared alphabetically but\nusing a smarter algorithm (the same distutils uses) so r10 compares\ngreater to r9, for example.\n\n\nHow Scrapyd works\u00b6\nScrapyd is an application (typically run as a daemon) that continually polls\nfor spiders that need to run.\nWhen a spider needs to run, a process is started to crawl the spider:\nscrapy crawl myspider\n\nScrapyd also runs multiple processes in parallel, allocating them in a fixed\nnumber of slots given by the max_proc and max_proc_per_cpu options,\nstarting as many processes as possible to handle the load.\nIn addition to dispatching and managing processes, Scrapyd provides a\nJSON web service to upload new project versions\n(as eggs) and schedule spiders. This feature is optional and can be disabled if\nyou want to implement your own custom Scrapyd. The components are pluggable and\ncan be changed, if you\u2019re familiar with the Twisted Application Framework\nwhich Scrapyd is implemented in.\nStarting from 0.11, Scrapyd also provides a minimal web interface.\n\n\nStarting Scrapyd\u00b6\nScrapyd is implemented using the standard Twisted Application Framework. To\nstart the service, use the extras/scrapyd.tac file provided in the Scrapy\ndistribution, like this:\ntwistd -ny extras/scrapyd.tac\n\nThat should get your Scrapyd started.\nOr, if you want to start Scrapyd from inside a Scrapy project you can use the\nserver command, like this:\nscrapy server\n\n\n\nInstalling Scrapyd\u00b6\nHow to deploy Scrapyd on your servers depends on the platform you\u2019re using.\nScrapy comes with Ubuntu packages for Scrapyd ready for deploying it as a\nsystem service, to ease the installation and administration, but you can create\npackages for other distribution or operating systems (including Windows). If\nyou do so, and want to contribute them, send a message to\nscrapy-developers@googlegroups.com and say hi. The community will appreciate\nit.\n\nInstalling Scrapyd in Ubuntu\u00b6\nWhen deploying Scrapyd, it\u2019s very useful to have a version already packaged for\nyour system. For this reason, Scrapyd comes with Ubuntu packages ready to use\nin your Ubuntu servers.\nSo, if you plan to deploy Scrapyd on a Ubuntu server, just add the Ubuntu\nrepositories as described in Ubuntu packages and then run:\naptitude install scrapyd-X.YY\n\nWhere X.YY is the Scrapy version, for example: 0.14.\nThis will install Scrapyd in your Ubuntu server creating a scrapy user\nwhich Scrapyd will run as. It will also create some directories and files that\nare listed below:\n\n/etc/scrapyd\u00b6\nScrapyd configuration files. See Scrapyd Configuration file.\n\n\n/var/log/scrapyd/scrapyd.log\u00b6\nScrapyd main log file.\n\n\n/var/log/scrapyd/scrapyd.out\u00b6\nThe standard output captured from Scrapyd process and any\nsub-process spawned from it.\n\n\n/var/log/scrapyd/scrapyd.err\u00b6\nThe standard error captured from Scrapyd and any sub-process spawned\nfrom it. Remember to check this file if you\u2019re having problems, as the errors\nmay not get logged to the scrapyd.log file.\n\n\n/var/log/scrapyd/project\u00b6\nBesides the main service log file, Scrapyd stores one log file per crawling\nprocess in:\n/var/log/scrapyd/PROJECT/SPIDER/ID.log\n\nWhere ID is a unique id for the run.\n\n\n/var/lib/scrapyd/\u00b6\nDirectory used to store data files (uploaded eggs and spider queues).\n\n\n\n\nScrapyd Configuration file\u00b6\nScrapyd searches for configuration files in the following locations, and parses\nthem in order with the latest ones taking more priority:\n/etc/scrapyd/scrapyd.conf (Unix)\nc:\\scrapyd\\scrapyd.conf (Windows)\n/etc/scrapyd/conf.d/* (in alphabetical order, Unix)\nscrapyd.conf\nThe configuration file supports the following options (see default values in\nthe example).\n\nhttp_port\u00b6\nThe TCP port where the HTTP JSON API will listen. Defaults to 6800.\n\n\nbind_address\u00b6\nThe IP address where the HTTP JSON API will listen. Defaults to 0.0.0.0 (all)\n\n\nmax_proc\u00b6\nThe maximum number of concurrent Scrapy process that will be started. If unset\nor 0 it will use the number of cpus available in the system mulitplied by\nthe value in max_proc_per_cpu option. Defaults to 0.\n\n\nmax_proc_per_cpu\u00b6\nThe maximum number of concurrent Scrapy process that will be started per cpu.\nDefaults to 4.\n\n\ndebug\u00b6\nWhether debug mode is enabled. Defaults to off. When debug mode is enabled\nthe full Python traceback will be returned (as plain text responses) when there\nis an error processing a JSON API call.\n\n\neggs_dir\u00b6\nThe directory where the project eggs will be stored.\n\n\ndbs_dir\u00b6\nThe directory where the project databases will be stored (this includes the\nspider queues).\n\n\nlogs_dir\u00b6\nThe directory where the Scrapy logs will be stored. If you want to disable\nstoring logs set this option empty, like this:\nlogs_dir =\n\n\n\nitems_dir\u00b6\n\nNew in version 0.15.\nThe directory where the Scrapy items will be stored. If you want to disable\nstoring feeds of scraped items (perhaps, because you use a database or other\nstorage) set this option empty, like this:\nitems_dir =\n\n\n\njobs_to_keep\u00b6\n\nNew in version 0.15.\nThe number of finished jobs to keep per spider. Defaults to 5. This\nincludes logs and items.\nThis setting was named logs_to_keep in previous versions.\n\n\nrunner\u00b6\nThe module that will be used for launching sub-processes. You can customize the\nScrapy processes launched from Scrapyd by using your own module.\n\n\napplication\u00b6\nA function that returns the (Twisted) Application object to use. This can be\nused if you want to extend Scrapyd by adding and removing your own components\nand services.\nFor more info see Twisted Application Framework\n\n\nExample configuration file\u00b6\nHere is an example configuration file with all the defaults:\n[scrapyd]\neggs_dir    = eggs\nlogs_dir    = logs\nitems_dir   = items\njobs_to_keep = 5\ndbs_dir     = dbs\nmax_proc    = 0\nmax_proc_per_cpu = 4\nfinished_to_keep = 100\nhttp_port   = 6800\ndebug       = off\nrunner      = scrapyd.runner\napplication = scrapyd.app.application\nlauncher    = scrapyd.launcher.Launcher\n\n[services]\nschedule.json     = scrapyd.webservice.Schedule\ncancel.json       = scrapyd.webservice.Cancel\naddversion.json   = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json  = scrapyd.webservice.ListSpiders\ndelproject.json   = scrapyd.webservice.DeleteProject\ndelversion.json   = scrapyd.webservice.DeleteVersion\nlistjobs.json     = scrapyd.webservice.ListJobs\n\n\n\n\n\nDeploying your project\u00b6\nDeploying your project into a Scrapyd server typically involves two steps:\nbuilding a Python egg of your project. This is called \u201ceggifying\u201d your\nproject. You\u2019ll need to install setuptools for this. See\nEgg caveats below.\nuploading the egg to the Scrapyd server\nThe simplest way to deploy your project is by using the deploy\ncommand, which automates the process of building the egg uploading it using the\nScrapyd HTTP JSON API.\nThe deploy command supports multiple targets (Scrapyd servers that\ncan host your project) and each target supports multiple projects.\nEach time you deploy a new version of a project, you can name it for later\nreference.\n\nShow and define targets\u00b6\nTo see all available targets type:\nscrapy deploy -l\n\nThis will return a list of available targets and their URLs. For example:\nscrapyd              http://localhost:6800/\n\nYou can define targets by adding them to your project\u2019s scrapy.cfg file,\nor any other supported location like ~/.scrapy.cfg, /etc/scrapy.cfg,\nor c:\\scrapy\\scrapy.cfg (in Windows).\nHere\u2019s an example of defining a new target scrapyd2 with restricted access\nthrough HTTP basic authentication:\n[deploy:scrapyd2]\nurl = http://scrapyd.mydomain.com/api/scrapyd/\nusername = john\npassword = secret\n\n\nNote\nThe deploy command also supports netrc for getting the\ncredentials.\n\nNow, if you type scrapy deploy -l you\u2019ll see:\nscrapyd              http://localhost:6800/\nscrapyd2             http://scrapyd.mydomain.com/api/scrapyd/\n\n\n\nSee available projects\u00b6\nTo see all available projects in a specific target use:\nscrapy deploy -L scrapyd\n\nIt would return something like this:\nproject1\nproject2\n\n\n\n\nDeploying a project\u00b6\nFinally, to deploy your project use:\nscrapy deploy scrapyd -p project1\n\nThis will eggify your project and upload it to the target, printing the JSON\nresponse returned from the Scrapyd server. If you have a setup.py file in\nyour project, that one will be used. Otherwise a setup.py file will be\ncreated automatically (based on a simple template) that you can edit later.\nAfter running that command you will see something like this, meaning your\nproject was uploaded successfully:\nDeploying myproject-1287453519 to http://localhost:6800/addversion.json\nServer response (200):\n{\"status\": \"ok\", \"spiders\": [\"spider1\", \"spider2\"]}\n\nBy default scrapy deploy uses the current timestamp for generating the\nproject version, as you can see in the output above. However, you can pass a\ncustom version with the --version option:\nscrapy deploy scrapyd -p project1 --version 54\n\nAlso, if you use Mercurial for tracking your project source code, you can use\nHG for the version which will be replaced by the current Mercurial\nrevision, for example r382:\nscrapy deploy scrapyd -p project1 --version HG\n\nAnd, if you use Git for tracking your project source code, you can use\nGIT for the version which will be replaced by the SHA1 of current Git\nrevision, for example b0582849179d1de7bd86eaa7201ea3cda4b5651f:\nscrapy deploy scrapyd -p project1 --version GIT\n\nSupport for other version discovery sources may be added in the future.\nFinally, if you don\u2019t want to specify the target, project and version every\ntime you run scrapy deploy you can define the defaults in the\nscrapy.cfg file. For example:\n[deploy]\nurl = http://scrapyd.mydomain.com/api/scrapyd/\nusername = john\npassword = secret\nproject = project1\nversion = HG\n\nThis way, you can deploy your project just by using:\nscrapy deploy\n\n\n\nLocal settings\u00b6\nSometimes, while your working on your projects, you may want to override your\ncertain settings with certain local settings that shouldn\u2019t be deployed to\nScrapyd, but only used locally to develop and debug your spiders.\nOne way to deal with this is to have a local_settings.py at the root of\nyour project (where the scrapy.cfg file resides) and add these lines to the\nend of your project settings:\ntry:\n    from local_settings import *\nexcept ImportError:\n    pass\n\n\nscrapy deploy won\u2019t deploy anything outside the project module so the\nlocal_settings.py file won\u2019t be deployed.\nHere\u2019s the directory structure, to illustrate:\nscrapy.cfg\nlocal_settings.py\nmyproject/\n    __init__.py\n    settings.py\n    spiders/\n        ...\n\n\n\nEgg caveats\u00b6\nThere are some things to keep in mind when building eggs of your Scrapy\nproject:\nmake sure no local development settings are included in the egg when you\nbuild it. The find_packages function may be picking up your custom\nsettings. In most cases you want to upload the egg with the default project\nsettings.\nyou shouldn\u2019t use __file__ in your project code as it doesn\u2019t play well\nwith eggs. Consider using pkgutil.get_data() instead.\nbe careful when writing to disk in your project (in any spider, extension or\nmiddleware) as Scrapyd will probably run with a different user which may not\nhave write access to certain directories. If you can, avoid writing to disk\nand always use tempfile for temporary files.\n\n\n\nScheduling a spider run\u00b6\nTo schedule a spider run:\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider2\n{\"status\": \"ok\", \"jobid\": \"26d1b1a6d6f111e0be5c001e648c57f8\"}\n\nFor more resources see: JSON API reference for more available resources.\n\n\nWeb Interface\u00b6\n\nNew in version 0.11.\nScrapyd comes with a minimal web interface (for monitoring running processes\nand accessing logs) which can be accessed at http://localhost:6800/\n\n\nJSON API reference\u00b6\nThe following section describes the available resources in Scrapyd JSON API.\n\naddversion.json\u00b6\nAdd a version to a project, creating the project if it doesn\u2019t exist.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\nversion (string, required) - the project version\negg (file, required) - a Python egg containing the project\u2019s code\n\nExample request:\n$ curl http://localhost:6800/addversion.json -F project=myproject -F version=r23 -F egg=@myproject.egg\n\nExample response:\n{\"status\": \"ok\", \"spiders\": 3}\n\n\n\n\nschedule.json\u00b6\nSchedule a spider run (also known as a job), returning the job id.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\nspider (string, required) - the spider name\nsetting (string, optional) - a scrapy setting to use when running the spider\nany other parameter is passed as spider argument\n\nExample request:\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider\n\nExample response:\n{\"status\": \"ok\", \"jobid\": \"6487ec79947edab326d6db28a2d86511e8247444\"}\n\n\nExample request passing a spider argument (arg1) and a setting\n(DOWNLOAD_DELAY):\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider -d setting=DOWNLOAD_DELAY=2 -d arg1=val1\n\n\n\ncancel.json\u00b6\n\nNew in version 0.15.\nCancel a spider run (aka. job). If the job is pending, it will be removed. If\nthe job is running, it will be terminated.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\njob (string, required) - the job id\n\nExample request:\n$ curl http://localhost:6800/cancel.json -d project=myproject -d job=6487ec79947edab326d6db28a2d86511e8247444\n\nExample response:\n{\"status\": \"ok\", \"prevstate\": \"running\"}\n\n\n\n\nlistprojects.json\u00b6\nGet the list of projects uploaded to this Scrapy server.\nSupported Request Methods: GET\nParameters: none\nExample request:\n$ curl http://localhost:6800/listprojects.json\n\nExample response:\n{\"status\": \"ok\", \"projects\": [\"myproject\", \"otherproject\"]}\n\n\n\n\nlistversions.json\u00b6\nGet the list of versions available for some project. The versions are returned\nin order, the last one is the currently used version.\nSupported Request Methods: GET\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/listversions.json?project=myproject\n\nExample response:\n{\"status\": \"ok\", \"versions\": [\"r99\", \"r156\"]}\n\n\n\n\nlistspiders.json\u00b6\nGet the list of spiders available in the last version of some project.\nSupported Request Methods: GET\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/listspiders.json?project=myproject\n\nExample response:\n{\"status\": \"ok\", \"spiders\": [\"spider1\", \"spider2\", \"spider3\"]}\n\n\n\n\nlistjobs.json\u00b6\n\nNew in version 0.15.\nGet the list of pending, running and finished jobs of some project.\nSupported Request Methods: GET\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/listjobs.json?project=myproject\n\nExample response:\n{\"status\": \"ok\",\n \"pending\": [{\"id\": \"78391cc0fcaf11e1b0090800272a6d06\", \"spider\": \"spider1\"}],\n \"running\": [{\"id\": \"422e608f9f28cef127b3d5ef93fe9399\", \"spider\": \"spider2\"}],\n \"finished\": [{\"id\": \"2f16646cfcaf11e1b0090800272a6d06\", \"spider\": \"spider3\", \"start_time\": \"2012-09-12 10:14:03.594664\", \"end_time\": \"2012-09-12 10:24:03.594664\"}]}\n\n\n\nNote\nAll job data is kept in memory and will be reset when the Scrapyd service is restarted. See issue 173.\n\n\n\ndelversion.json\u00b6\nDelete a project version. If there are no more versions available for a given\nproject, that project will be deleted too.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\nversion (string, required) - the project version\n\nExample request:\n$ curl http://localhost:6800/delversion.json -d project=myproject -d version=r99\n\nExample response:\n{\"status\": \"ok\"}\n\n\n\n\ndelproject.json\u00b6\nDelete a project and all its uploaded versions.\nSupported Request Methods: POST\nParameters:project (string, required) - the project name\n\nExample request:\n$ curl http://localhost:6800/delproject.json -d project=myproject\n\nExample response:\n{\"status\": \"ok\"}\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/scrapyd.html", "title": ["Scrapy Service (scrapyd) \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nAutoThrottle extension\u00b6\nThis is an extension for automatically throttling crawling speed based on load\nof both the Scrapy server and the website you are crawling.\n\nDesign goals\u00b6\nbe nicer to sites instead of using default download delay of zero\nautomatically adjust scrapy to the optimum crawling speed, so the user\ndoesn\u2019t have to tune the download delays and concurrent requests to find the\noptimum one. the user only needs to specify the maximum concurrent requests\nit allows, and the extension does the rest.\n\n\nHow it works\u00b6\nIn Scrapy, the download latency is measured as the time elapsed between\nestablishing the TCP connection and receiving the HTTP headers.\nNote that these latencies are very hard to measure accurately in a cooperative\nmultitasking environment because Scrapy may be busy processing a spider\ncallback, for example, and unable to attend downloads. However, these latencies\nshould still give a reasonable estimate of how busy Scrapy (and ultimately, the\nserver) is, and this extension builds on that premise.\n\n\nThrottling algorithm\u00b6\nThis adjusts download delays and concurrency based on the following rules:\nspiders always start with one concurrent request and a download delay of\nAUTOTHROTTLE_START_DELAY\nwhen a response is received, the download delay is adjusted to the\naverage of previous download delay and the latency of the response.\nafter AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD responses have\npassed, the average latency of this period is checked against the previous\none and:if the latency remained constant (within standard deviation limits), it is increased\nif the latency has increased (beyond standard deviation limits) and the concurrency is higher than 1, the concurrency is decreased\n\n\nNote\nThe AutoThrottle extension honours the standard Scrapy settings for\nconcurrency and delay. This means that it will never set a download delay\nlower than DOWNLOAD_DELAY or a concurrency higher than CONCURRENT_REQUESTS_PER_DOMAIN (or CONCURRENT_REQUESTS_PER_IP, depending on which one you use).\n\n\n\nSettings\u00b6\nThe settings used to control the AutoThrottle extension are:\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\nAUTOTHROTTLE_DEBUG\nDOWNLOAD_DELAY\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nFor more information see Throttling algorithm.\n\nAUTOTHROTTLE_ENABLED\u00b6\nDefault: False\nEnables the AutoThrottle extension.\n\n\nAUTOTHROTTLE_START_DELAY\u00b6\nDefault: 5.0\nThe initial download delay (in seconds).\n\n\nAUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\u00b6\nDefault: 10\nHow many responses should pass to perform concurrency adjustments.\n\n\nAUTOTHROTTLE_DEBUG\u00b6\nDefault: False\nEnable AutoThrottle debug mode which will display stats on every response\nreceived, so you can see how the throttling parameters are being adjusted in\nreal time.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/autothrottle.html", "title": ["AutoThrottle extension \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nJobs: pausing and resuming crawls\u00b6\nSometimes, for big sites, it\u2019s desirable to pause crawls and be able to resume\nthem later.\nScrapy supports this functionality out of the box by providing the following\nfacilities:\na scheduler that persists scheduled requests on disk\na duplicates filter that persists visited requests on disk\nan extension that keeps some spider state (key/value pairs) persistent\nbetween batches\n\nJob directory\u00b6\nTo enable persistence support you just need to define a job directory through\nthe JOBDIR setting. This directory will be for storing all required data to\nkeep the state of a single job (ie. a spider run).  It\u2019s important to note that\nthis directory must not be shared by different spiders, or even different\njobs/runs of the same spider, as it\u2019s meant to be used for storing the state of\na single job.\n\n\nHow to use it\u00b6\nTo start a spider with persistence supported enabled, run it like this:\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n\nThen, you can stop the spider safely at any time (by pressing Ctrl-C or sending\na signal), and resume it later by issuing the same command:\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n\n\n\nKeeping persitent state between batches\u00b6\nSometimes you\u2019ll want to keep some persistent spider state between pause/resume\nbatches. You can use the spider.state attribute for that, which should be a\ndict. There\u2019s a built-in extension that takes care of serializing, storing and\nloading that attribute from the job directory, when the spider starts and\nstops.\nHere\u2019s an example of a callback that uses the spider state (other spider code\nis omitted for brevity):\ndef parse_item(self, response):\n    # parse item here\n    self.state['items_count'] = self.state.get('items_count', 0) + 1\n\n\n\n\nPersistence gotchas\u00b6\nThere are a few things to keep in mind if you want to be able to use the Scrapy\npersistence support:\n\nCookies expiration\u00b6\nCookies may expire. So, if you don\u2019t resume your spider quickly the requests\nscheduled may no longer work. This won\u2019t be an issue if you spider doesn\u2019t rely\non cookies.\n\n\nRequest serialization\u00b6\nRequests must be serializable by the pickle module, in order for persistence\nto work, so you should make sure that your requests are serializable.\nThe most common issue here is to use lambda functions on request callbacks that\ncan\u2019t be persisted.\nSo, for example, this won\u2019t work:\ndef some_callback(self, response):\n    somearg = 'test'\n    return Request('http://www.example.com', callback=lambda r: self.other_callback(r, somearg))\n\ndef other_callback(self, response, somearg):\n    print \"the argument passed is:\", somearg\n\n\nBut this will:\ndef some_callback(self, response):\n    somearg = 'test'\n    return Request('http://www.example.com', meta={'somearg': somearg})\n\ndef other_callback(self, response):\n    somearg = response.meta['somearg']\n    print \"the argument passed is:\", somearg\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/jobs.html", "title": ["Jobs: pausing and resuming crawls \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDjangoItem\u00b6\nDjangoItem is a class of item that gets its fields definition from a\nDjango model, you simply create a DjangoItem and specify what Django\nmodel it relates to.\nBesides of getting the model fields defined on your item, DjangoItem\nprovides a method to create and populate a Django model instance with the item\ndata.\n\nUsing DjangoItem\u00b6\nDjangoItem works much like ModelForms in Django, you create a subclass\nand define its django_model attribute to be a valid Django model. With this\nyou will get an item with a field for each Django model field.\nIn addition, you can define fields that aren\u2019t present in the model and even\noverride fields that are present in the model defining them in the item.\nLet\u2019s see some examples:\nDjango model for the examples:\nclass Person(models.Model):\n    name = models.CharField(max_length=255)\n    age = models.IntegerField()\n\n\nDefining a basic DjangoItem:\nclass PersonItem(DjangoItem):\n    django_model = Person\n\n\nDjangoItem work just like Item:\np = PersonItem()\np['name'] = 'John'\np['age'] = '22'\n\n\nTo obtain the Django model from the item, we call the extra method\nsave() of the DjangoItem:\n>>> person = p.save()\n>>> person.name\n'John'\n>>> person.age\n'22'\n>>> person.id\n1\n\n\nAs you see the model is already saved when we call save(), we\ncan prevent this by calling it with commit=False. We can use\ncommit=False in save() method to obtain an unsaved model:\n>>> person = p.save(commit=False)\n>>> person.name\n'John'\n>>> person.age\n'22'\n>>> person.id\nNone\n\n\nAs said before, we can add other fields to the item:\nclass PersonItem(DjangoItem):\n    django_model = Person\n    sex = Field()\n\np = PersonItem()\np['name'] = 'John'\np['age'] = '22'\np['sex'] = 'M'\n\n\n\nNote\nfields added to the item won\u2019t be taken into account when doing a save()\n\nAnd we can override the fields of the model with your own:\nclass PersonItem(DjangoItem):\n    django_model = Person\n    name = Field(default='No Name')\n\n\nThis is useful to provide properties to the field, like a default or any other\nproperty that your project uses.\n\n\nDjangoItem caveats\u00b6\nDjangoItem is a rather convenient way to integrate Scrapy projects with Django\nmodels, but bear in mind that Django ORM may not scale well if you scrape a lot\nof items (ie. millions) with Scrapy. This is because a relational backend is\noften not a good choice for a write intensive application (such as a web\ncrawler), specially if the database is highly normalized and with many indices.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/djangoitem.html", "title": ["DjangoItem \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nArchitecture overview\u00b6\nThis document describes the architecture of Scrapy and how its components\ninteract.\n\nOverview\u00b6\nThe following diagram shows an overview of the Scrapy architecture with its\ncomponents and an outline of the data flow that takes place inside the system\n(shown by the green arrows). A brief description of the components is included\nbelow with links for more detailed information about them. The data flow is\nalso described below.\n\n\n\nComponents\u00b6\n\nScrapy Engine\u00b6\nThe engine is responsible for controlling the data flow between all components\nof the system, and triggering events when certain actions occur. See the Data\nFlow section below for more details.\n\n\nScheduler\u00b6\nThe Scheduler receives requests from the engine and enqueues them for feeding\nthem later (also to the engine) when the engine requests them.\n\n\nDownloader\u00b6\nThe Downloader is responsible for fetching web pages and feeding them to the\nengine which, in turn, feeds them to the spiders.\n\n\nSpiders\u00b6\nSpiders are custom classes written by Scrapy users to parse responses and\nextract items (aka scraped items) from them or additional URLs (requests) to\nfollow. Each spider is able to handle a specific domain (or group of domains).\nFor more information see Spiders.\n\n\nItem Pipeline\u00b6\nThe Item Pipeline is responsible for processing the items once they have been\nextracted (or scraped) by the spiders. Typical tasks include cleansing,\nvalidation and persistence (like storing the item in a database). For more\ninformation see Item Pipeline.\n\n\nDownloader middlewares\u00b6\nDownloader middlewares are specific hooks that sit between the Engine and the\nDownloader and process requests when they pass from the Engine to the\nDownloader, and responses that pass from Downloader to the Engine. They provide\na convenient mechanism for extending Scrapy functionality by plugging custom\ncode. For more information see Downloader Middleware.\n\n\nSpider middlewares\u00b6\nSpider middlewares are specific hooks that sit between the Engine and the\nSpiders and are able to process spider input (responses) and output (items and\nrequests). They provide a convenient mechanism for extending Scrapy\nfunctionality by plugging custom code. For more information see\nSpider Middleware.\n\n\n\nData flow\u00b6\nThe data flow in Scrapy is controlled by the execution engine, and goes like\nthis:\nThe Engine opens a domain, locates the Spider that handles that domain, and\nasks the spider for the first URLs to crawl.\nThe Engine gets the first URLs to crawl from the Spider and schedules them\nin the Scheduler, as Requests.\nThe Engine asks the Scheduler for the next URLs to crawl.\nThe Scheduler returns the next URLs to crawl to the Engine and the Engine\nsends them to the Downloader, passing through the Downloader Middleware\n(request direction).\nOnce the page finishes downloading the Downloader generates a Response (with\nthat page) and sends it to the Engine, passing through the Downloader\nMiddleware (response direction).\nThe Engine receives the Response from the Downloader and sends it to the\nSpider for processing, passing through the Spider Middleware (input direction).\nThe Spider processes the Response and returns scraped Items and new Requests\n(to follow) to the Engine.\nThe Engine sends scraped Items (returned by the Spider) to the Item Pipeline\nand Requests (returned by spider) to the Scheduler\nThe process repeats (from step 2) until there are no more requests from the\nScheduler, and the Engine closes the domain.\n\n\nEvent-driven networking\u00b6\nScrapy is written with Twisted, a popular event-driven networking framework\nfor Python. Thus, it\u2019s implemented using a non-blocking (aka asynchronous) code\nfor concurrency.\nFor more information about asynchronous programming and Twisted see these\nlinks:\nAsynchronous Programming with Twisted\nTwisted - hello, asynchronous programming\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/architecture.html", "title": ["Architecture overview \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nDownloader Middleware\u00b6\nThe downloader middleware is a framework of hooks into Scrapy\u2019s\nrequest/response processing.  It\u2019s a light, low-level system for globally\naltering Scrapy\u2019s requests and responses.\n\nActivating a downloader middleware\u00b6\nTo activate a downloader middleware component, add it to the\nDOWNLOADER_MIDDLEWARES setting, which is a dict whose keys are the\nmiddleware class paths and their values are the middleware orders.\nHere\u2019s an example:\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.middlewares.CustomDownloaderMiddleware': 543,\n}\n\n\nThe DOWNLOADER_MIDDLEWARES setting is merged with the\nDOWNLOADER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to\nbe overridden) and then sorted by order to get the final sorted list of enabled\nmiddlewares: the first middleware is the one closer to the engine and the last\nis the one closer to the downloader.\nTo decide which order to assign to your middleware see the\nDOWNLOADER_MIDDLEWARES_BASE setting and pick a value according to\nwhere you want to insert the middleware. The order does matter because each\nmiddleware performs a different action and your middleware could depend on some\nprevious (or subsequent) middleware being applied.\nIf you want to disable a built-in middleware (the ones defined in\nDOWNLOADER_MIDDLEWARES_BASE and enabled by default) you must define it\nin your project\u2019s DOWNLOADER_MIDDLEWARES setting and assign None\nas its value.  For example, if you want to disable the off-site middleware:\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.middlewares.CustomDownloaderMiddleware': 543,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n}\n\n\nFinally, keep in mind that some middlewares may need to be enabled through a\nparticular setting. See each middleware documentation for more info.\n\n\nWriting your own downloader middleware\u00b6\nWriting your own downloader middleware is easy. Each middleware component is a\nsingle Python class that defines one or more of the following methods:\n\nclass scrapy.contrib.downloadermiddleware.DownloaderMiddleware\u00b6\n\nprocess_request(request, spider)\u00b6\nThis method is called for each request that goes through the download\nmiddleware.\nprocess_request() should return either None, a\nResponse object, or a Request\nobject.\nIf it returns None, Scrapy will continue processing this request, executing all\nother middlewares until, finally, the appropriate downloader handler is called\nthe request performed (and its response downloaded).\nIf it returns a Response object, Scrapy won\u2019t bother\ncalling ANY other request or exception middleware, or the appropriate\ndownload function; it\u2019ll return that Response. Response middleware is\nalways called on every Response.\nIf it returns a Request object, the returned request will be\nrescheduled (in the Scheduler) to be downloaded in the future. The callback of\nthe original request will always be called. If the new request has a callback\nit will be called with the response downloaded, and the output of that callback\nwill then be passed to the original callback. If the new request doesn\u2019t have a\ncallback, the response downloaded will be just passed to the original request\ncallback.\nIf it returns an IgnoreRequest exception, the\nentire request will be dropped completely and its callback never called.\nParameters:request (Request object) \u2013 the request being processed\nspider (BaseSpider object) \u2013 the spider for which this request is intended\n\n\nprocess_response(request, response, spider)\u00b6\nprocess_response() should return a Response\nobject or raise a IgnoreRequest exception.\nIf it returns a Response (it could be the same given\nresponse, or a brand-new one), that response will continue to be processed\nwith the process_response() of the next middleware in the pipeline.\nIf it returns an IgnoreRequest exception, the\nresponse will be dropped completely and its callback never called.\nParameters:request (is a Request object) \u2013 the request that originated the response\nresponse (Response object) \u2013 the response being processed\nspider (BaseSpider object) \u2013 the spider for which this response is intended\n\n\nprocess_exception(request, exception, spider)\u00b6\nScrapy calls process_exception() when a download handler\nor a process_request() (from a downloader middleware) raises an\nexception.\nprocess_exception() should return either None,\nResponse or Request object.\nIf it returns None, Scrapy will continue processing this exception,\nexecuting any other exception middleware, until no middleware is left and\nthe default exception handling kicks in.\nIf it returns a Response object, the response middleware\nkicks in, and won\u2019t bother calling any other exception middleware.\nIf it returns a Request object, the returned request is\nused to instruct an immediate redirection.\nThe original request won\u2019t finish until the redirected\nrequest is completed. This stops the process_exception()\nmiddleware the same as returning Response would do.\nParameters:request (is a Request object) \u2013 the request that generated the exception\nexception (an Exception object) \u2013 the raised exception\nspider (BaseSpider object) \u2013 the spider for which this request is intended\n\n\n\nBuilt-in downloader middleware reference\u00b6\nThis page describes all downloader middleware components that come with\nScrapy. For information on how to use them and how to write your own downloader\nmiddleware, see the downloader middleware usage guide.\nFor a list of the components enabled by default (and their orders) see the\nDOWNLOADER_MIDDLEWARES_BASE setting.\n\nCookiesMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware\u00b6\nThis middleware enables working with sites that require cookies, such as\nthose that use sessions. It keeps track of cookies sent by web servers, and\nsend them back on subsequent requests (from that spider), just like web\nbrowsers do.\nThe following settings can be used to configure the cookie middleware:\nCOOKIES_ENABLED\nCOOKIES_DEBUG\n\nMultiple cookie sessions per spider\u00b6\n\nNew in version 0.15.\nThere is support for keeping multiple cookie sessions per spider by using the\ncookiejar Request meta key. By default it uses a single cookie jar\n(session), but you can pass an identifier to use different ones.\nFor example:\nfor i, url in enumerate(urls):\n    yield Request(\"http://www.example.com\", meta={'cookiejar': i},\n        callback=self.parse_page)\n\n\nKeep in mind that the cookiejar meta key is not \u201csticky\u201d. You need to keep\npassing it along on subsequent requests. For example:\ndef parse_page(self, response):\n    # do some processing\n    return Request(\"http://www.example.com/otherpage\",\n        meta={'cookiejar': response.meta['cookiejar']},\n        callback=self.parse_other_page)\n\n\n\n\nCOOKIES_ENABLED\u00b6\nDefault: True\nWhether to enable the cookies middleware. If disabled, no cookies will be sent\nto web servers.\n\n\nCOOKIES_DEBUG\u00b6\nDefault: False\nIf enabled, Scrapy will log all cookies sent in requests (ie. Cookie\nheader) and all cookies received in responses (ie. Set-Cookie header).\nHere\u2019s an example of a log with COOKIES_DEBUG enabled:\n2011-04-06 14:35:10-0300 [diningcity] INFO: Spider opened\n2011-04-06 14:35:10-0300 [diningcity] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>\n        Cookie: clientlanguage_nl=en_EN\n2011-04-06 14:35:14-0300 [diningcity] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>\n        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/\n        Set-Cookie: ip_isocode=US\n        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/\n2011-04-06 14:49:50-0300 [diningcity] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)\n[...]\n\n\n\n\nDefaultHeadersMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware\u00b6\nThis middleware sets all default requests headers specified in the\nDEFAULT_REQUEST_HEADERS setting.\n\n\nDownloadTimeoutMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware\u00b6\nThis middleware sets the download timeout for requests specified in the\nDOWNLOAD_TIMEOUT setting.\n\n\nHttpAuthMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware\u00b6\nThis middleware authenticates all requests generated from certain spiders\nusing Basic access authentication (aka. HTTP auth).\nTo enable HTTP authentication from certain spiders, set the http_user\nand http_pass attributes of those spiders.\nExample:\nclass SomeIntranetSiteSpider(CrawlSpider):\n\n    http_user = 'someuser'\n    http_pass = 'somepass'\n    name = 'intranet.example.com'\n\n    # .. rest of the spider code omitted ...\n\n\n\n\nHttpCacheMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\u00b6\nThis middleware provides low-level cache to all HTTP requests and responses.\nEvery request and its corresponding response are cached. When the same\nrequest is seen again, the response is returned without transferring\nanything from the Internet.\nThe HTTP cache is useful for testing spiders faster (without having to wait for\ndownloads every time) and for trying your spider offline, when an Internet\nconnection is not available.\nScrapy ships with two storage backends for the HTTP cache middleware:\nDBM storage backend (default)\nFile system backend\nYou can change the storage backend with the HTTPCACHE_STORAGE\nsetting. Or you can also implement your own backend.\n\nDBM storage backend (default)\u00b6\n\nNew in version 0.13.\nA DBM storage backend is available for the HTTP cache middleware. To use it\n(note: it is the default storage backend) set HTTPCACHE_STORAGE\nto scrapy.contrib.httpcache.DbmCacheStorage.\nBy default, it uses the anydbm module, but you can change it with the\nHTTPCACHE_DBM_MODULE setting.\n\n\nFile system backend\u00b6\nA file system storage backend is also available for the HTTP cache middleware.\nTo use it (instead of the default DBM storage backend) set HTTPCACHE_STORAGE\nto scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage.\nEach request/response pair is stored in a different directory containing\nthe following files:\n\nrequest_body - the plain request body\nrequest_headers - the request headers (in raw HTTP format)\nresponse_body - the plain response body\nresponse_headers - the request headers (in raw HTTP format)\nmeta - some metadata of this cache resource in Python repr() format\n(grep-friendly format)\npickled_meta - the same metadata in meta but pickled for more\nefficient deserialization\n\nThe directory name is made from the request fingerprint (see\nscrapy.utils.request.fingerprint), and one level of subdirectories is\nused to avoid creating too many files into the same directory (which is\ninefficient in many file systems). An example directory could be:\n/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7\n\n\n\nHTTPCache middleware settings\u00b6\nThe HttpCacheMiddleware can be configured through the following\nsettings:\n\nHTTPCACHE_ENABLED\u00b6\n\nNew in version 0.11.\nDefault: False\nWhether the HTTP cache will be enabled.\n\nChanged in version 0.11: Before 0.11, HTTPCACHE_DIR was used to enable cache.\n\n\nHTTPCACHE_EXPIRATION_SECS\u00b6\nDefault: 0\nExpiration time for cached requests, in seconds.\nCached requests older than this time will be re-downloaded. If zero, cached\nrequests will never expire.\n\nChanged in version 0.11: Before 0.11, zero meant cached requests always expire.\n\n\nHTTPCACHE_DIR\u00b6\nDefault: 'httpcache'\nThe directory to use for storing the (low-level) HTTP cache. If empty, the HTTP\ncache will be disabled. If a relative path is given, is taken relative to the\nproject data dir. For more info see: Default structure of Scrapy projects.\n\n\nHTTPCACHE_IGNORE_HTTP_CODES\u00b6\n\nNew in version 0.10.\nDefault: []\nDon\u2019t cache response with these HTTP codes.\n\n\nHTTPCACHE_IGNORE_MISSING\u00b6\nDefault: False\nIf enabled, requests not found in the cache will be ignored instead of downloaded.\n\n\nHTTPCACHE_IGNORE_SCHEMES\u00b6\n\nNew in version 0.10.\nDefault: ['file']\nDon\u2019t cache responses with these URI schemes.\n\n\nHTTPCACHE_STORAGE\u00b6\nDefault: 'scrapy.contrib.httpcache.DbmCacheStorage'\nThe class which implements the cache storage backend.\n\n\nHTTPCACHE_DBM_MODULE\u00b6\n\nNew in version 0.13.\nDefault: 'anydbm'\nThe database module to use in the DBM storage backend. This setting is specific to the DBM backend.\n\n\n\n\nHttpCompressionMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware\u00b6\nThis middleware allows compressed (gzip, deflate) traffic to be\nsent/received from web sites.\n\n\nChunkedTransferMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware\u00b6\nThis middleware adds support for chunked transfer encoding\n\n\nHttpProxyMiddleware\u00b6\n\nNew in version 0.8.\n\nclass scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware\u00b6\nThis middleware sets the HTTP proxy to use for requests, by setting the\nproxy meta value to Request objects.\nLike the Python standard library modules urllib and urllib2, it obeys\nthe following environment variables:\nhttp_proxy\nhttps_proxy\nno_proxy\n\n\nRedirectMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware\u00b6\nThis middleware handles redirection of requests based on response status and\nmeta-refresh html tag.\nThe urls which the request goes through (while being redirected) can be found\nin the redirect_urls Request.meta key.\nThe RedirectMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\nREDIRECT_ENABLED\nREDIRECT_MAX_TIMES\nREDIRECT_MAX_METAREFRESH_DELAY\nIf Request.meta contains the\ndont_redirect key, the request will be ignored by this middleware.\n\nRedirectMiddleware settings\u00b6\n\nREDIRECT_ENABLED\u00b6\n\nNew in version 0.13.\nDefault: True\nWhether the Redirect middleware will be enabled.\n\n\nREDIRECT_MAX_TIMES\u00b6\nDefault: 20\nThe maximum number of redirections that will be follow for a single request.\n\n\nREDIRECT_MAX_METAREFRESH_DELAY\u00b6\nDefault: 100\nThe maximum meta-refresh delay (in seconds) to follow the redirection.\n\n\n\n\nRetryMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.retry.RetryMiddleware\u00b6\nA middlware to retry failed requests that are potentially caused by\ntemporary problems such as a connection timeout or HTTP 500 error.\nFailed pages are collected on the scraping process and rescheduled at the\nend, once the spider has finished crawling all regular (non failed) pages.\nOnce there are no more failed pages to retry, this middleware sends a signal\n(retry_complete), so other extensions could connect to that signal.\nThe RetryMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\nRETRY_ENABLED\nRETRY_TIMES\nRETRY_HTTP_CODES\nAbout HTTP errors to consider:\nYou may want to remove 400 from RETRY_HTTP_CODES, if you stick to the\nHTTP protocol. It\u2019s included by default because it\u2019s a common code used\nto indicate server overload, which would be something we want to retry.\nIf Request.meta contains the dont_retry\nkey, the request will be ignored by this middleware.\n\nRetryMiddleware Settings\u00b6\n\nRETRY_ENABLED\u00b6\n\nNew in version 0.13.\nDefault: True\nWhether the Retry middleware will be enabled.\n\n\nRETRY_TIMES\u00b6\nDefault: 2\nMaximum number of times to retry, in addition to the first download.\n\n\nRETRY_HTTP_CODES\u00b6\nDefault: [500, 503, 504, 400, 408]\nWhich HTTP response codes to retry. Other errors (DNS lookup issues,\nconnections lost, etc) are always retried.\n\n\n\n\nRobotsTxtMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware\u00b6\nThis middleware filters out requests forbidden by the robots.txt exclusion\nstandard.\nTo make sure Scrapy respects robots.txt make sure the middleware is enabled\nand the ROBOTSTXT_OBEY setting is enabled.\n\nWarning\nKeep in mind that, if you crawl using multiple concurrent\nrequests per domain, Scrapy could still  download some forbidden pages\nif they were requested before the robots.txt file was downloaded. This\nis a known limitation of the current robots.txt middleware and will\nbe fixed in the future.\n\n\n\nDownloaderStats\u00b6\n\nclass scrapy.contrib.downloadermiddleware.stats.DownloaderStats\u00b6\nMiddleware that stores stats of all requests, responses and exceptions that\npass through it.\nTo use this middleware you must enable the DOWNLOADER_STATS\nsetting.\n\n\nUserAgentMiddleware\u00b6\n\nclass scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware\u00b6\nMiddleware that allows spiders to override the default user agent.\nIn order for a spider to override the default user agent, its user_agent\nattribute must be set.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/downloader-middleware.html", "title": ["Downloader Middleware \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSpider Middleware\u00b6\nThe spider middleware is a framework of hooks into Scrapy\u2019s spider processing\nmechanism where you can plug custom functionality to process the requests that\nare sent to Spiders for processing and to process the responses\nand items that are generated from spiders.\n\nActivating a spider middleware\u00b6\nTo activate a spider middleware component, add it to the\nSPIDER_MIDDLEWARES setting, which is a dict whose keys are the\nmiddleware class path and their values are the middleware orders.\nHere\u2019s an example:\nSPIDER_MIDDLEWARES = {\n    'myproject.middlewares.CustomSpiderMiddleware': 543,\n}\n\n\nThe SPIDER_MIDDLEWARES setting is merged with the\nSPIDER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to\nbe overridden) and then sorted by order to get the final sorted list of enabled\nmiddlewares: the first middleware is the one closer to the engine and the last\nis the one closer to the spider.\nTo decide which order to assign to your middleware see the\nSPIDER_MIDDLEWARES_BASE setting and pick a value according to where\nyou want to insert the middleware. The order does matter because each\nmiddleware performs a different action and your middleware could depend on some\nprevious (or subsequent) middleware being applied.\nIf you want to disable a builtin middleware (the ones defined in\nSPIDER_MIDDLEWARES_BASE, and enabled by default) you must define it\nin your project SPIDER_MIDDLEWARES setting and assign None as its\nvalue.  For example, if you want to disable the off-site middleware:\nSPIDER_MIDDLEWARES = {\n    'myproject.middlewares.CustomSpiderMiddleware': 543,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': None,\n}\n\n\nFinally, keep in mind that some middlewares may need to be enabled through a\nparticular setting. See each middleware documentation for more info.\n\n\nWriting your own spider middleware\u00b6\nWriting your own spider middleware is easy. Each middleware component is a\nsingle Python class that defines one or more of the following methods:\n\nclass scrapy.contrib.spidermiddleware.SpiderMiddleware\u00b6\n\nprocess_spider_input(response, spider)\u00b6\nThis method is called for each response that goes through the spider\nmiddleware and into the spider, for processing.\nprocess_spider_input() should return None or raise an\nexception.\nIf it returns None, Scrapy will continue processing this response,\nexecuting all other middlewares until, finally, the response is handled\nto the spider for processing.\nIf it raises an exception, Scrapy won\u2019t bother calling any other spider\nmiddleware process_spider_input() and will call the request\nerrback.  The output of the errback is chained back in the other\ndirection for process_spider_output() to process it, or\nprocess_spider_exception() if it raised an exception.\nParameters:response (Response object) \u2013 the response being processed\nspider (BaseSpider object) \u2013 the spider for which this response is intended\n\n\nprocess_spider_output(response, result, spider)\u00b6\nThis method is called with the results returned from the Spider, after\nit has processed the response.\nprocess_spider_output() must return an iterable of\nRequest or Item objects.\nParameters:response (class:~scrapy.http.Response object) \u2013 the response which generated this output from the\nspider\nresult (an iterable of Request or\nItem objects) \u2013 the result returned by the spider\nspider (BaseSpider object) \u2013 the spider whose result is being processed\n\n\nprocess_spider_exception(response, exception, spider)\u00b6\nThis method is called when when a spider or process_spider_input()\nmethod (from other spider middleware) raises an exception.\nprocess_spider_exception() should return either None or an\niterable of Response or\nItem objects.\nIf it returns None, Scrapy will continue processing this exception,\nexecuting any other process_spider_exception() in the following\nmiddleware components, until no middleware components are left and the\nexception reaches the engine (where it\u2019s logged and discarded).\nIf it returns an iterable the process_spider_output() pipeline\nkicks in, and no other process_spider_exception() will be called.\nParameters:response (Response object) \u2013 the response being processed when the exception was\nraised\nexception (Exception object) \u2013 the exception raised\nspider (scrapy.spider.BaseSpider object) \u2013 the spider which raised the exception\n\n\nprocess_start_requests(start_requests, spider)\u00b6\n\nNew in version 0.15.\nThis method is called with the start requests of the spider, and works\nsimilarly to the process_spider_output() method, except that it\ndoesn\u2019t have a response associated and must return only requests (not\nitems).\nIt receives an iterable (in the start_requests parameter) and must\nreturn another iterable of Request objects.\n\nNote\nWhen implementing this method in your spider middleware, you\nshould always return an iterable (that follows the input one) and\nnot consume all start_requests iterator because it can be very\nlarge (or even unbounded) and cause a memory overflow. The Scrapy\nengine is designed to pull start requests while it has capacity to\nprocess them, so the start requests iterator can be effectively\nendless where there is some other condition for stopping the spider\n(like a time limit or item/page count).\n\nParameters:start_requests (an iterable of Request) \u2013 the start requests\nspider (BaseSpider object) \u2013 the spider to whom the start requests belong\n\n\n\nBuilt-in spider middleware reference\u00b6\nThis page describes all spider middleware components that come with Scrapy. For\ninformation on how to use them and how to write your own spider middleware, see\nthe spider middleware usage guide.\nFor a list of the components enabled by default (and their orders) see the\nSPIDER_MIDDLEWARES_BASE setting.\n\nDepthMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.depth.DepthMiddleware\u00b6\nDepthMiddleware is a scrape middleware used for tracking the depth of each\nRequest inside the site being scraped. It can be used to limit the maximum\ndepth to scrape or things like that.\nThe DepthMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\n\nDEPTH_LIMIT - The maximum depth that will be allowed to\ncrawl for any site. If zero, no limit will be imposed.\nDEPTH_STATS - Whether to collect depth stats.\nDEPTH_PRIORITY - Whether to prioritize the requests based on\ntheir depth.\n\n\n\nHttpErrorMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware\u00b6\nFilter out unsuccessful (erroneous) HTTP responses so that spiders don\u2019t\nhave to deal with them, which (most of the time) imposes an overhead,\nconsumes more resources, and makes the spider logic more complex.\nAccording to the HTTP standard, successful responses are those whose\nstatus codes are in the 200-300 range.\nIf you still want to process response codes outside that range, you can\nspecify which response codes the spider is able to handle using the\nhandle_httpstatus_list spider attribute.\nFor example, if you want your spider to handle 404 responses you can do\nthis:\nclass MySpider(CrawlSpider):\n    handle_httpstatus_list = [404]\n\n\nThe handle_httpstatus_list key of Request.meta can also be used to specify which response codes to\nallow on a per-request basis.\nKeep in mind, however, that it\u2019s usually a bad idea to handle non-200\nresponses, unless you really know what you\u2019re doing.\nFor more information see: HTTP Status Code Definitions.\n\n\nOffsiteMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware\u00b6\nFilters out Requests for URLs outside the domains covered by the spider.\nThis middleware filters out every request whose host names aren\u2019t in the\nspider\u2019s allowed_domains attribute.\nWhen your spider returns a request for a domain not belonging to those\ncovered by the spider, this middleware will log a debug message similar to\nthis one:\nDEBUG: Filtered offsite request to 'www.othersite.com': <GET http://www.othersite.com/some/page.html>\n\nTo avoid filling the log with too much noise, it will only print one of\nthese messages for each new domain filtered. So, for example, if another\nrequest for www.othersite.com is filtered, no log message will be\nprinted. But if a request for someothersite.com is filtered, a message\nwill be printed (but only for the first request filtered).\nIf the spider doesn\u2019t define an\nallowed_domains attribute, or the\nattribute is empty, the offsite middleware will allow all requests.\nIf the request has the dont_filter attribute\nset, the offsite middleware will allow the request even if its domain is not\nlisted in allowed domains.\n\n\nRefererMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.referer.RefererMiddleware\u00b6\nPopulates Request referer field, based on the Response which originated it.\n\nRefererMiddleware settings\u00b6\n\nREFERER_ENABLED\u00b6\n\nNew in version 0.15.\nDefault: True\nWhether to enable referer middleware.\n\n\n\n\nUrlLengthMiddleware\u00b6\n\nclass scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware\u00b6\nFilters out requests with URLs longer than URLLENGTH_LIMIT\nThe UrlLengthMiddleware can be configured through the following\nsettings (see the settings documentation for more info):\n\nURLLENGTH_LIMIT - The maximum URL length to allow for crawled URLs.\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/spider-middleware.html", "title": ["Spider Middleware \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExtensions\u00b6\nThe extensions framework provides a mechanism for inserting your own\ncustom functionality into Scrapy.\nExtensions are just regular classes that are instantiated at Scrapy startup,\nwhen extensions are initialized.\n\nExtension settings\u00b6\nExtensions use the Scrapy settings to manage their\nsettings, just like any other Scrapy code.\nIt is customary for extensions to prefix their settings with their own name, to\navoid collision with existing (and future) extensions. For example, an\nhypothetic extension to handle Google Sitemaps would use settings like\nGOOGLESITEMAP_ENABLED, GOOGLESITEMAP_DEPTH, and so on.\n\n\nLoading & activating extensions\u00b6\nExtensions are loaded and activated at startup by instantiating a single\ninstance of the extension class. Therefore, all the extension initialization\ncode must be performed in the class constructor (__init__ method).\nTo make an extension available, add it to the EXTENSIONS setting in\nyour Scrapy settings. In EXTENSIONS, each extension is represented\nby a string: the full Python path to the extension\u2019s class name. For example:\nEXTENSIONS = {\n    'scrapy.contrib.corestats.CoreStats': 500,\n    'scrapy.webservice.WebService': 500,\n    'scrapy.telnet.TelnetConsole': 500,\n}\n\n\nAs you can see, the EXTENSIONS setting is a dict where the keys are\nthe extension paths, and their values are the orders, which define the\nextension loading order. Extensions orders are not as important as middleware\norders though, and they are typically irrelevant, ie. it doesn\u2019t matter in\nwhich order the extensions are loaded because they don\u2019t depend on each other\n[1].\nHowever, this feature can be exploited if you need to add an extension which\ndepends on other extensions already loaded.\n[1] This is is why the EXTENSIONS_BASE setting in Scrapy (which\ncontains all built-in extensions enabled by default) defines all the extensions\nwith the same order (500).\n\n\nAvailable, enabled and disabled extensions\u00b6\nNot all available extensions will be enabled. Some of them usually depend on a\nparticular setting. For example, the HTTP Cache extension is available by default\nbut disabled unless the HTTPCACHE_ENABLED setting is set.\n\n\nDisabling an extension\u00b6\nIn order to disable an extension that comes enabled by default (ie. those\nincluded in the EXTENSIONS_BASE setting) you must set its order to\nNone. For example:\nEXTENSIONS = {\n    'scrapy.contrib.corestats.CoreStats': None,\n}\n\n\n\n\nWriting your own extension\u00b6\nWriting your own extension is easy. Each extension is a single Python class\nwhich doesn\u2019t need to implement any particular method.\nThe main entry point for a Scrapy extension (this also includes middlewares and\npipelines) is the from_crawler class method which receives a\nCrawler instance which is the main object controlling the Scrapy crawler.\nThrough that object you can access settings, signals, stats, and also control\nthe crawler behaviour, if your extension needs to such thing.\nTypically, extensions connect to signals and perform\ntasks triggered by them.\nFinally, if the from_crawler method raises the\nNotConfigured exception, the extension will be\ndisabled. Otherwise, the extension will be enabled.\n\nSample extension\u00b6\nHere we will implement a simple extension to illustrate the concepts described\nin the previous section. This extension will log a message every time:\na spider is opened\na spider is closed\na specific number of items are scraped\nThe extension will be enabled through the MYEXT_ENABLED setting and the\nnumber of items will be specified through the MYEXT_ITEMCOUNT setting.\nHere is the code of such extension:\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\n\nclass SpiderOpenCloseLogging(object):\n\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool('MYEXT_ENABLED'):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint('MYEXT_ITEMCOUNT', 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        spider.log(\"opened spider %s\" % spider.name)\n\n    def spider_closed(self, spider):\n        spider.log(\"closed spider %s\" % spider.name)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped == self.item_count:\n            spider.log(\"scraped %d items, resetting counter\" % self.items_scraped)\n            self.item_count = 0\n\n\n\n\n\nBuilt-in extensions reference\u00b6\n\nGeneral purpose extensions\u00b6\n\nLog Stats extension\u00b6\n\nclass scrapy.contrib.logstats.LogStats\u00b6\nLog basic stats like crawled pages and scraped items.\n\n\nCore Stats extension\u00b6\n\nclass scrapy.contrib.corestats.CoreStats\u00b6\nEnable the collection of core statistics, provided the stats collection is\nenabled (see Stats Collection).\n\n\nWeb service extension\u00b6\n\nclass scrapy.webservice.WebService\u00b6\nSee topics-webservice.\n\n\nTelnet console extension\u00b6\n\nclass scrapy.telnet.TelnetConsole\u00b6\nProvides a telnet console for getting into a Python interpreter inside the\ncurrently running Scrapy process, which can be very useful for debugging.\nThe telnet console must be enabled by the TELNETCONSOLE_ENABLED\nsetting, and the server will listen in the port specified in\nTELNETCONSOLE_PORT.\n\n\nMemory usage extension\u00b6\n\nclass scrapy.contrib.memusage.MemoryUsage\u00b6\n\nNote\nThis extension does not work in Windows.\n\nMonitors the memory used by the Scrapy process that runs the spider and:\n1, sends a notification e-mail when it exceeds a certain value\n2. closes the spider when it exceeds a certain value\nThe notification e-mails can be triggered when a certain warning value is\nreached (MEMUSAGE_WARNING_MB) and when the maximum value is reached\n(MEMUSAGE_LIMIT_MB) which will also cause the spider to be closed\nand the Scrapy process to be terminated.\nThis extension is enabled by the MEMUSAGE_ENABLED setting and\ncan be configured with the following settings:\nMEMUSAGE_LIMIT_MB\nMEMUSAGE_WARNING_MB\nMEMUSAGE_NOTIFY_MAIL\nMEMUSAGE_REPORT\n\n\nMemory debugger extension\u00b6\n\nclass scrapy.contrib.memdebug.MemoryDebugger\u00b6\nAn extension for debugging memory usage. It collects information about:\nobjects uncollected by the Python garbage collector\nlibxml2 memory leaks\nobjects left alive that shouldn\u2019t. For more info, see Debugging memory leaks with trackref\nTo enable this extension, turn on the MEMDEBUG_ENABLED setting. The\ninfo will be stored in the stats.\n\n\nClose spider extension\u00b6\n\nclass scrapy.contrib.closespider.CloseSpider\u00b6\nCloses a spider automatically when some conditions are met, using a specific\nclosing reason for each condition.\nThe conditions for closing a spider can be configured through the following\nsettings:\nCLOSESPIDER_TIMEOUT\nCLOSESPIDER_ITEMCOUNT\nCLOSESPIDER_PAGECOUNT\nCLOSESPIDER_ERRORCOUNT\n\nCLOSESPIDER_TIMEOUT\u00b6\nDefault: 0\nAn integer which specifies a number of seconds. If the spider remains open for\nmore than that number of second, it will be automatically closed with the\nreason closespider_timeout. If zero (or non set), spiders won\u2019t be closed by\ntimeout.\n\n\nCLOSESPIDER_ITEMCOUNT\u00b6\nDefault: 0\nAn integer which specifies a number of items. If the spider scrapes more than\nthat amount if items and those items are passed by the item pipeline, the\nspider will be closed with the reason closespider_itemcount. If zero (or\nnon set), spiders won\u2019t be closed by number of passed items.\n\n\nCLOSESPIDER_PAGECOUNT\u00b6\n\nNew in version 0.11.\nDefault: 0\nAn integer which specifies the maximum number of responses to crawl. If the spider\ncrawls more than that, the spider will be closed with the reason\nclosespider_pagecount. If zero (or non set), spiders won\u2019t be closed by\nnumber of crawled responses.\n\n\nCLOSESPIDER_ERRORCOUNT\u00b6\n\nNew in version 0.11.\nDefault: 0\nAn integer which specifies the maximum number of errors to receive before\nclosing the spider. If the spider generates more than that number of errors,\nit will be closed with the reason closespider_errorcount. If zero (or non\nset), spiders won\u2019t be closed by number of errors.\n\n\n\nStatsMailer extension\u00b6\n\nclass scrapy.contrib.statsmailer.StatsMailer\u00b6\nThis simple extension can be used to send a notification e-mail every time a\ndomain has finished scraping, including the Scrapy stats collected. The email\nwill be sent to all recipients specified in the STATSMAILER_RCPTS\nsetting.\n\n\n\nDebugging extensions\u00b6\n\nStack trace dump extension\u00b6\n\nclass scrapy.contrib.debug.StackTraceDump\u00b6\nDumps information about the running process when a SIGQUIT or SIGUSR2\nsignal is received. The information dumped is the following:\nengine status (using scrapy.utils.engine.get_engine_status())\nlive references (see Debugging memory leaks with trackref)\nstack trace of all threads\nAfter the stack trace and engine status is dumped, the Scrapy process continues\nrunning normally.\nThis extension only works on POSIX-compliant platforms (ie. not Windows),\nbecause the SIGQUIT and SIGUSR2 signals are not available on Windows.\nThere are at least two ways to send Scrapy the SIGQUIT signal:\nBy pressing Ctrl-while a Scrapy process is running (Linux only?)\n\nBy running this command (assuming <pid> is the process id of the Scrapy\nprocess):\nkill -QUIT <pid>\n\n\n\n\nDebugger extension\u00b6\n\nclass scrapy.contrib.debug.Debugger\u00b6\nInvokes a Python debugger inside a running Scrapy process when a SIGUSR2\nsignal is received. After the debugger is exited, the Scrapy process continues\nrunning normally.\nFor more info see Debugging in Python.\nThis extension only works on POSIX-compliant platforms (ie. not Windows).\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/extensions.html", "title": ["Extensions \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nCore API\u00b6\n\nNew in version 0.15.\nThis section documents the Scrapy core API, and it\u2019s intended for developers of\nextensions and middlewares.\n\nCrawler API\u00b6\nThe main entry point to Scrapy API is the Crawler\nobject, passed to extensions through the from_crawler class method. This\nobject provides access to all Scrapy core components, and it\u2019s the only way for\nextensions to access them and hook their functionality into Scrapy.\nThe Extension Manager is responsible for loading and keeping track of installed\nextensions and it\u2019s configured through the EXTENSIONS setting which\ncontains a dictionary of all available extensions and their order similar to\nhow you configure the downloader middlewares.\n\nclass scrapy.crawler.Crawler(settings)\u00b6\nThe Crawler object must be instantiated with a\nscrapy.settings.Settings object.\n\nsettings\u00b6\nThe settings manager of this crawler.\nThis is used by extensions & middlewares to access the Scrapy settings\nof this crawler.\nFor an introduction on Scrapy settings see Settings.\nFor the API see Settings class.\n\nsignals\u00b6\nThe signals manager of this crawler.\nThis is used by extensions & middlewares to hook themselves into Scrapy\nfunctionality.\nFor an introduction on signals see Signals.\nFor the API see SignalManager class.\n\nstats\u00b6\nThe stats collector of this crawler.\nThis is used from extensions & middlewares to record stats of their\nbehaviour, or access stats collected by other extensions.\nFor an introduction on stats collection see Stats Collection.\nFor the API see StatsCollector class.\n\nextensions\u00b6\nThe extension manager that keeps track of enabled extensions.\nMost extensions won\u2019t need to access this attribute.\nFor an introduction on extensions and a list of available extensions on\nScrapy see Extensions.\n\nspiders\u00b6\nThe spider manager which takes care of loading and instantiating\nspiders.\nMost extensions won\u2019t need to access this attribute.\n\nengine\u00b6\nThe execution engine, which coordinates the core crawling logic\nbetween the scheduler, downloader and spiders.\nSome extension may want to access the Scrapy engine, to modify inspect\nor modify the downloader and scheduler behaviour, although this is an\nadvanced use and this API is not yet stable.\n\nconfigure()\u00b6\nConfigure the crawler.\nThis loads extensions, middlewares and spiders, leaving the crawler\nready to be started. It also configures the execution engine.\n\nstart()\u00b6\nStart the crawler. This calls configure() if it hasn\u2019t been called yet.\n\n\nSettings API\u00b6\n\nclass scrapy.settings.Settings\u00b6\nThis object that provides access to Scrapy settings.\n\noverrides\u00b6\nGlobal overrides are the ones that take most precedence, and are usually\npopulated by command-line options.\nOverrides should be populated before configuring the Crawler object\n(through the configure() method),\notherwise they won\u2019t have any effect. You don\u2019t typically need to worry\nabout overrides unless you are implementing your own Scrapy command.\n\nget(name, default=None)\u00b6\nGet a setting value without affecting its original type.\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetbool(name, default=False)\u00b6\nGet a setting value as a boolean. For example, both 1 and '1', and\nTrue return True, while 0, '0', False and None\nreturn False``\nFor example, settings populated through environment variables set to '0'\nwill return False when using this method.\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetint(name, default=0)\u00b6\nGet a setting value as an int\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetfloat(name, default=0.0)\u00b6\nGet a setting value as a float\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\ngetlist(name, default=None)\u00b6\nGet a setting value as a list. If the setting original type is a list it\nwill be returned verbatim. If it\u2019s a string it will be split by \u201d,\u201d.\nFor example, settings populated through environment variables set to\n'one,two' will return a list [\u2018one\u2019, \u2018two\u2019] when using this method.\nParameters:name (string) \u2013 the setting name\ndefault (any) \u2013 the value to return if no setting is found\n\n\n\nSignals API\u00b6\n\nclass scrapy.signalmanager.SignalManager\u00b6\n\nconnect(receiver, signal)\u00b6\nConnect a receiver function to a signal.\nThe signal can be any object, although Scrapy comes with some\npredefined signals that are documented in the Signals\nsection.\nParameters:receiver (callable) \u2013 the function to be connected\nsignal (object) \u2013 the signal to connect to\n\n\nsend_catch_log(signal, **kwargs)\u00b6\nSend a signal, catch exceptions and log them.\nThe keyword arguments are passed to the signal handlers (connected\nthrough the connect() method).\n\nsend_catch_log_deferred(signal, **kwargs)\u00b6\nLike send_catch_log() but supports returning deferreds from\nsignal handlers.\nReturns a deferred that gets fired once all signal handlers\ndeferreds were fired. Send a signal, catch exceptions and log them.\nThe keyword arguments are passed to the signal handlers (connected\nthrough the connect() method).\n\ndisconnect(receiver, signal)\u00b6\nDisconnect a receiver function from a signal. This has the opposite\neffect of the connect() method, and the arguments are the same.\n\ndisconnect_all(signal)\u00b6\nDisconnect all receivers from the given signal.\nParameters:signal (object) \u2013 the signal to disconnect from\n\n\nStats Collector API\u00b6\nThere are several Stats Collectors available under the\nscrapy.statscol module and they all implement the Stats\nCollector API defined by the StatsCollector\nclass (which they all inherit from).\n\nclass scrapy.statscol.StatsCollector\u00b6\n\nget_value(key, default=None)\u00b6\nReturn the value for the given stats key or default if it doesn\u2019t exist.\n\nget_stats()\u00b6\nGet all stats from the currently running spider as a dict.\n\nset_value(key, value)\u00b6\nSet the given value for the given stats key.\n\nset_stats(stats)\u00b6\nOverride the current stats with the dict passed in stats argument.\n\ninc_value(key, count=1, start=0)\u00b6\nIncrement the value of the given stats key, by the given count,\nassuming the start value given (when it\u2019s not set).\n\nmax_value(key, value)\u00b6\nSet the given value for the given key only if current value for the\nsame key is lower than value. If there is no current value for the\ngiven key, the value is always set.\n\nmin_value(key, value)\u00b6\nSet the given value for the given key only if current value for the\nsame key is greater than value. If there is no current value for the\ngiven key, the value is always set.\n\nclear_stats()\u00b6\nClear all stats.\nThe following methods are not part of the stats collection api but instead\nused when implementing custom stats collectors:\n\nopen_spider(spider)\u00b6\nOpen the given spider for stats collection.\n\nclose_spider(spider)\u00b6\nClose the given spider. After this is called, no more specific stats\ncan be accessed or collected.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/api.html", "title": ["Core API \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nRequests and Responses\u00b6\nScrapy uses Request and Response objects for crawling web\nsites.\nTypically, Request objects are generated in the spiders and pass\nacross the system until they reach the Downloader, which executes the request\nand returns a Response object which travels back to the spider that\nissued the request.\nBoth Request and Response classes have subclasses which add\nfunctionality not required in the base classes. These are described\nbelow in Request subclasses and\nResponse subclasses.\n\nRequest objects\u00b6\n\nclass scrapy.http.Request(url[, method='GET', body, headers, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, callback, errback])\u00b6\nA Request object represents an HTTP request, which is usually\ngenerated in the Spider and executed by the Downloader, and thus generating\na Response.\nParameters:url (string) \u2013 the URL of this request\nmethod (string) \u2013 the HTTP method of this request. Defaults to 'GET'.\nmeta (dict) \u2013 the initial values for the Request.meta attribute. If\ngiven, the dict passed in this parameter will be shallow copied.\nbody (str or unicode) \u2013 the request body. If a unicode is passed, then it\u2019s encoded to\nstr using the encoding passed (which defaults to utf-8). If\nbody is not given,, an empty string is stored. Regardless of the\ntype of this argument, the final value stored will be a str` (never\nunicode or None).\nheaders (dict) \u2013 the headers of this request. The dict values can be strings\n(for single valued headers) or lists (for multi-valued headers).\ncookies (dict or list) \u2013 the request cookies. These can be sent in two forms.\nUsing a dict:request_with_cookies = Request(url=\"http://www.example.com\",\n                               cookies={'currency': 'USD', 'country': 'UY'})\n\n\n\nUsing a list of dicts:request_with_cookies = Request(url=\"http://www.example.com\",\n                               cookies=[{'name': 'currency',\n                                        'value': 'USD',\n                                        'domain': 'example.com',\n                                        'path': '/currency'}])\n\n\n\nThe latter form allows for customizing the domain and path\nattributes of the cookie. These is only useful if the cookies are saved\nfor later requests.\nWhen some site returns cookies (in a response) those are stored in the\ncookies for that domain and will be sent again in future requests. That\u2019s\nthe typical behaviour of any regular web browser. However, if, for some\nreason, you want to avoid merging with existing cookies you can instruct\nScrapy to do so by setting the dont_merge_cookies key in the\nRequest.meta.\nExample of request without merging cookies:\nrequest_with_cookies = Request(url=\"http://www.example.com\",\n                               cookies={'currency': 'USD', 'country': 'UY'},\n                               meta={'dont_merge_cookies': True})\n\n\nFor more info see CookiesMiddleware.\n\nencoding (string) \u2013 the encoding of this request (defaults to 'utf-8').\nThis encoding will be used to percent-encode the URL and to convert the\nbody to str (if given as unicode).\npriority (int) \u2013 the priority of this request (defaults to 0).\nThe priority is used by the scheduler to define the order used to process\nrequests.\ndont_filter (boolean) \u2013 indicates that this request should not be filtered by\nthe scheduler. This is used when you want to perform an identical\nrequest multiple times, to ignore the duplicates filter. Use it with\ncare, or you will get into crawling loops. Default to False.\ncallback (callable) \u2013 the function that will be called with the response of this\nrequest (once its downloaded) as its first parameter. For more information\nsee Passing additional data to callback functions below.\nIf a Request doesn\u2019t specify a callback, the spider\u2019s\nparse() method will be used.\nerrback (callable) \u2013 a function that will be called if any exception was\nraised while processing the request. This includes pages that failed\nwith 404 HTTP errors and such. It receives a Twisted Failure instance\nas first parameter.\n\n\nurl\u00b6\nA string containing the URL of this request. Keep in mind that this\nattribute contains the escaped URL, so it can differ from the URL passed in\nthe constructor.\nThis attribute is read-only. To change the URL of a Request use\nreplace().\n\nmethod\u00b6\nA string representing the HTTP method in the request. This is guaranteed to\nbe uppercase. Example: \"GET\", \"POST\", \"PUT\", etc\n\nheaders\u00b6\nA dictionary-like object which contains the request headers.\n\nbody\u00b6\nA str that contains the request body.\nThis attribute is read-only. To change the body of a Request use\nreplace().\n\nmeta\u00b6\nA dict that contains arbitrary metadata for this request. This dict is\nempty for new Requests, and is usually  populated by different Scrapy\ncomponents (extensions, middlewares, etc). So the data contained in this\ndict depends on the extensions you have enabled.\nSee Request.meta special keys for a list of special meta keys\nrecognized by Scrapy.\nThis dict is shallow copied when the request is cloned using the\ncopy() or replace() methods, and can also be accessed, in your\nspider, from the response.meta attribute.\n\ncopy()\u00b6\nReturn a new Request which is a copy of this Request. See also:\nPassing additional data to callback functions.\n\nreplace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])\u00b6\nReturn a Request object with the same members, except for those members\ngiven new values by whichever keyword arguments are specified. The\nattribute Request.meta is copied by default (unless a new value\nis given in the meta argument). See also\nPassing additional data to callback functions.\n\nPassing additional data to callback functions\u00b6\nThe callback of a request is a function that will be called when the response\nof that request is downloaded. The callback function will be called with the\ndownloaded Response object as its first argument.\nExample:\ndef parse_page1(self, response):\n    return Request(\"http://www.example.com/some_page.html\",\n                      callback=self.parse_page2)\n\ndef parse_page2(self, response):\n    # this would log http://www.example.com/some_page.html\n    self.log(\"Visited %s\" % response.url)\n\n\nIn some cases you may be interested in passing arguments to those callback\nfunctions so you can receive the arguments later, in the second callback. You\ncan use the Request.meta attribute for that.\nHere\u2019s an example of how to pass an item using this mechanism, to populate\ndifferent fields from different pages:\ndef parse_page1(self, response):\n    item = MyItem()\n    item['main_url'] = response.url\n    request = Request(\"http://www.example.com/some_page.html\",\n                      callback=self.parse_page2)\n    request.meta['item'] = item\n    return request\n\ndef parse_page2(self, response):\n    item = response.meta['item']\n    item['other_url'] = response.url\n    return item\n\n\n\n\n\nRequest.meta special keys\u00b6\nThe Request.meta attribute can contain any arbitrary data, but there\nare some special keys recognized by Scrapy and its built-in extensions.\nThose are:\ndont_redirect\ndont_retry\nhandle_httpstatus_list\ndont_merge_cookies (see cookies parameter of Request constructor)\ncookiejar\nredirect_urls\n\n\nRequest subclasses\u00b6\nHere is the list of built-in Request subclasses. You can also subclass\nit to implement your own custom functionality.\n\nFormRequest objects\u00b6\nThe FormRequest class extends the base Request with functionality for\ndealing with HTML forms. It uses lxml.html forms  to pre-populate form\nfields with form data from Response objects.\n\nclass scrapy.http.FormRequest(url[, formdata, ...])\u00b6\nThe FormRequest class adds a new argument to the constructor. The\nremaining arguments are the same as for the Request class and are\nnot documented here.\nParameters:formdata (dict or iterable of tuples) \u2013 is a dictionary (or iterable of (key, value) tuples)\ncontaining HTML Form data which will be url-encoded and assigned to the\nbody of the request.\nThe FormRequest objects support the following class method in\naddition to the standard Request methods:\n\nclassmethod from_response(response[, formname=None, formnumber=0, formdata=None, dont_click=False, ...])\u00b6\nReturns a new FormRequest object with its form field values\npre-populated with those found in the HTML <form> element contained\nin the given response. For an example see\nUsing FormRequest.from_response() to simulate a user login.\nThe policy is to automatically simulate a click, by default, on any form\ncontrol that looks clickable, like a <input type=\"submit\">.  Even\nthough this is quite convenient, and often the desired behaviour,\nsometimes it can cause problems which could be hard to debug. For\nexample, when working with forms that are filled and/or submitted using\njavascript, the default from_response() behaviour may not be the\nmost appropriate. To disable this behaviour you can set the\ndont_click argument to True. Also, if you want to change the\ncontrol clicked (instead of disabling it) you can also use the\nclickdata argument.\nParameters:response (Response object) \u2013 the response containing a HTML form which will be used\nto pre-populate the form fields\nformname (string) \u2013 if given, the form with name attribute set to this value\nwill be used. Otherwise, formnumber will be used for selecting\nthe form.\nformnumber (integer) \u2013 the number of form to use, when the response contains\nmultiple forms. The first one (and also the default) is 0.\nformdata (dict) \u2013 fields to override in the form data. If a field was\nalready present in the response <form> element, its value is\noverridden by the one passed in this parameter.\ndont_click (boolean) \u2013 If True, the form data will be submitted without\nclicking in any element.\n\nThe other parameters of this class method are passed directly to the\nFormRequest constructor.\n\nNew in version 0.10.3: The formname parameter.\n\n\nRequest usage examples\u00b6\n\nUsing FormRequest to send data via HTTP POST\u00b6\nIf you want to simulate a HTML Form POST in your spider and send a couple of\nkey-value fields, you can return a FormRequest object (from your\nspider) like this:\nreturn [FormRequest(url=\"http://www.example.com/post/action\",\n                    formdata={'name': 'John Doe', age: '27'},\n                    callback=self.after_post)]\n\n\n\n\nUsing FormRequest.from_response() to simulate a user login\u00b6\nIt is usual for web sites to provide pre-populated form fields through <input\ntype=\"hidden\"> elements, such as session related data or authentication\ntokens (for login pages). When scraping, you\u2019ll want these fields to be\nautomatically pre-populated and only override a couple of them, such as the\nuser name and password. You can use the FormRequest.from_response()\nmethod for this job. Here\u2019s an example spider which uses it:\nclass LoginSpider(BaseSpider):\n    name = 'example.com'\n    start_urls = ['http://www.example.com/users/login.php']\n\n    def parse(self, response):\n        return [FormRequest.from_response(response,\n                    formdata={'username': 'john', 'password': 'secret'},\n                    callback=self.after_login)]\n\n    def after_login(self, response):\n        # check login succeed before going on\n        if \"authentication failed\" in response.body:\n            self.log(\"Login failed\", level=log.ERROR)\n            return\n\n        # continue scraping with authenticated session...\n\n\n\n\n\n\nResponse objects\u00b6\n\nclass scrapy.http.Response(url[, status=200, headers, body, flags])\u00b6\nA Response object represents an HTTP response, which is usually\ndownloaded (by the Downloader) and fed to the Spiders for processing.\nParameters:url (string) \u2013 the URL of this response\nheaders (dict) \u2013 the headers of this response. The dict values can be strings\n(for single valued headers) or lists (for multi-valued headers).\nstatus (integer) \u2013 the HTTP status of the response. Defaults to 200.\nbody (str) \u2013 the response body. It must be str, not unicode, unless you\u2019re\nusing a encoding-aware Response subclass, such as\nTextResponse.\nmeta (dict) \u2013 the initial values for the Response.meta attribute. If\ngiven, the dict will be shallow copied.\nflags (list) \u2013 is a list containing the initial values for the\nResponse.flags attribute. If given, the list will be shallow\ncopied.\n\n\nurl\u00b6\nA string containing the URL of the response.\nThis attribute is read-only. To change the URL of a Response use\nreplace().\n\nstatus\u00b6\nAn integer representing the HTTP status of the response. Example: 200,\n404.\n\nheaders\u00b6\nA dictionary-like object which contains the response headers.\n\nbody\u00b6\nA str containing the body of this Response. Keep in mind that Reponse.body\nis always a str. If you want the unicode version use\nTextResponse.body_as_unicode() (only available in\nTextResponse and subclasses).\nThis attribute is read-only. To change the body of a Response use\nreplace().\n\nrequest\u00b6\nThe Request object that generated this response. This attribute is\nassigned in the Scrapy engine, after the response and the request have passed\nthrough all Downloader Middlewares.\nIn particular, this means that:\nHTTP redirections will cause the original request (to the URL before\nredirection) to be assigned to the redirected response (with the final\nURL after redirection).\nResponse.request.url doesn\u2019t always equal Response.url\nThis attribute is only available in the spider code, and in the\nSpider Middlewares, but not in\nDownloader Middlewares (although you have the Request available there by\nother means) and handlers of the response_downloaded signal.\n\nmeta\u00b6\nA shortcut to the Request.meta attribute of the\nResponse.request object (ie. self.request.meta).\nUnlike the Response.request attribute, the Response.meta\nattribute is propagated along redirects and retries, so you will get\nthe original Request.meta sent from your spider.\n\nSee also\nRequest.meta attribute\n\n\nflags\u00b6\nA list that contains flags for this response. Flags are labels used for\ntagging Responses. For example: \u2018cached\u2019, \u2018redirected\u2018, etc. And\nthey\u2019re shown on the string representation of the Response (__str__\nmethod) which is used by the engine for logging.\n\ncopy()\u00b6\nReturns a new Response which is a copy of this Response.\n\nreplace([url, status, headers, body, meta, flags, cls])\u00b6\nReturns a Response object with the same members, except for those members\ngiven new values by whichever keyword arguments are specified. The\nattribute Response.meta is copied by default (unless a new value\nis given in the meta argument).\n\n\nResponse subclasses\u00b6\nHere is the list of available built-in Response subclasses. You can also\nsubclass the Response class to implement your own functionality.\n\nTextResponse objects\u00b6\n\nclass scrapy.http.TextResponse(url[, encoding[, ...]])\u00b6\nTextResponse objects adds encoding capabilities to the base\nResponse class, which is meant to be used only for binary data,\nsuch as images, sounds or any media file.\nTextResponse objects support a new constructor argument, in\naddition to the base Response objects. The remaining functionality\nis the same as for the Response class and is not documented here.\nParameters:encoding (string) \u2013 is a string which contains the encoding to use for this\nresponse. If you create a TextResponse object with a unicode\nbody, it will be encoded using this encoding (remember the body attribute\nis always a string). If encoding is None (default value), the\nencoding will be looked up in the response headers and body instead.\nTextResponse objects support the following attributes in addition\nto the standard Response ones:\n\nencoding\u00b6\nA string with the encoding of this response. The encoding is resolved by\ntrying the following mechanisms, in order:\nthe encoding passed in the constructor encoding argument\nthe encoding declared in the Content-Type HTTP header. If this\nencoding is not valid (ie. unknown), it is ignored and the next\nresolution mechanism is tried.\nthe encoding declared in the response body. The TextResponse class\ndoesn\u2019t provide any special functionality for this. However, the\nHtmlResponse and XmlResponse classes do.\nthe encoding inferred by looking at the response body. This is the more\nfragile method but also the last one tried.\nTextResponse objects support the following methods in addition to\nthe standard Response ones:\n\nbody_as_unicode()\u00b6\nReturns the body of the response as unicode. This is equivalent to:\nresponse.body.decode(response.encoding)\n\n\nBut not equivalent to:\nunicode(response.body)\n\n\nSince, in the latter case, you would be using you system default encoding\n(typically ascii) to convert the body to uniode, instead of the response\nencoding.\n\n\nHtmlResponse objects\u00b6\n\nclass scrapy.http.HtmlResponse(url[, ...])\u00b6\nThe HtmlResponse class is a subclass of TextResponse\nwhich adds encoding auto-discovering support by looking into the HTML meta\nhttp-equiv attribute.  See TextResponse.encoding.\n\n\nXmlResponse objects\u00b6\n\nclass scrapy.http.XmlResponse(url[, ...])\u00b6\nThe XmlResponse class is a subclass of TextResponse which\nadds encoding auto-discovering support by looking into the XML declaration\nline.  See TextResponse.encoding.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/request-response.html", "title": ["Requests and Responses \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSettings\u00b6\nThe Scrapy settings allows you to customize the behaviour of all Scrapy\ncomponents, including the core, extensions, pipelines and spiders themselves.\nThe infrastructure of the settings provides a global namespace of key-value mappings\nthat the code can use to pull configuration values from. The settings can be\npopulated through different mechanisms, which are described below.\nThe settings are also the mechanism for selecting the currently active Scrapy\nproject (in case you have many).\nFor a list of available built-in settings see: Built-in settings reference.\n\nDesignating the settings\u00b6\nWhen you use Scrapy, you have to tell it which settings you\u2019re using. You can\ndo this by using an environment variable, SCRAPY_SETTINGS_MODULE.\nThe value of SCRAPY_SETTINGS_MODULE should be in Python path syntax, e.g.\nmyproject.settings. Note that the settings module should be on the\nPython import search path.\n\n\nPopulating the settings\u00b6\nSettings can be populated using different mechanisms, each of which having a\ndifferent precedence. Here is the list of them in decreasing order of\nprecedence:\n\nGlobal overrides (most precedence)\nProject settings module\nDefault settings per-command\nDefault global settings (less precedence)\n\nThese mechanisms are described in more detail below.\n\n1. Global overrides\u00b6\nGlobal overrides are the ones that take most precedence, and are usually\npopulated by command-line options. You can also override one (or more) settings\nfrom command line using the -s (or --set) command line option.\nFor more information see the overrides\nSettings attribute.\nExample:\nscrapy crawl domain.com -s LOG_FILE=scrapy.log\n\n\n\n\n2. Project settings module\u00b6\nThe project settings module is the standard configuration file for your Scrapy\nproject.  It\u2019s where most of your custom settings will be populated. For\nexample:: myproject.settings.\n\n\n3. Default settings per-command\u00b6\nEach Scrapy tool command can have its own default\nsettings, which override the global default settings. Those custom command\nsettings are specified in the default_settings attribute of the command\nclass.\n\n\n4. Default global settings\u00b6\nThe global defaults are located in the scrapy.settings.default_settings\nmodule and documented in the Built-in settings reference section.\n\n\n\nHow to access settings\u00b6\nSettings can be accessed through the scrapy.crawler.Crawler.settings\nattribute of the Crawler that is passed to from_crawler method in\nextensions and middlewares:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\nIn other words, settings can be accessed like a dict, but it\u2019s usually preferred\nto extract the setting in the format you need it to avoid type errors. In order\nto do that you\u2019ll have to use one of the methods provided the\nSettings API.\n\n\nRationale for setting names\u00b6\nSetting names are usually prefixed with the component that they configure. For\nexample, proper setting names for a fictional robots.txt extension would be\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR, etc.\n\n\nBuilt-in settings reference\u00b6\nHere\u2019s a list of all available Scrapy settings, in alphabetical order, along\nwith their default values and the scope where they apply.\nThe scope, where available, shows where the setting is being used, if it\u2019s tied\nto any particular component. In that case the module of that component will be\nshown, typically an extension, middleware or pipeline. It also means that the\ncomponent must be enabled in order for the setting to have any effect.\n\nAWS_ACCESS_KEY_ID\u00b6\nDefault: None\nThe AWS access key used by code that requires access to Amazon Web services,\nsuch as the S3 feed storage backend.\n\n\nAWS_SECRET_ACCESS_KEY\u00b6\nDefault: None\nThe AWS secret key used by code that requires access to Amazon Web services,\nsuch as the S3 feed storage backend.\n\n\nBOT_NAME\u00b6\nDefault: 'scrapybot'\nThe name of the bot implemented by this Scrapy project (also known as the\nproject name). This will be used to construct the User-Agent by default, and\nalso for logging.\nIt\u2019s automatically populated with your project name when you create your\nproject with the startproject command.\n\n\nCONCURRENT_ITEMS\u00b6\nDefault: 100\nMaximum number of concurrent items (per response) to process in parallel in the\nItem Processor (also known as the Item Pipeline).\n\n\nCONCURRENT_REQUESTS\u00b6\nDefault: 16\nThe maximum number of concurrent (ie. simultaneous) requests that will be\nperformed by the Scrapy downloader.\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN\u00b6\nDefault: 8\nThe maximum number of concurrent (ie. simultaneous) requests that will be\nperformed to any single domain.\n\n\nCONCURRENT_REQUESTS_PER_IP\u00b6\nDefault: 0\nThe maximum number of concurrent (ie. simultaneous) requests that will be\nperformed to any single IP. If non-zero, the\nCONCURRENT_REQUESTS_PER_DOMAIN setting is ignored, and this one is\nused instead. In other words, concurrency limits will be applied per IP, not\nper domain.\n\n\nDEFAULT_ITEM_CLASS\u00b6\nDefault: 'scrapy.item.Item'\nThe default class that will be used for instantiating items in the the\nScrapy shell.\n\n\nDEFAULT_REQUEST_HEADERS\u00b6\nDefault:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nThe default headers used for Scrapy HTTP Requests. They\u2019re populated in the\nDefaultHeadersMiddleware.\n\n\nDEPTH_LIMIT\u00b6\nDefault: 0\nThe maximum depth that will be allowed to crawl for any site. If zero, no limit\nwill be imposed.\n\n\nDEPTH_PRIORITY\u00b6\nDefault: 0\nAn integer that is used to adjust the request priority based on its depth.\nIf zero, no priority adjustment is made from depth.\n\n\nDEPTH_STATS\u00b6\nDefault: True\nWhether to collect maximum depth stats.\n\n\nDEPTH_STATS_VERBOSE\u00b6\nDefault: False\nWhether to collect verbose depth stats. If this is enabled, the number of\nrequests for each depth is collected in the stats.\n\n\nDNSCACHE_ENABLED\u00b6\nDefault: True\nWhether to enable DNS in-memory cache.\n\n\nDOWNLOADER_DEBUG\u00b6\nDefault: False\nWhether to enable the Downloader debugging mode.\n\n\nDOWNLOADER_MIDDLEWARES\u00b6\nDefault:: {}\nA dict containing the downloader middlewares enabled in your project, and their\norders. For more info see Activating a downloader middleware.\n\n\nDOWNLOADER_MIDDLEWARES_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\nA dict containing the downloader middlewares enabled by default in Scrapy. You\nshould never modify this setting in your project, modify\nDOWNLOADER_MIDDLEWARES instead.  For more info see\nActivating a downloader middleware.\n\n\nDOWNLOADER_STATS\u00b6\nDefault: True\nWhether to enable downloader stats collection.\n\n\nDOWNLOAD_DELAY\u00b6\nDefault: 0\nThe amount of time (in secs) that the downloader should wait before downloading\nconsecutive pages from the same spider. This can be used to throttle the\ncrawling speed to avoid hitting servers too hard. Decimal numbers are\nsupported.  Example:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\nThis setting is also affected by the RANDOMIZE_DOWNLOAD_DELAY\nsetting (which is enabled by default). By default, Scrapy doesn\u2019t wait a fixed\namount of time between requests, but uses a random interval between 0.5 and 1.5\n* DOWNLOAD_DELAY.\nYou can also change this setting per spider.\n\n\nDOWNLOAD_HANDLERS\u00b6\nDefault: {}\nA dict containing the request downloader handlers enabled in your project.\nSee DOWNLOAD_HANDLERS_BASE for example format.\n\n\nDOWNLOAD_HANDLERS_BASE\u00b6\nDefault:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\nA dict containing the request download handlers enabled by default in Scrapy.\nYou should never modify this setting in your project, modify\nDOWNLOAD_HANDLERS instead.\n\n\nDOWNLOAD_TIMEOUT\u00b6\nDefault: 180\nThe amount of time (in secs) that the downloader will wait before timing out.\n\n\nDUPEFILTER_CLASS\u00b6\nDefault: 'scrapy.dupefilter.RFPDupeFilter'\nThe class used to detect and filter duplicate requests.\nThe default (RFPDupeFilter) filters based on request fingerprint using\nthe scrapy.utils.request.request_fingerprint function.\n\n\nEDITOR\u00b6\nDefault: depends on the environment\nThe editor to use for editing spiders with the edit command. It\ndefaults to the EDITOR environment variable, if set. Otherwise, it defaults\nto vi (on Unix systems) or the IDLE editor (on Windows).\n\n\nEXTENSIONS\u00b6\nDefault:: {}\nA dict containing the extensions enabled in your project, and their orders.\n\n\nEXTENSIONS_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.webservice.WebService': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\nThe list of available extensions. Keep in mind that some of them need to\nbe enabled through a setting. By default, this setting contains all stable\nbuilt-in extensions.\nFor more information See the extensions user guide\nand the list of available extensions.\n\n\nITEM_PIPELINES\u00b6\nDefault: []\nThe item pipelines to use (a list of classes).\nExample:\nITEM_PIPELINES = [\n    'mybot.pipeline.validate.ValidateMyItem',\n    'mybot.pipeline.validate.StoreMyItem'\n]\n\n\n\n\nLOG_ENABLED\u00b6\nDefault: True\nWhether to enable logging.\n\n\nLOG_ENCODING\u00b6\nDefault: 'utf-8'\nThe encoding to use for logging.\n\n\nLOG_FILE\u00b6\nDefault: None\nFile name to use for logging output. If None, standard error will be used.\n\n\nLOG_LEVEL\u00b6\nDefault: 'DEBUG'\nMinimum level to log. Available levels are: CRITICAL, ERROR, WARNING,\nINFO, DEBUG. For more info see Logging.\n\n\nLOG_STDOUT\u00b6\nDefault: False\nIf True, all standard output (and error) of your process will be redirected\nto the log. For example if you print 'hello' it will appear in the Scrapy\nlog.\n\n\nMEMDEBUG_ENABLED\u00b6\nDefault: False\nWhether to enable memory debugging.\n\n\nMEMDEBUG_NOTIFY\u00b6\nDefault: []\nWhen memory debugging is enabled a memory report will be sent to the specified\naddresses if this setting is not empty, otherwise the report will be written to\nthe log.\nExample:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED\u00b6\nDefault: False\nScope: scrapy.contrib.memusage\nWhether to enable the memory usage extension that will shutdown the Scrapy\nprocess when it exceeds a memory limit, and also notify by email when that\nhappened.\nSee Memory usage extension.\n\n\nMEMUSAGE_LIMIT_MB\u00b6\nDefault: 0\nScope: scrapy.contrib.memusage\nThe maximum amount of memory to allow (in megabytes) before shutting down\nScrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.\nSee Memory usage extension.\n\n\nMEMUSAGE_NOTIFY_MAIL\u00b6\nDefault: False\nScope: scrapy.contrib.memusage\nA list of emails to notify if the memory limit has been reached.\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee Memory usage extension.\n\n\nMEMUSAGE_REPORT\u00b6\nDefault: False\nScope: scrapy.contrib.memusage\nWhether to send a memory usage report after each spider has been closed.\nSee Memory usage extension.\n\n\nMEMUSAGE_WARNING_MB\u00b6\nDefault: 0\nScope: scrapy.contrib.memusage\nThe maximum amount of memory to allow (in megabytes) before sending a warning\nemail notifying about it. If zero, no warning will be produced.\n\n\nNEWSPIDER_MODULE\u00b6\nDefault: ''\nModule where to create new spiders using the genspider command.\nExample:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY\u00b6\nDefault: True\nIf enabled, Scrapy will wait a random amount of time (between 0.5 and 1.5\n* DOWNLOAD_DELAY) while fetching requests from the same\nspider.\nThis randomization decreases the chance of the crawler being detected (and\nsubsequently blocked) by sites which analyze requests looking for statistically\nsignificant similarities in the time between their requests.\nThe randomization policy is the same used by wget --random-wait option.\nIf DOWNLOAD_DELAY is zero (default) this option has no effect.\n\n\nREDIRECT_MAX_TIMES\u00b6\nDefault: 20\nDefines the maximum times a request can be redirected. After this maximum the\nrequest\u2019s response is returned as is. We used Firefox default value for the\nsame task.\n\n\nREDIRECT_MAX_METAREFRESH_DELAY\u00b6\nDefault: 100\nSome sites use meta-refresh for redirecting to a session expired page, so we\nrestrict automatic redirection to a maximum delay (in seconds)\n\n\nREDIRECT_PRIORITY_ADJUST\u00b6\nDefault: +2\nAdjust redirect request priority relative to original request.\nA negative priority adjust means more priority.\n\n\nROBOTSTXT_OBEY\u00b6\nDefault: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\nIf enabled, Scrapy will respect robots.txt policies. For more information see\nRobotsTxtMiddleware\n\n\nSCHEDULER\u00b6\nDefault: 'scrapy.core.scheduler.Scheduler'\nThe scheduler to use for crawling.\n\n\nSPIDER_CONTRACTS\u00b6\nDefault:: {}\nA dict containing the scrapy contracts enabled in your project, used for\ntesting spiders. For more info see Spiders Contracts.\n\n\nSPIDER_CONTRACTS_BASE\u00b6\nDefault:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\nA dict containing the scrapy contracts enabled by default in Scrapy. You should\nnever modify this setting in your project, modify SPIDER_CONTRACTS\ninstead. For more info see Spiders Contracts.\n\n\nSPIDER_MIDDLEWARES\u00b6\nDefault:: {}\nA dict containing the spider middlewares enabled in your project, and their\norders. For more info see Activating a spider middleware.\n\n\nSPIDER_MIDDLEWARES_BASE\u00b6\nDefault:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\nA dict containing the spider middlewares enabled by default in Scrapy. You\nshould never modify this setting in your project, modify\nSPIDER_MIDDLEWARES instead. For more info see\nActivating a spider middleware.\n\n\nSPIDER_MODULES\u00b6\nDefault: []\nA list of modules where Scrapy will look for spiders.\nExample:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS\u00b6\nDefault: 'scrapy.statscol.MemoryStatsCollector'\nThe class to use for collecting stats, who must implement the\nStats Collector API.\n\n\nSTATS_DUMP\u00b6\nDefault: True\nDump the Scrapy stats (to the Scrapy log) once the spider\nfinishes.\nFor more info see: Stats Collection.\n\n\nSTATSMAILER_RCPTS\u00b6\nDefault: [] (empty list)\nSend Scrapy stats after spiders finish scraping. See\nStatsMailer for more info.\n\n\nTELNETCONSOLE_ENABLED\u00b6\nDefault: True\nA boolean which specifies if the telnet console\nwill be enabled (provided its extension is also enabled).\n\n\nTELNETCONSOLE_PORT\u00b6\nDefault: [6023, 6073]\nThe port range to use for the telnet console. If set to None or 0, a\ndynamically assigned port is used. For more info see\nTelnet Console.\n\n\nTEMPLATES_DIR\u00b6\nDefault: templates dir inside scrapy module\nThe directory where to look for templates when creating new projects with\nstartproject command.\n\n\nURLLENGTH_LIMIT\u00b6\nDefault: 2083\nScope: contrib.spidermiddleware.urllength\nThe maximum URL length to allow for crawled URLs. For more information about\nthe default value for this setting see: http://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT\u00b6\nDefault: \"Scrapy/VERSION (+http://scrapy.org)\"\nThe default User-Agent to use when crawling, unless overridden.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/settings.html", "title": ["Settings \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nSignals\u00b6\nScrapy uses signals extensively to notify when certain events occur. You can\ncatch some of those signals in your Scrapy project (using an extension, for example) to perform additional tasks or extend Scrapy\nto add functionality not provided out of the box.\nEven though signals provide several arguments, the handlers that catch them\ndon\u2019t need to accept all of them - the signal dispatching mechanism will only\ndeliver the arguments that the handler receives.\nYou can connect to signals (or send your own) through the\nSignals API.\n\nDeferred signal handlers\u00b6\nSome signals support returning Twisted deferreds from their handlers, see\nthe Built-in signals reference below to know which ones.\n\n\nBuilt-in signals reference\u00b6\nHere\u2019s the list of Scrapy built-in signals and their meaning.\n\nengine_started\u00b6\n\nscrapy.signals.engine_started()\u00b6\nSent when the Scrapy engine has started crawling.\nThis signal supports returning deferreds from their handlers.\n\nNote\nThis signal may be fired after the spider_opened signal,\ndepending on how the spider was started. So don\u2019t rely on this signal\ngetting fired before spider_opened.\n\n\n\nengine_stopped\u00b6\n\nscrapy.signals.engine_stopped()\u00b6\nSent when the Scrapy engine is stopped (for example, when a crawling\nprocess has finished).\nThis signal supports returning deferreds from their handlers.\n\n\nitem_scraped\u00b6\n\nscrapy.signals.item_scraped(item, response, spider)\u00b6\nSent when an item has been scraped, after it has passed all the\nItem Pipeline stages (without being dropped).\nThis signal supports returning deferreds from their handlers.\nParameters:item (Item object) \u2013 the item scraped\nresponse (Response object) \u2013 the response from where the item was scraped\nspider (BaseSpider object) \u2013 the spider which scraped the item\n\n\n\nitem_dropped\u00b6\n\nscrapy.signals.item_dropped(item, spider, exception)\u00b6\nSent after an item has been dropped from the Item Pipeline\nwhen some stage raised a DropItem exception.\nThis signal supports returning deferreds from their handlers.\nParameters:item (Item object) \u2013 the item dropped from the Item Pipeline\nspider (BaseSpider object) \u2013 the spider which scraped the item\nexception (DropItem exception) \u2013 the exception (which must be a\nDropItem subclass) which caused the item\nto be dropped\n\n\n\nspider_closed\u00b6\n\nscrapy.signals.spider_closed(spider, reason)\u00b6\nSent after a spider has been closed. This can be used to release per-spider\nresources reserved on spider_opened.\nThis signal supports returning deferreds from their handlers.\nParameters:spider (BaseSpider object) \u2013 the spider which has been closed\nreason (str) \u2013 a string which describes the reason why the spider was closed. If\nit was closed because the spider has completed scraping, the reason\nis 'finished'. Otherwise, if the spider was manually closed by\ncalling the close_spider engine method, then the reason is the one\npassed in the reason argument of that method (which defaults to\n'cancelled'). If the engine was shutdown (for example, by hitting\nCtrl-C to stop it) the reason will be 'shutdown'.\n\n\n\nspider_opened\u00b6\n\nscrapy.signals.spider_opened(spider)\u00b6\nSent after a spider has been opened for crawling. This is typically used to\nreserve per-spider resources, but can be used for any task that needs to be\nperformed when a spider is opened.\nThis signal supports returning deferreds from their handlers.\nParameters:spider (BaseSpider object) \u2013 the spider which has been opened\n\n\nspider_idle\u00b6\n\nscrapy.signals.spider_idle(spider)\u00b6\nSent when a spider has gone idle, which means the spider has no further:\n\nrequests waiting to be downloaded\nrequests scheduled\nitems being processed in the item pipeline\n\nIf the idle state persists after all handlers of this signal have finished,\nthe engine starts closing the spider. After the spider has finished\nclosing, the spider_closed signal is sent.\nYou can, for example, schedule some requests in your spider_idle\nhandler to prevent the spider from being closed.\nThis signal does not support returning deferreds from their handlers.\nParameters:spider (BaseSpider object) \u2013 the spider which has gone idle\n\n\nspider_error\u00b6\n\nscrapy.signals.spider_error(failure, response, spider)\u00b6\nSent when a spider callback generates an error (ie. raises an exception).\nParameters:failure (Failure object) \u2013 the exception raised as a Twisted Failure object\nresponse (Response object) \u2013 the response being processed when the exception was raised\nspider (BaseSpider object) \u2013 the spider which raised the exception\n\n\n\nrequest_received\u00b6\n\nscrapy.signals.request_received(request, spider)\u00b6\nSent when the engine receives a Request from a spider.\nThis signal does not support returning deferreds from their handlers.\nParameters:request (Request object) \u2013 the request received\nspider (BaseSpider object) \u2013 the spider which generated the request\n\n\n\nresponse_received\u00b6\n\nscrapy.signals.response_received(response, request, spider)\u00b6\nSent when the engine receives a new Response from the\ndownloader.\nThis signal does not support returning deferreds from their handlers.\nParameters:response (Response object) \u2013 the response received\nrequest (Request object) \u2013 the request that generated the response\nspider (BaseSpider object) \u2013 the spider for which the response is intended\n\n\n\nresponse_downloaded\u00b6\n\nscrapy.signals.response_downloaded(response, request, spider)\u00b6\nSent by the downloader right after a HTTPResponse is downloaded.\nThis signal does not support returning deferreds from their handlers.\nParameters:response (Response object) \u2013 the response downloaded\nrequest (Request object) \u2013 the request that generated the response\nspider (BaseSpider object) \u2013 the spider for which the response is intended\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/signals.html", "title": ["Signals \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExceptions\u00b6\n\nBuilt-in Exceptions reference\u00b6\nHere\u2019s a list of all exceptions included in Scrapy and their usage.\n\nDropItem\u00b6\n\nexception scrapy.exceptions.DropItem\u00b6\nThe exception that must be raised by item pipeline stages to stop processing an\nItem. For more information see Item Pipeline.\n\n\nCloseSpider\u00b6\n\nexception scrapy.exceptions.CloseSpider(reason='cancelled')\u00b6\nThis exception can be raised from a spider callback to request the spider to be\nclosed/stopped. Supported arguments:\nParameters:reason (str) \u2013 the reason for closing\nFor example:\ndef parse_page(self, response):\n    if 'Bandwidth exceeded' in response.body:\n        raise CloseSpider('bandwidth_exceeded')\n\n\n\n\nIgnoreRequest\u00b6\n\nexception scrapy.exceptions.IgnoreRequest\u00b6\nThis exception can be raised by the Scheduler or any downloader middleware to\nindicate that the request should be ignored.\n\n\nNotConfigured\u00b6\n\nexception scrapy.exceptions.NotConfigured\u00b6\nThis exception can be raised by some components to indicate that they will\nremain disabled. Those components include:\n\nExtensions\nItem pipelines\nDownloader middlwares\nSpider middlewares\n\nThe exception must be raised in the component constructor.\n\n\nNotSupported\u00b6\n\nexception scrapy.exceptions.NotSupported\u00b6\nThis exception is raised to indicate an unsupported feature.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/exceptions.html", "title": ["Exceptions \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nItem Exporters\u00b6\nOnce you have scraped your Items, you often want to persist or export those\nitems, to use the data in some other application. That is, after all, the whole\npurpose of the scraping process.\nFor this purpose Scrapy provides a collection of Item Exporters for different\noutput formats, such as XML, CSV or JSON.\n\nUsing Item Exporters\u00b6\nIf you are in a hurry, and just want to use an Item Exporter to output scraped\ndata see the Feed exports. Otherwise, if you want to know how\nItem Exporters work or need more custom functionality (not covered by the\ndefault exports), continue reading below.\nIn order to use an Item Exporter, you  must instantiate it with its required\nargs. Each Item Exporter requires different arguments, so check each exporter\ndocumentation to be sure, in Built-in Item Exporters reference. After you have\ninstantiated you exporter, you have to:\n1. call the method start_exporting() in order to\nsignal the beginning of the exporting process\n2. call the export_item() method for each item you want\nto export\n3. and finally call the finish_exporting() to signal\nthe end of the exporting process\nHere you can see an Item Pipeline which uses an Item\nExporter to export scraped items to different files, one per spider:\nfrom scrapy import signals\nfrom scrapy.contrib.exporter import XmlItemExporter\n\nclass XmlExportPipeline(object):\n\n    def __init__(self):\n        self.files = {}\n\n     @classmethod\n     def from_crawler(cls, crawler):\n         pipeline = cls()\n         crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)\n         crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)\n         return pipeline\n\n    def spider_opened(self, spider):\n        file = open('%s_products.xml' % spider.name, 'w+b')\n        self.files[spider] = file\n        self.exporter = XmlItemExporter(file)\n        self.exporter.start_exporting()\n\n    def spider_closed(self, spider):\n        self.exporter.finish_exporting()\n        file = self.files.pop(spider)\n        file.close()\n\n    def process_item(self, item, spider):\n        self.exporter.export_item(item)\n        return item\n\n\n\nSerialization of item fields\u00b6\nBy default, the field values are passed unmodified to the underlying\nserialization library, and the decision of how to serialize them is delegated\nto each particular serialization library.\nHowever, you can customize how each field value is serialized before it is\npassed to the serialization library.\nThere are two ways to customize how a field will be serialized, which are\ndescribed next.\n\n1. Declaring a serializer in the field\u00b6\nYou can declare a serializer in the field metadata. The serializer must be a callable which receives a\nvalue and returns its serialized form.\nExample:\nfrom scrapy.item import Item, Field\n\ndef serialize_price(value):\n   return '$ %s' % str(value)\n\nclass Product(Item):\n    name = Field()\n    price = Field(serializer=serialize_price)\n\n\n\n\n2. Overriding the serialize_field() method\u00b6\nYou can also override the serialize() method to\ncustomize how your field value will be exported.\nMake sure you call the base class serialize() method\nafter your custom code.\nExample:\nfrom scrapy.contrib.exporter import XmlItemExporter\n\nclass ProductXmlExporter(XmlItemExporter):\n\n    def serialize_field(self, field, name, value):\n        if field == 'price':\n            return '$ %s' % str(value)\n        return super(Product, self).serialize_field(field, name, value)\n\n\n\n\n\nBuilt-in Item Exporters reference\u00b6\nHere is a list of the Item Exporters bundled with Scrapy. Some of them contain\noutput examples, which assume you\u2019re exporting these two items:\nItem(name='Color TV', price='1200')\nItem(name='DVD player', price='200')\n\n\n\nBaseItemExporter\u00b6\n\nclass scrapy.contrib.exporter.BaseItemExporter(fields_to_export=None, export_empty_fields=False, encoding='utf-8')\u00b6\nThis is the (abstract) base class for all Item Exporters. It provides\nsupport for common features used by all (concrete) Item Exporters, such as\ndefining what fields to export, whether to export empty fields, or which\nencoding to use.\nThese features can be configured through the constructor arguments which\npopulate their respective instance attributes: fields_to_export,\nexport_empty_fields, encoding.\n\nexport_item(item)\u00b6\nExports the given item. This method must be implemented in subclasses.\n\nserialize_field(field, name, value)\u00b6\nReturn the serialized value for the given field. You can override this\nmethod (in your custom Item Exporters) if you want to control how a\nparticular field or value will be serialized/exported.\nBy default, this method looks for a serializer declared in the item\nfield and returns the result of applying\nthat serializer to the value. If no serializer is found, it returns the\nvalue unchanged except for unicode values which are encoded to\nstr using the encoding declared in the encoding attribute.\nParameters:field (Field object) \u2013 the field being serialized\nname (str) \u2013 the name of the field being serialized\nvalue \u2013 the value being serialized\n\n\nstart_exporting()\u00b6\nSignal the beginning of the exporting process. Some exporters may use\nthis to generate some required header (for example, the\nXmlItemExporter). You must call this method before exporting any\nitems.\n\nfinish_exporting()\u00b6\nSignal the end of the exporting process. Some exporters may use this to\ngenerate some required footer (for example, the\nXmlItemExporter). You must always call this method after you\nhave no more items to export.\n\nfields_to_export\u00b6\nA list with the name of the fields that will be exported, or None if you\nwant to export all fields. Defaults to None.\nSome exporters (like CsvItemExporter) respect the order of the\nfields defined in this attribute.\n\nexport_empty_fields\u00b6\nWhether to include empty/unpopulated item fields in the exported data.\nDefaults to False. Some exporters (like CsvItemExporter)\nignore this attribute and always export all empty fields.\n\nencoding\u00b6\nThe encoding that will be used to encode unicode values. This only\naffects unicode values (which are always serialized to str using this\nencoding). Other value types are passed unchanged to the specific\nserialization library.\n\n\nXmlItemExporter\u00b6\n\nclass scrapy.contrib.exporter.XmlItemExporter(file, item_element='item', root_element='items', **kwargs)\u00b6\nExports Items in XML format to the specified file object.\nParameters:file \u2013 the file-like object to use for exporting the data.\nroot_element (str) \u2013 The name of root element in the exported XML.\nitem_element (str) \u2013 The name of each item element in the exported XML.\n\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor.\nA typical output of this exporter would be:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>Color TV</name>\n    <price>1200</price>\n </item>\n  <item>\n    <name>DVD player</name>\n    <price>200</price>\n </item>\n</items>\n\n\nUnless overridden in the serialize_field() method, multi-valued fields are\nexported by serializing each value inside a <value> element. This is for\nconvenience, as multi-valued fields are very common.\nFor example, the item:\nItem(name=['John', 'Doe'], age='23')\n\n\nWould be serialized as:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>\n      <value>John</value>\n      <value>Doe</value>\n    </name>\n    <age>23</age>\n  </item>\n</items>\n\n\n\n\nCsvItemExporter\u00b6\n\nclass scrapy.contrib.exporter.CsvItemExporter(file, include_headers_line=True, join_multivalued=', ', **kwargs)\u00b6\nExports Items in CSV format to the given file-like object. If the\nfields_to_export attribute is set, it will be used to define the\nCSV columns and their order. The export_empty_fields attribute has\nno effect on this exporter.\nParameters:file \u2013 the file-like object to use for exporting the data.\ninclude_headers_line (str) \u2013 If enabled, makes the exporter output a header\nline with the field names taken from\nBaseItemExporter.fields_to_export or the first exported item fields.\njoin_multivalued \u2013 The char (or chars) that will be used for joining\nmulti-valued fields, if found.\n\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor, and the leftover arguments to the\ncsv.writer constructor, so you can use any csv.writer constructor\nargument to customize this exporter.\nA typical output of this exporter would be:\nproduct,price\nColor TV,1200\nDVD player,200\n\n\n\n\nPickleItemExporter\u00b6\n\nclass scrapy.contrib.exporter.PickleItemExporter(file, protocol=0, **kwargs)\u00b6\nExports Items in pickle format to the given file-like object.\nParameters:file \u2013 the file-like object to use for exporting the data.\nprotocol (int) \u2013 The pickle protocol to use.\n\nFor more information, refer to the pickle module documentation.\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor.\nPickle isn\u2019t a human readable format, so no output examples are provided.\n\n\nPprintItemExporter\u00b6\n\nclass scrapy.contrib.exporter.PprintItemExporter(file, **kwargs)\u00b6\nExports Items in pretty print format to the specified file object.\nParameters:file \u2013 the file-like object to use for exporting the data.\nThe additional keyword arguments of this constructor are passed to the\nBaseItemExporter constructor.\nA typical output of this exporter would be:\n{'name': 'Color TV', 'price': '1200'}\n{'name': 'DVD player', 'price': '200'}\n\n\nLonger lines (when present) are pretty-formatted.\n\n\nJsonItemExporter\u00b6\n\nclass scrapy.contrib.exporter.JsonItemExporter(file, **kwargs)\u00b6\nExports Items in JSON format to the specified file-like object, writing all\nobjects as a list of objects. The additional constructor arguments are\npassed to the BaseItemExporter constructor, and the leftover\narguments to the JSONEncoder constructor, so you can use any\nJSONEncoder constructor argument to customize this exporter.\nParameters:file \u2013 the file-like object to use for exporting the data.\nA typical output of this exporter would be:\n[{\"name\": \"Color TV\", \"price\": \"1200\"},\n{\"name\": \"DVD player\", \"price\": \"200\"}]\n\n\n\nWarning\nJSON is very simple and flexible serialization format, but it\ndoesn\u2019t scale well for large amounts of data since incremental (aka.\nstream-mode) parsing is not well supported (if at all) among JSON parsers\n(on any language), and most of them just parse the entire object in\nmemory. If you want the power and simplicity of JSON with a more\nstream-friendly format, consider using JsonLinesItemExporter\ninstead, or splitting the output in multiple chunks.\n\n\n\nJsonLinesItemExporter\u00b6\n\nclass scrapy.contrib.exporter.JsonLinesItemExporter(file, **kwargs)\u00b6\nExports Items in JSON format to the specified file-like object, writing one\nJSON-encoded item per line. The additional constructor arguments are passed\nto the BaseItemExporter constructor, and the leftover arguments to\nthe JSONEncoder constructor, so you can use any JSONEncoder\nconstructor argument to customize this exporter.\nParameters:file \u2013 the file-like object to use for exporting the data.\nA typical output of this exporter would be:\n{\"name\": \"Color TV\", \"price\": \"1200\"}\n{\"name\": \"DVD player\", \"price\": \"200\"}\n\n\nUnlike the one produced by JsonItemExporter, the format produced by\nthis exporter is well suited for serializing large amounts of data.\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/topics/exporters.html", "title": ["Item Exporters \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nRelease notes\u00b6\n\n0.16.4 (released 2013-01-23)\u00b6\nfixes spelling errors in documentation (commit 6d2b3aa)\nadd doc about disabling an extension. refs #132 (commit c90de33)\nFixed error message formatting. log.err() doesn\u2019t support cool formatting and when error occured, the message was:    \u201cERROR: Error processing %(item)s\u201d (commit c16150c)\nlint and improve images pipeline error logging (commit 56b45fc)\nfixed doc typos (commit 243be84)\nadd documentation topics: Broad Crawls & Common Practies (commit 1fbb715)\nfix bug in scrapy parse command when spider is not specified explicitly. closes #209 (commit c72e682)\nUpdate docs/topics/commands.rst (commit 28eac7a)\n\n\n0.16.3 (released 2012-12-07)\u00b6\nRemove concurrency limitation when using download delays and still ensure inter-request delays are enforced (commit 487b9b5)\nadd error details when image pipeline fails (commit 8232569)\nimprove mac os compatibility (commit 8dcf8aa)\nsetup.py: use README.rst to populate long_description (commit 7b5310d)\ndoc: removed obsolete references to ClientForm (commit 80f9bb6)\ncorrect docs for default storage backend (commit 2aa491b)\ndoc: removed broken proxyhub link from FAQ (commit bdf61c4)\nMerge branch \u20180.16\u2019 of github.com:scrapy/scrapy into 0.16 (commit d5087b0)\nFixed docs typo in SpiderOpenCloseLogging example (commit 7184094)\n\n\n0.16.2 (released 2012-11-09)\u00b6\nscrapy contracts: python2.6 compat (commit a4a9199)\nscrapy contracts verbose option (commit ec41673)\nproper unittest-like output for scrapy contracts (commit 86635e4)\nadded open_in_browser to debugging doc (commit c9b690d)\nremoved reference to global scrapy stats from settings doc (commit dd55067)\nFix SpiderState bug in Windows platforms (commit 58998f4)\n\n\n0.16.1 (released 2012-10-26)\u00b6\nfixed LogStats extension, which got broken after a wrong merge before the 0.16 release (commit 8c780fd)\nbetter backwards compatibility for scrapy.conf.settings (commit 3403089)\nextended documentation on how to access crawler stats from extensions (commit c4da0b5)\nremoved .hgtags (no longer needed now that scrapy uses git) (commit d52c188)\nfix dashes under rst headers (commit fa4f7f9)\nset release date for 0.16.0 in news (commit e292246)\n\n\n0.16.0 (released 2012-10-18)\u00b6\nScrapy changes:\nadded Spiders Contracts, a mechanism for testing spiders in a formal/reproducible way\nadded options -o and -t to the runspider command\ndocumented AutoThrottle extension and added to extensions installed by default. You still need to enable it with AUTOTHROTTLE_ENABLED\nmajor Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (stats_spider_opened, etc). Stats are much simpler now, backwards compatibility is kept on the Stats Collector API and signals.\nadded process_start_requests() method to spider middlewares\ndropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.\ndropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.\ndropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.\ndocumented Core API\nlxml is now the default selectors backend instead of libxml2\nported FormRequest.from_response() to use lxml instead of ClientForm\nremoved modules: scrapy.xlib.BeautifulSoup and scrapy.xlib.ClientForm\nSitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (commit 10ed28b)\nStackTraceDump extension: also dump trackref live references (commit fe2ce93)\nnested items now fully supported in JSON and JSONLines exporters\nadded cookiejar Request meta key to support multiple cookie sessions per spider\ndecoupled encoding detection code to w3lib.encoding, and ported Scrapy code to use that mdule\ndropped support for Python 2.5. See http://blog.scrapy.org/scrapy-dropping-support-for-python-25\ndropped support for Twisted 2.5\nadded REFERER_ENABLED setting, to control referer middleware\nchanged default user agent to: Scrapy/VERSION (+http://scrapy.org)\nremoved (undocumented) HTMLImageLinkExtractor class from scrapy.contrib.linkextractors.image\nremoved per-spider settings (to be replaced by instantiating multiple crawler objects)\nUSER_AGENT spider attribute will no longer work, use user_agent attribute instead\nDOWNLOAD_TIMEOUT spider attribute will no longer work, use download_timeout attribute instead\nremoved ENCODING_ALIASES setting, as encoding auto-detection has been moved to the w3lib library\npromoted DjangoItem to main contrib\nLogFormatter method now return dicts(instead of strings) to support lazy formatting (issue 164, commit dcef7b0)\ndownloader handlers (DOWNLOAD_HANDLERS setting) now receive settings as the first argument of the constructor\nreplaced memory usage acounting with (more portable) resource module, removed scrapy.utils.memory module\nremoved signal: scrapy.mail.mail_sent\nremoved TRACK_REFS setting, now trackrefs is always enabled\nDBM is now the default storage backend for HTTP cache middleware\nnumber of log messages (per level) are now tracked through Scrapy stats (stat name: log_count/LEVEL)\nnumber received responses are now tracked through Scrapy stats (stat name: response_received_count)\nremoved scrapy.log.started attribute\nScrapyd changes:\nNew Scrapyd API methods: listjobs.json and cancel.json\nNew Scrapyd settings: items_dir and jobs_to_keep\nItems are now stored on disk using feed exports, and accessible through the Scrapyd web interface\nSupport making Scrapyd listen into a specific IP address (see bind_address option)\n\n\n0.14.4\u00b6\nadded precise to supported ubuntu distros (commit b7e46df)\nfixed bug in json-rpc webservice reported in https://groups.google.com/d/topic/scrapy-users/qgVBmFybNAQ/discussion. also removed no longer supported \u2018run\u2019 command from extras/scrapy-ws.py (commit 340fbdb)\nmeta tag attributes for content-type http equiv can be in any order. #123 (commit 0cb68af)\nreplace \u201cimport Image\u201d by more standard \u201cfrom PIL import Image\u201d. closes #88 (commit 4d17048)\nreturn trial status as bin/runtests.sh exit value. #118 (commit b7b2e7f)\n\n\n0.14.3\u00b6\nforgot to include pydispatch license. #118 (commit fd85f9c)\ninclude egg files used by testsuite in source distribution. #118 (commit c897793)\nupdate docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (commit 2548dcc)\nadded note to docs/topics/firebug.rst about google directory being shut down (commit 668e352)\nMerge branch \u20180.14\u2019 of github.com:scrapy/scrapy into 0.14 (commit 835d082)\ndont discard slot when empty, just save in another dict in order to recycle if needed again. (commit 8e9f607)\ndo not fail handling unicode xpaths in libxml2 backed selectors (commit b830e95)\nfixed minor mistake in Request objects documentation (commit bf3c9ee)\nfixed minor defect in link extractors documentation (commit ba14f38)\nremoved some obsolete remaining code related to sqlite support in scrapy (commit 0665175)\n\n\n0.14.2\u00b6\nmove buffer pointing to start of file before computing checksum. refs #92 (commit 6a5bef2)\nCompute image checksum before persisting images. closes #92 (commit 9817df1)\nremove leaking references in cached failures (commit 673a120)\nfixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (commit 11133e9)\nMerge branch \u20180.14\u2019 of github.com:scrapy/scrapy into 0.14 (commit 1627320)\nfixed struct.error on http compression middleware. closes #87 (commit 1423140)\najax crawling wasn\u2019t expanding for unicode urls (commit 0de3fb4)\nCatch start_requests iterator errors. refs #83 (commit 454a21d)\nSpeed-up libxml2 XPathSelector (commit 2fbd662)\nupdated versioning doc according to recent changes (commit 0a070f5)\nscrapyd: fixed documentation link (commit 2b4e4c3)\nextras/makedeb.py: no longer obtaining version from git (commit caffe0e)\n\n\n0.14.1\u00b6\nextras/makedeb.py: no longer obtaining version from git (commit caffe0e)\nbumped version to 0.14.1 (commit 6cb9e1c)\nfixed reference to tutorial directory (commit 4b86bd6)\ndoc: removed duplicated callback argument from Request.replace() (commit 1aeccdd)\nfixed formatting of scrapyd doc (commit 8bf19e6)\nDump stacks for all running threads and fix engine status dumped by StackTraceDump extension (commit 14a8e6e)\nadded comment about why we disable ssl on boto images upload (commit 5223575)\nSSL handshaking hangs when doing too many parallel connections to S3 (commit 63d583d)\nchange tutorial to follow changes on dmoz site (commit bcb3198)\nAvoid _disconnectedDeferred AttributeError exception in Twisted>=11.1.0 (commit 98f3f87)\nallow spider to set autothrottle max concurrency (commit 175a4b5)\n\n\n0.14\u00b6\n\nNew features and settings\u00b6\nSupport for AJAX crawleable urls\n\nNew persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (r2737)\n\nadded -o option to scrapy crawl, a shortcut for dumping scraped items into a file (or standard output using -)\n\nAdded support for passing custom settings to Scrapyd schedule.json api (r2779, r2783)\n\nNew ChunkedTransferMiddleware (enabled by default) to support chunked transfer encoding (r2769)\n\nAdd boto 2.0 support for S3 downloader handler (r2763)\n\nAdded marshal to formats supported by feed exports (r2744)\n\nIn request errbacks, offending requests are now received in failure.request attribute (r2738)\n\nBig downloader refactoring to support per domain/ip concurrency limits (r2732)\nCONCURRENT_REQUESTS_PER_SPIDER setting has been deprecated and replaced by:\nCONCURRENT_REQUESTS, CONCURRENT_REQUESTS_PER_DOMAIN, CONCURRENT_REQUESTS_PER_IP\n\n\ncheck the documentation for more details\n\n\n\nAdded builtin caching DNS resolver (r2728)\n\nMoved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](https://github.com/scrapinghub/scaws) (r2706, r2714)\n\nMoved spider queues to scrapyd: scrapy.spiderqueue -> scrapyd.spiderqueue (r2708)\n\nMoved sqlite utils to scrapyd: scrapy.utils.sqlite -> scrapyd.sqlite (r2781)\n\nReal support for returning iterators on start_requests() method. The iterator is now consumed during the crawl when the spider is getting idle (r2704)\n\nAdded REDIRECT_ENABLED setting to quickly enable/disable the redirect middleware (r2697)\n\nAdded RETRY_ENABLED setting to quickly enable/disable the retry middleware (r2694)\n\nAdded CloseSpider exception to manually close spiders (r2691)\n\nImproved encoding detection by adding support for HTML5 meta charset declaration (r2690)\n\nRefactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (r2688)\n\nAdded SitemapSpider (see documentation in Spiders page) (r2658)\n\nAdded LogStats extension for periodically logging basic stats (like crawled pages and scraped items) (r2657)\n\nMake handling of gzipped responses more robust (#319, r2643). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an IOError.\n\nSimplified !MemoryDebugger extension to use stats for dumping memory debugging info (r2639)\n\nAdded new command to edit spiders: scrapy edit (r2636) and -e flag to genspider command that uses it (r2653)\n\nChanged default representation of items to pretty-printed dicts. (r2631). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.\n\nAdded spider_error signal (r2628)\n\nAdded COOKIES_ENABLED setting (r2625)\n\nStats are now dumped to Scrapy log (default value of STATS_DUMP setting has been changed to True). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.\n\nAdded support for dynamically adjusting download delay and maximum concurrent requests (r2599)\n\nAdded new DBM HTTP cache storage backend (r2576)\n\nAdded listjobs.json API to Scrapyd (r2571)\n\nCsvItemExporter: added join_multivalued parameter (r2578)\n\nAdded namespace support to xmliter_lxml (r2552)\n\nImproved cookies middleware by making COOKIES_DEBUG nicer and documenting it (r2579)\n\nSeveral improvements to Scrapyd and Link extractors\n\n\n\nCode rearranged and removed\u00b6\nMerged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (r2630)\noriginal item_scraped signal was removed\noriginal item_passed signal was renamed to item_scraped\nold log lines Scraped Item... were removed\nold log lines Passed Item... were renamed to Scraped Item... lines and downgraded to DEBUG level\n\n\nReduced Scrapy codebase by striping part of Scrapy code into two new libraries:\nw3lib (several functions from scrapy.utils.{http,markup,multipart,response,url}, done in r2584)\nscrapely (was scrapy.contrib.ibl, done in r2586)\n\n\nRemoved unused function: scrapy.utils.request.request_info() (r2577)\n\nRemoved googledir project from examples/googledir. There\u2019s now a new example project called dirbot available on github: https://github.com/scrapy/dirbot\n\nRemoved support for default field values in Scrapy items (r2616)\n\nRemoved experimental crawlspider v2 (r2632)\n\nRemoved scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (DUPEFILTER_CLASS setting) (r2640)\n\nRemoved support for passing urls to scrapy crawl command (use scrapy parse instead) (r2704)\n\nRemoved deprecated Execution Queue (r2704)\n\nRemoved (undocumented) spider context extension (from scrapy.contrib.spidercontext) (r2780)\n\nremoved CONCURRENT_SPIDERS setting (use scrapyd maxproc instead) (r2789)\n\nRenamed attributes of core components: downloader.sites -> downloader.slots, scraper.sites -> scraper.slots (r2717, r2718)\n\nRenamed setting CLOSESPIDER_ITEMPASSED to CLOSESPIDER_ITEMCOUNT (r2655). Backwards compatibility kept.\n\n\n\n\n0.12\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features and improvements\u00b6\nPassed item is now sent in the item argument of the item_passed (#273)\nAdded verbose option to scrapy version command, useful for bug reports (#298)\nHTTP cache now stored by default in the project data dir (#279)\nAdded project data storage directory (#276, #277)\nDocumented file structure of Scrapy projects (see command-line tool doc)\nNew lxml backend for XPath selectors (#147)\nPer-spider settings (#245)\nSupport exit codes to signal errors in Scrapy commands (#248)\nAdded -c argument to scrapy shell command\nMade libxml2 optional (#260)\nNew deploy command (#261)\nAdded CLOSESPIDER_PAGECOUNT setting (#253)\nAdded CLOSESPIDER_ERRORCOUNT setting (#254)\n\n\nScrapyd changes\u00b6\nScrapyd now uses one process per spider\nIt stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)\nA minimal web ui was added, available at http://localhost:6800 by default\nThere is now a scrapy server command to start a Scrapyd server of the current project\n\n\nChanges to settings\u00b6\nadded HTTPCACHE_ENABLED setting (False by default) to enable HTTP cache middleware\nchanged HTTPCACHE_EXPIRATION_SECS semantics: now zero means \u201cnever expire\u201d.\n\n\nDeprecated/obsoleted functionality\u00b6\nDeprecated runserver command in favor of server command which starts a Scrapyd server. See also: Scrapyd changes\nDeprecated queue command in favor of using Scrapyd schedule.json API. See also: Scrapyd changes\nRemoved the !LxmlItemLoader (experimental contrib which never graduated to main contrib)\n\n\n\n0.10\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features and improvements\u00b6\nNew Scrapy service called scrapyd for deploying Scrapy crawlers in production (#218) (documentation available)\nSimplified Images pipeline usage which doesn\u2019t require subclassing your own images pipeline now (#217)\nScrapy shell now shows the Scrapy log by default (#206)\nRefactored execution queue in a common base code and pluggable backends called \u201cspider queues\u201d (#220)\nNew persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.\nAdded documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)\nFeed exporters with pluggable backends (#197) (documentation available)\nDeferred signals (#193)\nAdded two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)\nSupport for overriding default request headers per spider (#181)\nReplaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)\nSplitted Debian package into two packages - the library and the service (#187)\nScrapy log refactoring (#188)\nNew extension for keeping persistent spider contexts among different runs (#203)\nAdded dont_redirect request.meta key for avoiding redirects (#233)\nAdded dont_retry request.meta key for avoiding retries (#234)\n\n\nCommand-line tool changes\u00b6\nNew scrapy command which replaces the old scrapy-ctl.py (#199)\n- there is only one global scrapy command now, instead of one scrapy-ctl.py per project\n- Added scrapy.bat script for running more conveniently from Windows\nAdded bash completion to command-line tool (#210)\nRenamed command start to runserver (#209)\n\n\nAPI changes\u00b6\nurl and body attributes of Request objects are now read-only (#230)\n\nRequest.copy() and Request.replace() now also copies their callback and errback attributes (#231)\n\nRemoved UrlFilterMiddleware from scrapy.contrib (already disabled by default)\n\nOffsite middelware doesn\u2019t filter out any request coming from a spider that doesn\u2019t have a allowed_domains attribute (#225)\n\nRemoved Spider Manager load() method. Now spiders are loaded in the constructor itself.\n\nChanges to Scrapy Manager (now called \u201cCrawler\u201d):\nscrapy.core.manager.ScrapyManager class renamed to scrapy.crawler.Crawler\nscrapy.core.manager.scrapymanager singleton moved to scrapy.project.crawler\n\n\nMoved module: scrapy.contrib.spidermanager to scrapy.spidermanager\n\nSpider Manager singleton moved from scrapy.spider.spiders to the spiders` attribute of ``scrapy.project.crawler singleton.\n\nmoved Stats Collector classes: (#204)\nscrapy.stats.collector.StatsCollector to scrapy.statscol.StatsCollector\nscrapy.stats.collector.SimpledbStatsCollector to scrapy.contrib.statscol.SimpledbStatsCollector\n\n\ndefault per-command settings are now specified in the default_settings attribute of command object class (#201)\n\nchanged arguments of Item pipeline process_item() method from (spider, item) to (item, spider)\nbackwards compatibility kept (with deprecation warning)\n\n\nmoved scrapy.core.signals module to scrapy.signals\nbackwards compatibility kept (with deprecation warning)\n\n\nmoved scrapy.core.exceptions module to scrapy.exceptions\nbackwards compatibility kept (with deprecation warning)\n\n\nadded handles_request() class method to BaseSpider\n\ndropped scrapy.log.exc() function (use scrapy.log.err() instead)\n\ndropped component argument of scrapy.log.msg() function\n\ndropped scrapy.log.log_level attribute\n\nAdded from_settings() class methods to Spider Manager, and Item Pipeline Manager\n\n\n\nChanges to settings\u00b6\nAdded HTTPCACHE_IGNORE_SCHEMES setting to ignore certain schemes on !HttpCacheMiddleware (#225)\nAdded SPIDER_QUEUE_CLASS setting which defines the spider queue to use (#220)\nAdded KEEP_ALIVE setting (#220)\nRemoved SERVICE_QUEUE setting (#220)\nRemoved COMMANDS_SETTINGS_MODULE setting (#201)\nRenamed REQUEST_HANDLERS to DOWNLOAD_HANDLERS and make download handlers classes (instead of functions)\n\n\n\n0.9\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features and improvements\u00b6\nAdded SMTP-AUTH support to scrapy.mail\nNew settings added: MAIL_USER, MAIL_PASS (r2065 | #149)\nAdded new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (r2039)\nAdded web service for controlling Scrapy process (this also deprecates the web console. (r2053 | #167)\nSupport for running Scrapy as a service, for production systems (r1988, r2054, r2055, r2056, r2057 | #168)\nAdded wrapper induction library (documentation only available in source code for now). (r2011)\nSimplified and improved response encoding support (r1961, r1969)\nAdded LOG_ENCODING setting (r1956, documentation available)\nAdded RANDOMIZE_DOWNLOAD_DELAY setting (enabled by default) (r1923, doc available)\nMailSender is no longer IO-blocking (r1955 | #146)\nLinkextractors and new Crawlspider now handle relative base tag urls (r1960 | #148)\nSeveral improvements to Item Loaders and processors (r2022, r2023, r2024, r2025, r2026, r2027, r2028, r2029, r2030)\nAdded support for adding variables to telnet console (r2047 | #165)\nSupport for requests without callbacks (r2050 | #166)\n\n\nAPI changes\u00b6\nChange Spider.domain_name to Spider.name (SEP-012, r1975)\nResponse.encoding is now the detected encoding (r1961)\nHttpErrorMiddleware now returns None or raises an exception (r2006 | #157)\nscrapy.command modules relocation (r2035, r2036, r2037)\nAdded ExecutionQueue for feeding spiders to scrape (r2034)\nRemoved ExecutionEngine singleton (r2039)\nPorted S3ImagesStore (images pipeline) to use boto and threads (r2033)\nMoved module: scrapy.management.telnet to scrapy.telnet (r2047)\n\n\nChanges to default settings\u00b6\nChanged default SCHEDULER_ORDER to DFO (r1939)\n\n\n\n0.8\u00b6\nThe numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.\n\nNew features\u00b6\nAdded DEFAULT_RESPONSE_ENCODING setting (r1809)\nAdded dont_click argument to FormRequest.from_response() method (r1813, r1816)\nAdded clickdata argument to FormRequest.from_response() method (r1802, r1803)\nAdded support for HTTP proxies (HttpProxyMiddleware) (r1781, r1785)\nOffiste spider middleware now logs messages when filtering out requests (r1841)\n\n\nBackwards-incompatible changes\u00b6\nChanged scrapy.utils.response.get_meta_refresh() signature (r1804)\n\nRemoved deprecated scrapy.item.ScrapedItem class - use scrapy.item.Item instead (r1838)\n\nRemoved deprecated scrapy.xpath module - use scrapy.selector instead. (r1836)\n\nRemoved deprecated core.signals.domain_open signal - use core.signals.domain_opened instead (r1822)\n\nlog.msg() now receives a spider argument (r1822)\nOld domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the spider argument and pass spider references. If you really want to pass a string, use the component argument instead.\n\n\nChanged core signals domain_opened, domain_closed, domain_idle\n\nChanged Item pipeline to use spiders instead of domains\nThe domain argument of  process_item() item pipeline method was changed to  spider, the new signature is: process_item(spider, item) (r1827 | #105)\nTo quickly port your code (to work with Scrapy 0.8) just use spider.domain_name where you previously used domain.\n\n\nChanged Stats API to use spiders instead of domains (r1849 | #113)\nStatsCollector was changed to receive spider references (instead of domains) in its methods (set_value, inc_value, etc).\nadded StatsCollector.iter_spider_stats() method\nremoved StatsCollector.list_domains() method\nAlso, Stats signals were renamed and now pass around spider references (instead of domains). Here\u2019s a summary of the changes:\nTo quickly port your code (to work with Scrapy 0.8) just use spider.domain_name where you previously used domain. spider_stats contains exactly the same data as domain_stats.\n\n\nCloseDomain extension moved to scrapy.contrib.closespider.CloseSpider (r1833)\nIts settings were also renamed:\nCLOSEDOMAIN_TIMEOUT to CLOSESPIDER_TIMEOUT\nCLOSEDOMAIN_ITEMCOUNT to CLOSESPIDER_ITEMCOUNT\n\n\n\n\nRemoved deprecated SCRAPYSETTINGS_MODULE environment variable - use SCRAPY_SETTINGS_MODULE instead (r1840)\n\nRenamed setting: REQUESTS_PER_DOMAIN to CONCURRENT_REQUESTS_PER_SPIDER (r1830, r1844)\n\nRenamed setting: CONCURRENT_DOMAINS to CONCURRENT_SPIDERS (r1830)\n\nRefactored HTTP Cache middleware\n\nHTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (r1843 )\n\nRenamed exception: DontCloseDomain to DontCloseSpider (r1859 | #120)\n\nRenamed extension: DelayedCloseDomain to SpiderCloseDelay (r1861 | #121)\n\nRemoved obsolete scrapy.utils.markup.remove_escape_chars function - use scrapy.utils.markup.replace_escape_chars instead (r1865)\n\n\n\n\n0.7\u00b6\nFirst release of Scrapy.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/news.html", "title": ["Release notes \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nContributing to Scrapy\u00b6\nThere are many ways to contribute to Scrapy. Here are some of them:\nBlog about Scrapy. Tell the world how you\u2019re using Scrapy. This will help\nnewcomers with more examples and the Scrapy project to increase its\nvisibility.\nReport bugs and request features in the issue tracker, trying to follow\nthe guidelines detailed in Reporting bugs below.\nSubmit patches for new functionality and/or bug fixes. Please read\nWriting patches and Submitting patches below for details on how to\nwrite and submit a patch.\nJoin the scrapy-developers mailing list and share your ideas on how to\nimprove Scrapy. We\u2019re always open to suggestions.\n\nReporting bugs\u00b6\nWell-written bug reports are very helpful, so keep in mind the following\nguidelines when reporting a new bug.\ncheck the FAQ first to see if your issue is addressed in a\nwell-known question\ncheck the open issues to see if it has already been reported. If it has,\ndon\u2019t dismiss the report but check the ticket history and comments, you may\nfind additional useful information to contribute.\nsearch the scrapy-users list to see if it has been discussed there, or\nif you\u2019re not sure if what you\u2019re seeing is a bug. You can also ask in the\n#scrapy IRC channel.\nwrite complete, reproducible, specific bug reports. The smaller the test\ncase, the better. Remember that other developers won\u2019t have your project to\nreproduce the bug, so please include all relevant files required to reproduce\nit.\ninclude the output of scrapy version -v so developers working on your bug\nknow exactly which version and platform it occurred on, which is often very\nhelpful for reproducing it, or knowing if it was already fixed.\n\n\nWriting patches\u00b6\nThe better written a patch is, the higher chance that it\u2019ll get accepted and\nthe sooner that will be merged.\nWell-written patches should:\ncontain the minimum amount of code required for the specific change. Small\npatches are easier to review and merge. So, if you\u2019re doing more than one\nchange (or bug fix), please consider submitting one patch per change. Do not\ncollapse multiple changes into a single patch. For big changes consider using\na patch queue.\npass all unit-tests. See Running tests below.\ninclude one (or more) test cases that check the bug fixed or the new\nfunctionality added. See Writing tests below.\nif you\u2019re adding or changing a public (documented) API, please include\nthe documentation changes in the same patch.  See Documentation policies\nbelow.\n\n\nSubmitting patches\u00b6\nThe best way to submit a patch is to issue a pull request on Github,\noptionally creating a new issue first.\nAlternatively, we also accept the patches in the traditional way of sending\nthem to the scrapy-developers list.\nRegardless of which mechanism you use, remember to explain what was fixed or\nthe new functionality (what it is, why it\u2019s needed, etc). The more info you\ninclude, the easier will be for core developers to understand and accept your\npatch.\nYou can also discuss the new functionality (or bug fix) in scrapy-developers\nfirst, before creating the patch, but it\u2019s always good to have a patch ready to\nillustrate your arguments and show that you have put some additional thought\ninto the subject.\n\n\nCoding style\u00b6\nPlease follow these coding conventions when writing code for inclusion in\nScrapy:\nUnless otherwise specified, follow PEP 8.\nIt\u2019s OK to use lines longer than 80 chars if it improves the code\nreadability.\nDon\u2019t put your name in the code you contribute. Our policy is to keep\nthe contributor\u2019s name in the AUTHORS file distributed with Scrapy.\n\n\nDocumentation policies\u00b6\nDon\u2019t use docstrings for documenting classes, or methods which are\nalready documented in the official (sphinx) documentation. For example, the\nItemLoader.add_value() method should be documented in the sphinx\ndocumentation, not its docstring.\nDo use docstrings for documenting functions not present in the official\n(sphinx) documentation, such as functions from scrapy.utils package and\nits sub-modules.\n\n\nTests\u00b6\nTests are implemented using the Twisted unit-testing framework called\ntrial.\n\nRunning tests\u00b6\nTo run all tests go to the root directory of Scrapy source code and run:\n\nbin/runtests.sh (on unix)\nbin\\runtests.bat (on windows)\n\nTo run a specific test (say scrapy.tests.test_contrib_loader) use:\n\nbin/runtests.sh scrapy.tests.test_contrib_loader (on unix)\nbin\\runtests.bat scrapy.tests.test_contrib_loader (on windows)\n\n\n\nWriting tests\u00b6\nAll functionality (including new features and bug fixes) must include a test\ncase to check that it works as expected, so please include tests for your\npatches if you want them to get accepted sooner.\nScrapy uses unit-tests, which are located in the scrapy.tests package\n(scrapy/tests directory). Their module name typically resembles the full\npath of the module they\u2019re testing. For example, the item loaders code is in:\nscrapy.contrib.loader\n\n\nAnd their unit-tests are in:\nscrapy.tests.test_contrib_loader\n\n\n\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/contributing.html", "title": ["Contributing to Scrapy \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nVersioning and API Stability\u00b6\n\nVersioning\u00b6\nScrapy uses the odd-numbered versions for development releases.\nThere are 3 numbers in a Scrapy version: A.B.C\nA is the major version. This will rarely change and will signify very\nlarge changes. So far, only zero is available for A as Scrapy hasn\u2019t yet\nreached 1.0.\nB is the release number. This will include many changes including features\nand things that possibly break backwards compatibility. Even Bs will be\nstable branches, and odd Bs will be development.\nC is the bugfix release number.\nFor example:\n0.14.1 is the first bugfix release of the 0.14 series (safe to use in\nproduction)\n\n\nAPI Stability\u00b6\nAPI stability is one of Scrapy major goals for the 1.0 release, which doesn\u2019t\nhave a due date scheduled yet.\nMethods or functions that start with a single dash (_) are private and\nshould never be relied as stable. Besides those, the plan is to stabilize and\ndocument the entire API, as we approach the 1.0 release.\nAlso, keep in mind that stable doesn\u2019t mean complete: stable APIs could grow\nnew methods or functionality but the existing methods should keep working the\nsame way.\n\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/versioning.html", "title": ["Versioning and API Stability \u2014 Scrapy 0.16.4 documentation"]},
{"content": "\n          \n            \n  \nExperimental features\u00b6\nThis section documents experimental Scrapy features that may become stable in\nfuture releases, but whose API is not yet stable. Use them with caution, and\nsubscribe to the mailing lists to get\nnotified of any changes.\nSince it\u2019s not revised so frequently, this section may contain documentation\nwhich is outdated, incomplete or overlapping with stable documentation (until\nit\u2019s properly merged) . Use at your own risk.\n\nWarning\nThis documentation is a work in progress. Use at your own risk.\n\nNo experimental features at this time\n\n\n\n          \n        ", "link": "http://doc.scrapy.org/en/0.16/experimental/index.html", "title": ["Experimental features \u2014 Scrapy 0.16.4 documentation"]}]